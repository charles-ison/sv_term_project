getting data
Files already downloaded and verified
Files already downloaded and verified
training VGG
epoch: 0
Train Loss: 0.2981, Train Acc: 0.15
Test Loss: 0.2069, Test Acc: 0.23
epoch: 1
Train Loss: 0.1983, Train Acc: 0.28
Test Loss: 0.1828, Test Acc: 0.30
epoch: 2
Train Loss: 0.1728, Train Acc: 0.38
Test Loss: 0.1441, Test Acc: 0.47
epoch: 3
Train Loss: 0.1521, Train Acc: 0.45
Test Loss: 0.1523, Test Acc: 0.47
epoch: 4
Train Loss: 0.1243, Train Acc: 0.55
Test Loss: 0.1104, Test Acc: 0.62
epoch: 5
Train Loss: 0.1172, Train Acc: 0.56
Test Loss: 0.0953, Test Acc: 0.65
epoch: 6
Train Loss: 0.0980, Train Acc: 0.66
Test Loss: 0.0882, Test Acc: 0.68
epoch: 7
Train Loss: 0.0962, Train Acc: 0.66
Test Loss: 0.0784, Test Acc: 0.72
epoch: 8
Train Loss: 0.0933, Train Acc: 0.65
Test Loss: 0.0691, Test Acc: 0.76
epoch: 9
Train Loss: 0.0834, Train Acc: 0.72
Test Loss: 0.0721, Test Acc: 0.74
X: -0.05, Y: -0.05
Training Loss: 0.0745
X: -0.04, Y: -0.05
Training Loss: 0.0775
X: -0.03, Y: -0.05
Training Loss: 0.0738
X: -0.019999999999999997, Y: -0.05
Training Loss: 0.0733
X: -0.009999999999999995, Y: -0.05
Training Loss: 0.0696
X: 6.938893903907228e-18, Y: -0.05
Training Loss: 0.0693
X: 0.010000000000000009, Y: -0.05
Training Loss: 0.0709
X: 0.020000000000000004, Y: -0.05
Training Loss: 0.0711
X: 0.030000000000000013, Y: -0.05
Training Loss: 0.0696
X: 0.04000000000000002, Y: -0.05
Training Loss: 0.0702
X: -0.05, Y: -0.04
Training Loss: 0.0711
X: -0.04, Y: -0.04
Training Loss: 0.0716
X: -0.03, Y: -0.04
Training Loss: 0.0710
X: -0.019999999999999997, Y: -0.04
Training Loss: 0.0730
X: -0.009999999999999995, Y: -0.04
Training Loss: 0.0717
X: 6.938893903907228e-18, Y: -0.04
Training Loss: 0.0701
X: 0.010000000000000009, Y: -0.04
Training Loss: 0.0701
X: 0.020000000000000004, Y: -0.04
Training Loss: 0.0711
X: 0.030000000000000013, Y: -0.04
Training Loss: 0.0706
X: 0.04000000000000002, Y: -0.04
Training Loss: 0.0654
X: -0.05, Y: -0.03
Training Loss: 0.0764
X: -0.04, Y: -0.03
Training Loss: 0.0741
X: -0.03, Y: -0.03
Training Loss: 0.0761
X: -0.019999999999999997, Y: -0.03
Training Loss: 0.0736
X: -0.009999999999999995, Y: -0.03
Training Loss: 0.0729
X: 6.938893903907228e-18, Y: -0.03
Training Loss: 0.0711
X: 0.010000000000000009, Y: -0.03
Training Loss: 0.0674
X: 0.020000000000000004, Y: -0.03
Training Loss: 0.0696
X: 0.030000000000000013, Y: -0.03
Training Loss: 0.0686
X: 0.04000000000000002, Y: -0.03
Training Loss: 0.0714
X: -0.05, Y: -0.019999999999999997
Training Loss: 0.0772
X: -0.04, Y: -0.019999999999999997
Training Loss: 0.0740
X: -0.03, Y: -0.019999999999999997
Training Loss: 0.0744
X: -0.019999999999999997, Y: -0.019999999999999997
Training Loss: 0.0738
X: -0.009999999999999995, Y: -0.019999999999999997
Training Loss: 0.0751
X: 6.938893903907228e-18, Y: -0.019999999999999997
Training Loss: 0.0708
X: 0.010000000000000009, Y: -0.019999999999999997
Training Loss: 0.0655
X: 0.020000000000000004, Y: -0.019999999999999997
Training Loss: 0.0744
X: 0.030000000000000013, Y: -0.019999999999999997
Training Loss: 0.0639
X: 0.04000000000000002, Y: -0.019999999999999997
Training Loss: 0.0681
X: -0.05, Y: -0.009999999999999995
Training Loss: 0.0739
X: -0.04, Y: -0.009999999999999995
Training Loss: 0.0751
X: -0.03, Y: -0.009999999999999995
Training Loss: 0.0666
X: -0.019999999999999997, Y: -0.009999999999999995
Training Loss: 0.0729
X: -0.009999999999999995, Y: -0.009999999999999995
Training Loss: 0.0706
X: 6.938893903907228e-18, Y: -0.009999999999999995
Training Loss: 0.0767
X: 0.010000000000000009, Y: -0.009999999999999995
Training Loss: 0.0666
X: 0.020000000000000004, Y: -0.009999999999999995
Training Loss: 0.0689
X: 0.030000000000000013, Y: -0.009999999999999995
Training Loss: 0.0676
X: 0.04000000000000002, Y: -0.009999999999999995
Training Loss: 0.0682
X: -0.05, Y: 6.938893903907228e-18
Training Loss: 0.0737
X: -0.04, Y: 6.938893903907228e-18
Training Loss: 0.0694
X: -0.03, Y: 6.938893903907228e-18
Training Loss: 0.0726
X: -0.019999999999999997, Y: 6.938893903907228e-18
Training Loss: 0.0678
X: -0.009999999999999995, Y: 6.938893903907228e-18
Training Loss: 0.0705
X: 6.938893903907228e-18, Y: 6.938893903907228e-18
Training Loss: 0.0668
X: 0.010000000000000009, Y: 6.938893903907228e-18
Training Loss: 0.0689
X: 0.020000000000000004, Y: 6.938893903907228e-18
Training Loss: 0.0678
X: 0.030000000000000013, Y: 6.938893903907228e-18
Training Loss: 0.0684
X: 0.04000000000000002, Y: 6.938893903907228e-18
Training Loss: 0.0674
X: -0.05, Y: 0.010000000000000009
Training Loss: 0.0677
X: -0.04, Y: 0.010000000000000009
Training Loss: 0.0715
X: -0.03, Y: 0.010000000000000009
Training Loss: 0.0738
X: -0.019999999999999997, Y: 0.010000000000000009
Training Loss: 0.0695
X: -0.009999999999999995, Y: 0.010000000000000009
Training Loss: 0.0654
X: 6.938893903907228e-18, Y: 0.010000000000000009
Training Loss: 0.0656
X: 0.010000000000000009, Y: 0.010000000000000009
Training Loss: 0.0666
X: 0.020000000000000004, Y: 0.010000000000000009
Training Loss: 0.0706
X: 0.030000000000000013, Y: 0.010000000000000009
Training Loss: 0.0754
X: 0.04000000000000002, Y: 0.010000000000000009
Training Loss: 0.0665
X: -0.05, Y: 0.020000000000000004
Training Loss: 0.0717
X: -0.04, Y: 0.020000000000000004
Training Loss: 0.0715
X: -0.03, Y: 0.020000000000000004
Training Loss: 0.0679
X: -0.019999999999999997, Y: 0.020000000000000004
Training Loss: 0.0694
X: -0.009999999999999995, Y: 0.020000000000000004
Training Loss: 0.0693
X: 6.938893903907228e-18, Y: 0.020000000000000004
Training Loss: 0.0666
X: 0.010000000000000009, Y: 0.020000000000000004
Training Loss: 0.0684
X: 0.020000000000000004, Y: 0.020000000000000004
Training Loss: 0.0674
X: 0.030000000000000013, Y: 0.020000000000000004
Training Loss: 0.0751
X: 0.04000000000000002, Y: 0.020000000000000004
Training Loss: 0.0743
X: -0.05, Y: 0.030000000000000013
Training Loss: 0.0699
X: -0.04, Y: 0.030000000000000013
Training Loss: 0.0699
X: -0.03, Y: 0.030000000000000013
Training Loss: 0.0735
X: -0.019999999999999997, Y: 0.030000000000000013
Training Loss: 0.0695
X: -0.009999999999999995, Y: 0.030000000000000013
Training Loss: 0.0703
X: 6.938893903907228e-18, Y: 0.030000000000000013
Training Loss: 0.0716
X: 0.010000000000000009, Y: 0.030000000000000013
Training Loss: 0.0671
X: 0.020000000000000004, Y: 0.030000000000000013
Training Loss: 0.0683
X: 0.030000000000000013, Y: 0.030000000000000013
Training Loss: 0.0710
X: 0.04000000000000002, Y: 0.030000000000000013
Training Loss: 0.0759
X: -0.05, Y: 0.04000000000000002
Training Loss: 0.0715
X: -0.04, Y: 0.04000000000000002
Training Loss: 0.0730
X: -0.03, Y: 0.04000000000000002
Training Loss: 0.0733
X: -0.019999999999999997, Y: 0.04000000000000002
Training Loss: 0.0704
X: -0.009999999999999995, Y: 0.04000000000000002
Training Loss: 0.0732
X: 6.938893903907228e-18, Y: 0.04000000000000002
Training Loss: 0.0709
X: 0.010000000000000009, Y: 0.04000000000000002
Training Loss: 0.0634
X: 0.020000000000000004, Y: 0.04000000000000002
Training Loss: 0.0730
X: 0.030000000000000013, Y: 0.04000000000000002
Training Loss: 0.0709
X: 0.04000000000000002, Y: 0.04000000000000002
Training Loss: 0.0720
Saved to runs/10_epochs_10_19_12_7_2022/new_data_VGG_Random_Directions_random.ply
training ResNet
epoch: 0
Train Loss: 0.4645, Train Acc: 0.15
Test Loss: 0.2378, Test Acc: 0.39
epoch: 1
Train Loss: 0.1576, Train Acc: 0.49
Test Loss: 0.1201, Test Acc: 0.60
epoch: 2
Train Loss: 0.1123, Train Acc: 0.64
Test Loss: 0.0826, Test Acc: 0.74
epoch: 3
Train Loss: 0.0892, Train Acc: 0.71
Test Loss: 0.0756, Test Acc: 0.76
epoch: 4
Train Loss: 0.0775, Train Acc: 0.74
Test Loss: 0.0562, Test Acc: 0.82
epoch: 5
Train Loss: 0.0723, Train Acc: 0.75
Test Loss: 0.0522, Test Acc: 0.83
epoch: 6
Train Loss: 0.0624, Train Acc: 0.78
Test Loss: 0.0428, Test Acc: 0.86
epoch: 7
Train Loss: 0.0544, Train Acc: 0.80
Test Loss: 0.0412, Test Acc: 0.85
epoch: 8
Train Loss: 0.0552, Train Acc: 0.81
Test Loss: 0.0497, Test Acc: 0.85
epoch: 9
Train Loss: 0.0532, Train Acc: 0.82
Test Loss: 0.0404, Test Acc: 0.87
X: -0.05, Y: -0.05
Training Loss: 0.6926
X: -0.04, Y: -0.05
Training Loss: 0.6214
X: -0.03, Y: -0.05
Training Loss: 0.5833
X: -0.019999999999999997, Y: -0.05
Training Loss: 0.5373
X: -0.009999999999999995, Y: -0.05
Training Loss: 0.4357
X: 6.938893903907228e-18, Y: -0.05
Training Loss: 0.4233
X: 0.010000000000000009, Y: -0.05
Training Loss: 0.4386
X: 0.020000000000000004, Y: -0.05
Training Loss: 0.4258
X: 0.030000000000000013, Y: -0.05
Training Loss: 0.4185
X: 0.04000000000000002, Y: -0.05
Training Loss: 0.4145
X: -0.05, Y: -0.04
Training Loss: 0.7718
X: -0.04, Y: -0.04
Training Loss: 0.6640
X: -0.03, Y: -0.04
Training Loss: 0.5541
X: -0.019999999999999997, Y: -0.04
Training Loss: 0.5353
X: -0.009999999999999995, Y: -0.04
Training Loss: 0.4603
X: 6.938893903907228e-18, Y: -0.04
Training Loss: 0.4447
X: 0.010000000000000009, Y: -0.04
Training Loss: 0.4977
X: 0.020000000000000004, Y: -0.04
Training Loss: 0.4794
X: 0.030000000000000013, Y: -0.04
Training Loss: 0.4674
X: 0.04000000000000002, Y: -0.04
Training Loss: 0.4719
X: -0.05, Y: -0.03
Training Loss: 0.8972
X: -0.04, Y: -0.03
Training Loss: 0.8003
X: -0.03, Y: -0.03
Training Loss: 0.5879
X: -0.019999999999999997, Y: -0.03
Training Loss: 0.5178
X: -0.009999999999999995, Y: -0.03
Training Loss: 0.4108
X: 6.938893903907228e-18, Y: -0.03
Training Loss: 0.4453
X: 0.010000000000000009, Y: -0.03
Training Loss: 0.5475
X: 0.020000000000000004, Y: -0.03
Training Loss: 0.5490
X: 0.030000000000000013, Y: -0.03
Training Loss: 0.5423
X: 0.04000000000000002, Y: -0.03
Training Loss: 0.5655
X: -0.05, Y: -0.019999999999999997
Training Loss: 1.1118
X: -0.04, Y: -0.019999999999999997
Training Loss: 0.9764
X: -0.03, Y: -0.019999999999999997
Training Loss: 0.7186
X: -0.019999999999999997, Y: -0.019999999999999997
Training Loss: 0.4959
X: -0.009999999999999995, Y: -0.019999999999999997
Training Loss: 0.2785
X: 6.938893903907228e-18, Y: -0.019999999999999997
Training Loss: 0.3046
X: 0.010000000000000009, Y: -0.019999999999999997
Training Loss: 0.5321
X: 0.020000000000000004, Y: -0.019999999999999997
Training Loss: 0.5817
X: 0.030000000000000013, Y: -0.019999999999999997
Training Loss: 0.5900
X: 0.04000000000000002, Y: -0.019999999999999997
Training Loss: 0.6112
X: -0.05, Y: -0.009999999999999995
Training Loss: 1.2412
X: -0.04, Y: -0.009999999999999995
Training Loss: 1.1630
X: -0.03, Y: -0.009999999999999995
Training Loss: 0.9087
X: -0.019999999999999997, Y: -0.009999999999999995
Training Loss: 0.5411
X: -0.009999999999999995, Y: -0.009999999999999995
Training Loss: 0.2122
X: 6.938893903907228e-18, Y: -0.009999999999999995
Training Loss: 0.0889
X: 0.010000000000000009, Y: -0.009999999999999995
Training Loss: 0.3771
X: 0.020000000000000004, Y: -0.009999999999999995
Training Loss: 0.5734
X: 0.030000000000000013, Y: -0.009999999999999995
Training Loss: 0.6166
X: 0.04000000000000002, Y: -0.009999999999999995
Training Loss: 0.6455
X: -0.05, Y: 6.938893903907228e-18
Training Loss: 1.2960
X: -0.04, Y: 6.938893903907228e-18
Training Loss: 1.3630
X: -0.03, Y: 6.938893903907228e-18
Training Loss: 1.1404
X: -0.019999999999999997, Y: 6.938893903907228e-18
Training Loss: 0.7321
X: -0.009999999999999995, Y: 6.938893903907228e-18
Training Loss: 0.3009
X: 6.938893903907228e-18, Y: 6.938893903907228e-18
Training Loss: 0.0334
X: 0.010000000000000009, Y: 6.938893903907228e-18
Training Loss: 0.1288
X: 0.020000000000000004, Y: 6.938893903907228e-18
Training Loss: 0.5403
X: 0.030000000000000013, Y: 6.938893903907228e-18
Training Loss: 0.6235
X: 0.04000000000000002, Y: 6.938893903907228e-18
Training Loss: 0.6520
X: -0.05, Y: 0.010000000000000009
Training Loss: 1.3071
X: -0.04, Y: 0.010000000000000009
Training Loss: 1.4380
X: -0.03, Y: 0.010000000000000009
Training Loss: 1.3325
X: -0.019999999999999997, Y: 0.010000000000000009
Training Loss: 0.9888
X: -0.009999999999999995, Y: 0.010000000000000009
Training Loss: 0.4873
X: 6.938893903907228e-18, Y: 0.010000000000000009
Training Loss: 0.1221
X: 0.010000000000000009, Y: 0.010000000000000009
Training Loss: 0.0661
X: 0.020000000000000004, Y: 0.010000000000000009
Training Loss: 0.4652
X: 0.030000000000000013, Y: 0.010000000000000009
Training Loss: 0.6117
X: 0.04000000000000002, Y: 0.010000000000000009
Training Loss: 0.6391
X: -0.05, Y: 0.020000000000000004
Training Loss: 1.4247
X: -0.04, Y: 0.020000000000000004
Training Loss: 1.5135
X: -0.03, Y: 0.020000000000000004
Training Loss: 1.4116
X: -0.019999999999999997, Y: 0.020000000000000004
Training Loss: 1.1072
X: -0.009999999999999995, Y: 0.020000000000000004
Training Loss: 0.7120
X: 6.938893903907228e-18, Y: 0.020000000000000004
Training Loss: 0.3779
X: 0.010000000000000009, Y: 0.020000000000000004
Training Loss: 0.1133
X: 0.020000000000000004, Y: 0.020000000000000004
Training Loss: 0.2575
X: 0.030000000000000013, Y: 0.020000000000000004
Training Loss: 0.5634
X: 0.04000000000000002, Y: 0.020000000000000004
Training Loss: 0.6136
X: -0.05, Y: 0.030000000000000013
Training Loss: 1.4925
X: -0.04, Y: 0.030000000000000013
Training Loss: 1.4613
X: -0.03, Y: 0.030000000000000013
Training Loss: 1.2675
X: -0.019999999999999997, Y: 0.030000000000000013
Training Loss: 0.9623
X: -0.009999999999999995, Y: 0.030000000000000013
Training Loss: 0.6939
X: 6.938893903907228e-18, Y: 0.030000000000000013
Training Loss: 0.5028
X: 0.010000000000000009, Y: 0.030000000000000013
Training Loss: 0.3007
X: 0.020000000000000004, Y: 0.030000000000000013
Training Loss: 0.1743
X: 0.030000000000000013, Y: 0.030000000000000013
Training Loss: 0.4278
X: 0.04000000000000002, Y: 0.030000000000000013
Training Loss: 0.5686
X: -0.05, Y: 0.04000000000000002
Training Loss: 1.4442
X: -0.04, Y: 0.04000000000000002
Training Loss: 1.4243
X: -0.03, Y: 0.04000000000000002
Training Loss: 1.0473
X: -0.019999999999999997, Y: 0.04000000000000002
Training Loss: 0.7460
X: -0.009999999999999995, Y: 0.04000000000000002
Training Loss: 0.6513
X: 6.938893903907228e-18, Y: 0.04000000000000002
Training Loss: 0.5551
X: 0.010000000000000009, Y: 0.04000000000000002
Training Loss: 0.4153
X: 0.020000000000000004, Y: 0.04000000000000002
Training Loss: 0.2905
X: 0.030000000000000013, Y: 0.04000000000000002
Training Loss: 0.2720
X: 0.04000000000000002, Y: 0.04000000000000002
Training Loss: 0.4455
Saved to runs/10_epochs_10_19_12_7_2022/new_data_ResNet_Random_Directions_random.ply
