getting data
Files already downloaded and verified
Files already downloaded and verified
training VGG
epoch: 0
Train Loss: 0.3032, Train Acc: 0.16
Test Loss: 0.2000, Test Acc: 0.27
epoch: 1
Train Loss: 0.1908, Train Acc: 0.32
Test Loss: 0.1482, Test Acc: 0.48
epoch: 2
Train Loss: 0.1584, Train Acc: 0.44
Test Loss: 0.1278, Test Acc: 0.54
epoch: 3
Train Loss: 0.1228, Train Acc: 0.56
Test Loss: 0.1131, Test Acc: 0.61
epoch: 4
Train Loss: 0.1148, Train Acc: 0.61
Test Loss: 0.1029, Test Acc: 0.63
epoch: 5
Train Loss: 0.0998, Train Acc: 0.65
Test Loss: 0.0936, Test Acc: 0.67
epoch: 6
Train Loss: 0.0935, Train Acc: 0.70
Test Loss: 0.0750, Test Acc: 0.76
epoch: 7
Train Loss: 0.0859, Train Acc: 0.71
Test Loss: 0.0835, Test Acc: 0.72
epoch: 8
Train Loss: 0.0757, Train Acc: 0.74
Test Loss: 0.0682, Test Acc: 0.76
epoch: 9
Train Loss: 0.0822, Train Acc: 0.73
Test Loss: 0.0666, Test Acc: 0.79
X: -0.05, Y: -0.05
Training Loss: 0.0663
X: -0.0425, Y: -0.05
Training Loss: 0.0651
X: -0.035, Y: -0.05
Training Loss: 0.0683
X: -0.027500000000000004, Y: -0.05
Training Loss: 0.0654
X: -0.020000000000000004, Y: -0.05
Training Loss: 0.0676
X: -0.012500000000000004, Y: -0.05
Training Loss: 0.0665
X: -0.0050000000000000044, Y: -0.05
Training Loss: 0.0657
X: 0.0024999999999999953, Y: -0.05
Training Loss: 0.0690
X: 0.009999999999999995, Y: -0.05
Training Loss: 0.0679
X: 0.0175, Y: -0.05
Training Loss: 0.0676
X: 0.024999999999999994, Y: -0.05
Training Loss: 0.0721
X: 0.03249999999999999, Y: -0.05
Training Loss: 0.0686
X: 0.039999999999999994, Y: -0.05
Training Loss: 0.0727
X: 0.0475, Y: -0.05
Training Loss: 0.0653
X: -0.05, Y: -0.0425
Training Loss: 0.0639
X: -0.0425, Y: -0.0425
Training Loss: 0.0636
X: -0.035, Y: -0.0425
Training Loss: 0.0659
X: -0.027500000000000004, Y: -0.0425
Training Loss: 0.0647
X: -0.020000000000000004, Y: -0.0425
Training Loss: 0.0716
X: -0.012500000000000004, Y: -0.0425
Training Loss: 0.0731
X: -0.0050000000000000044, Y: -0.0425
Training Loss: 0.0667
X: 0.0024999999999999953, Y: -0.0425
Training Loss: 0.0695
X: 0.009999999999999995, Y: -0.0425
Training Loss: 0.0635
X: 0.0175, Y: -0.0425
Training Loss: 0.0606
X: 0.024999999999999994, Y: -0.0425
Training Loss: 0.0672
X: 0.03249999999999999, Y: -0.0425
Training Loss: 0.0645
X: 0.039999999999999994, Y: -0.0425
Training Loss: 0.0671
X: 0.0475, Y: -0.0425
Training Loss: 0.0673
X: -0.05, Y: -0.035
Training Loss: 0.0699
X: -0.0425, Y: -0.035
Training Loss: 0.0645
X: -0.035, Y: -0.035
Training Loss: 0.0652
X: -0.027500000000000004, Y: -0.035
Training Loss: 0.0689
X: -0.020000000000000004, Y: -0.035
Training Loss: 0.0605
X: -0.012500000000000004, Y: -0.035
Training Loss: 0.0679
X: -0.0050000000000000044, Y: -0.035
Training Loss: 0.0644
X: 0.0024999999999999953, Y: -0.035
Training Loss: 0.0627
X: 0.009999999999999995, Y: -0.035
Training Loss: 0.0629
X: 0.0175, Y: -0.035
Training Loss: 0.0629
X: 0.024999999999999994, Y: -0.035
Training Loss: 0.0696
X: 0.03249999999999999, Y: -0.035
Training Loss: 0.0645
X: 0.039999999999999994, Y: -0.035
Training Loss: 0.0661
X: 0.0475, Y: -0.035
Training Loss: 0.0657
X: -0.05, Y: -0.027500000000000004
Training Loss: 0.0700
X: -0.0425, Y: -0.027500000000000004
Training Loss: 0.0628
X: -0.035, Y: -0.027500000000000004
Training Loss: 0.0699
X: -0.027500000000000004, Y: -0.027500000000000004
Training Loss: 0.0665
X: -0.020000000000000004, Y: -0.027500000000000004
Training Loss: 0.0661
X: -0.012500000000000004, Y: -0.027500000000000004
Training Loss: 0.0640
X: -0.0050000000000000044, Y: -0.027500000000000004
Training Loss: 0.0676
X: 0.0024999999999999953, Y: -0.027500000000000004
Training Loss: 0.0664
X: 0.009999999999999995, Y: -0.027500000000000004
Training Loss: 0.0696
X: 0.0175, Y: -0.027500000000000004
Training Loss: 0.0689
X: 0.024999999999999994, Y: -0.027500000000000004
Training Loss: 0.0710
X: 0.03249999999999999, Y: -0.027500000000000004
Training Loss: 0.0657
X: 0.039999999999999994, Y: -0.027500000000000004
Training Loss: 0.0677
X: 0.0475, Y: -0.027500000000000004
Training Loss: 0.0688
X: -0.05, Y: -0.020000000000000004
Training Loss: 0.0652
X: -0.0425, Y: -0.020000000000000004
Training Loss: 0.0635
X: -0.035, Y: -0.020000000000000004
Training Loss: 0.0645
X: -0.027500000000000004, Y: -0.020000000000000004
Training Loss: 0.0669
X: -0.020000000000000004, Y: -0.020000000000000004
Training Loss: 0.0630
X: -0.012500000000000004, Y: -0.020000000000000004
Training Loss: 0.0650
X: -0.0050000000000000044, Y: -0.020000000000000004
Training Loss: 0.0652
X: 0.0024999999999999953, Y: -0.020000000000000004
Training Loss: 0.0648
X: 0.009999999999999995, Y: -0.020000000000000004
Training Loss: 0.0686
X: 0.0175, Y: -0.020000000000000004
Training Loss: 0.0687
X: 0.024999999999999994, Y: -0.020000000000000004
Training Loss: 0.0638
X: 0.03249999999999999, Y: -0.020000000000000004
Training Loss: 0.0723
X: 0.039999999999999994, Y: -0.020000000000000004
Training Loss: 0.0628
X: 0.0475, Y: -0.020000000000000004
Training Loss: 0.0708
X: -0.05, Y: -0.012500000000000004
Training Loss: 0.0661
X: -0.0425, Y: -0.012500000000000004
Training Loss: 0.0630
X: -0.035, Y: -0.012500000000000004
Training Loss: 0.0656
X: -0.027500000000000004, Y: -0.012500000000000004
Training Loss: 0.0637
X: -0.020000000000000004, Y: -0.012500000000000004
Training Loss: 0.0665
X: -0.012500000000000004, Y: -0.012500000000000004
Training Loss: 0.0675
X: -0.0050000000000000044, Y: -0.012500000000000004
Training Loss: 0.0669
X: 0.0024999999999999953, Y: -0.012500000000000004
Training Loss: 0.0699
X: 0.009999999999999995, Y: -0.012500000000000004
Training Loss: 0.0657
X: 0.0175, Y: -0.012500000000000004
Training Loss: 0.0666
X: 0.024999999999999994, Y: -0.012500000000000004
Training Loss: 0.0646
X: 0.03249999999999999, Y: -0.012500000000000004
Training Loss: 0.0683
X: 0.039999999999999994, Y: -0.012500000000000004
Training Loss: 0.0696
X: 0.0475, Y: -0.012500000000000004
Training Loss: 0.0636
X: -0.05, Y: -0.0050000000000000044
Training Loss: 0.0663
X: -0.0425, Y: -0.0050000000000000044
Training Loss: 0.0626
X: -0.035, Y: -0.0050000000000000044
Training Loss: 0.0678
X: -0.027500000000000004, Y: -0.0050000000000000044
Training Loss: 0.0706
X: -0.020000000000000004, Y: -0.0050000000000000044
Training Loss: 0.0603
X: -0.012500000000000004, Y: -0.0050000000000000044
Training Loss: 0.0646
X: -0.0050000000000000044, Y: -0.0050000000000000044
Training Loss: 0.0622
X: 0.0024999999999999953, Y: -0.0050000000000000044
Training Loss: 0.0623
X: 0.009999999999999995, Y: -0.0050000000000000044
Training Loss: 0.0642
X: 0.0175, Y: -0.0050000000000000044
Training Loss: 0.0658
X: 0.024999999999999994, Y: -0.0050000000000000044
Training Loss: 0.0666
X: 0.03249999999999999, Y: -0.0050000000000000044
Training Loss: 0.0672
X: 0.039999999999999994, Y: -0.0050000000000000044
Training Loss: 0.0636
X: 0.0475, Y: -0.0050000000000000044
Training Loss: 0.0621
X: -0.05, Y: 0.0024999999999999953
Training Loss: 0.0699
X: -0.0425, Y: 0.0024999999999999953
Training Loss: 0.0670
X: -0.035, Y: 0.0024999999999999953
Training Loss: 0.0658
X: -0.027500000000000004, Y: 0.0024999999999999953
Training Loss: 0.0656
X: -0.020000000000000004, Y: 0.0024999999999999953
Training Loss: 0.0673
X: -0.012500000000000004, Y: 0.0024999999999999953
Training Loss: 0.0639
X: -0.0050000000000000044, Y: 0.0024999999999999953
Training Loss: 0.0650
X: 0.0024999999999999953, Y: 0.0024999999999999953
Training Loss: 0.0617
X: 0.009999999999999995, Y: 0.0024999999999999953
Training Loss: 0.0637
X: 0.0175, Y: 0.0024999999999999953
Training Loss: 0.0662
X: 0.024999999999999994, Y: 0.0024999999999999953
Training Loss: 0.0674
X: 0.03249999999999999, Y: 0.0024999999999999953
Training Loss: 0.0661
X: 0.039999999999999994, Y: 0.0024999999999999953
Training Loss: 0.0656
X: 0.0475, Y: 0.0024999999999999953
Training Loss: 0.0663
X: -0.05, Y: 0.009999999999999995
Training Loss: 0.0726
X: -0.0425, Y: 0.009999999999999995
Training Loss: 0.0670
X: -0.035, Y: 0.009999999999999995
Training Loss: 0.0699
X: -0.027500000000000004, Y: 0.009999999999999995
Training Loss: 0.0686
X: -0.020000000000000004, Y: 0.009999999999999995
Training Loss: 0.0752
X: -0.012500000000000004, Y: 0.009999999999999995
Training Loss: 0.0697
X: -0.0050000000000000044, Y: 0.009999999999999995
Training Loss: 0.0658
X: 0.0024999999999999953, Y: 0.009999999999999995
Training Loss: 0.0706
X: 0.009999999999999995, Y: 0.009999999999999995
Training Loss: 0.0644
X: 0.0175, Y: 0.009999999999999995
Training Loss: 0.0682
X: 0.024999999999999994, Y: 0.009999999999999995
Training Loss: 0.0667
X: 0.03249999999999999, Y: 0.009999999999999995
Training Loss: 0.0598
X: 0.039999999999999994, Y: 0.009999999999999995
Training Loss: 0.0670
X: 0.0475, Y: 0.009999999999999995
Training Loss: 0.0653
X: -0.05, Y: 0.0175
Training Loss: 0.0705
X: -0.0425, Y: 0.0175
Training Loss: 0.0673
X: -0.035, Y: 0.0175
Training Loss: 0.0641
X: -0.027500000000000004, Y: 0.0175
Training Loss: 0.0699
X: -0.020000000000000004, Y: 0.0175
Training Loss: 0.0731
X: -0.012500000000000004, Y: 0.0175
Training Loss: 0.0659
X: -0.0050000000000000044, Y: 0.0175
Training Loss: 0.0574
X: 0.0024999999999999953, Y: 0.0175
Training Loss: 0.0629
X: 0.009999999999999995, Y: 0.0175
Training Loss: 0.0685
X: 0.0175, Y: 0.0175
Training Loss: 0.0675
X: 0.024999999999999994, Y: 0.0175
Training Loss: 0.0660
X: 0.03249999999999999, Y: 0.0175
Training Loss: 0.0681
X: 0.039999999999999994, Y: 0.0175
Training Loss: 0.0648
X: 0.0475, Y: 0.0175
Training Loss: 0.0715
X: -0.05, Y: 0.024999999999999994
Training Loss: 0.0692
X: -0.0425, Y: 0.024999999999999994
Training Loss: 0.0652
X: -0.035, Y: 0.024999999999999994
Training Loss: 0.0662
X: -0.027500000000000004, Y: 0.024999999999999994
Training Loss: 0.0714
X: -0.020000000000000004, Y: 0.024999999999999994
Training Loss: 0.0723
X: -0.012500000000000004, Y: 0.024999999999999994
Training Loss: 0.0705
X: -0.0050000000000000044, Y: 0.024999999999999994
Training Loss: 0.0688
X: 0.0024999999999999953, Y: 0.024999999999999994
Training Loss: 0.0665
X: 0.009999999999999995, Y: 0.024999999999999994
Training Loss: 0.0666
X: 0.0175, Y: 0.024999999999999994
Training Loss: 0.0643
X: 0.024999999999999994, Y: 0.024999999999999994
Training Loss: 0.0649
X: 0.03249999999999999, Y: 0.024999999999999994
Training Loss: 0.0667
X: 0.039999999999999994, Y: 0.024999999999999994
Training Loss: 0.0686
X: 0.0475, Y: 0.024999999999999994
Training Loss: 0.0661
X: -0.05, Y: 0.03249999999999999
Training Loss: 0.0709
X: -0.0425, Y: 0.03249999999999999
Training Loss: 0.0655
X: -0.035, Y: 0.03249999999999999
Training Loss: 0.0685
X: -0.027500000000000004, Y: 0.03249999999999999
Training Loss: 0.0666
X: -0.020000000000000004, Y: 0.03249999999999999
Training Loss: 0.0638
X: -0.012500000000000004, Y: 0.03249999999999999
Training Loss: 0.0760
X: -0.0050000000000000044, Y: 0.03249999999999999
Training Loss: 0.0671
X: 0.0024999999999999953, Y: 0.03249999999999999
Training Loss: 0.0680
X: 0.009999999999999995, Y: 0.03249999999999999
Training Loss: 0.0707
X: 0.0175, Y: 0.03249999999999999
Training Loss: 0.0673
X: 0.024999999999999994, Y: 0.03249999999999999
Training Loss: 0.0679
X: 0.03249999999999999, Y: 0.03249999999999999
Training Loss: 0.0686
X: 0.039999999999999994, Y: 0.03249999999999999
Training Loss: 0.0642
X: 0.0475, Y: 0.03249999999999999
Training Loss: 0.0667
X: -0.05, Y: 0.039999999999999994
Training Loss: 0.0681
X: -0.0425, Y: 0.039999999999999994
Training Loss: 0.0675
X: -0.035, Y: 0.039999999999999994
Training Loss: 0.0698
X: -0.027500000000000004, Y: 0.039999999999999994
Training Loss: 0.0709
X: -0.020000000000000004, Y: 0.039999999999999994
Training Loss: 0.0676
X: -0.012500000000000004, Y: 0.039999999999999994
Training Loss: 0.0644
X: -0.0050000000000000044, Y: 0.039999999999999994
Training Loss: 0.0637
X: 0.0024999999999999953, Y: 0.039999999999999994
Training Loss: 0.0632
X: 0.009999999999999995, Y: 0.039999999999999994
Training Loss: 0.0642
X: 0.0175, Y: 0.039999999999999994
Training Loss: 0.0672
X: 0.024999999999999994, Y: 0.039999999999999994
Training Loss: 0.0684
X: 0.03249999999999999, Y: 0.039999999999999994
Training Loss: 0.0630
X: 0.039999999999999994, Y: 0.039999999999999994
Training Loss: 0.0656
X: 0.0475, Y: 0.039999999999999994
Training Loss: 0.0659
X: -0.05, Y: 0.0475
Training Loss: 0.0724
X: -0.0425, Y: 0.0475
Training Loss: 0.0685
X: -0.035, Y: 0.0475
Training Loss: 0.0684
X: -0.027500000000000004, Y: 0.0475
Training Loss: 0.0634
X: -0.020000000000000004, Y: 0.0475
Training Loss: 0.0686
X: -0.012500000000000004, Y: 0.0475
Training Loss: 0.0662
X: -0.0050000000000000044, Y: 0.0475
Training Loss: 0.0676
X: 0.0024999999999999953, Y: 0.0475
Training Loss: 0.0604
X: 0.009999999999999995, Y: 0.0475
Training Loss: 0.0691
X: 0.0175, Y: 0.0475
Training Loss: 0.0656
X: 0.024999999999999994, Y: 0.0475
Training Loss: 0.0652
X: 0.03249999999999999, Y: 0.0475
Training Loss: 0.0688
X: 0.039999999999999994, Y: 0.0475
Training Loss: 0.0639
X: 0.0475, Y: 0.0475
Training Loss: 0.0658
Saved to runs/10_epochs_11_16_12_7_2022/new_data_VGG_Random_Directions_random.ply
training ResNet
epoch: 0
Train Loss: 0.4285, Train Acc: 0.20
Test Loss: 0.1922, Test Acc: 0.42
epoch: 1
Train Loss: 0.1524, Train Acc: 0.51
Test Loss: 0.1147, Test Acc: 0.64
epoch: 2
Train Loss: 0.1131, Train Acc: 0.61
Test Loss: 0.0856, Test Acc: 0.72
epoch: 3
Train Loss: 0.0820, Train Acc: 0.72
Test Loss: 0.0751, Test Acc: 0.75
epoch: 4
Train Loss: 0.0823, Train Acc: 0.72
Test Loss: 0.0608, Test Acc: 0.80
epoch: 5
Train Loss: 0.0759, Train Acc: 0.75
Test Loss: 0.0531, Test Acc: 0.84
epoch: 6
Train Loss: 0.0629, Train Acc: 0.79
Test Loss: 0.0443, Test Acc: 0.86
epoch: 7
Train Loss: 0.0599, Train Acc: 0.80
Test Loss: 0.0450, Test Acc: 0.86
epoch: 8
Train Loss: 0.0512, Train Acc: 0.83
Test Loss: 0.0407, Test Acc: 0.86
epoch: 9
Train Loss: 0.0558, Train Acc: 0.82
Test Loss: 0.0384, Test Acc: 0.88
X: -0.05, Y: -0.05
Training Loss: 1.7796
X: -0.0425, Y: -0.05
Training Loss: 1.4066
X: -0.035, Y: -0.05
Training Loss: 0.8245
X: -0.027500000000000004, Y: -0.05
Training Loss: 0.5267
X: -0.020000000000000004, Y: -0.05
Training Loss: 0.3919
X: -0.012500000000000004, Y: -0.05
Training Loss: 0.4481
X: -0.0050000000000000044, Y: -0.05
Training Loss: 0.4659
X: 0.0024999999999999953, Y: -0.05
Training Loss: 0.4529
X: 0.009999999999999995, Y: -0.05
Training Loss: 0.4521
X: 0.0175, Y: -0.05
Training Loss: 0.4385
X: 0.024999999999999994, Y: -0.05
Training Loss: 0.4703
X: 0.03249999999999999, Y: -0.05
Training Loss: 0.5359
X: 0.039999999999999994, Y: -0.05
Training Loss: 0.5555
X: 0.0475, Y: -0.05
Training Loss: 0.5867
X: -0.05, Y: -0.0425
Training Loss: 1.5857
X: -0.0425, Y: -0.0425
Training Loss: 1.3351
X: -0.035, Y: -0.0425
Training Loss: 0.8254
X: -0.027500000000000004, Y: -0.0425
Training Loss: 0.4788
X: -0.020000000000000004, Y: -0.0425
Training Loss: 0.3973
X: -0.012500000000000004, Y: -0.0425
Training Loss: 0.4634
X: -0.0050000000000000044, Y: -0.0425
Training Loss: 0.4621
X: 0.0024999999999999953, Y: -0.0425
Training Loss: 0.4631
X: 0.009999999999999995, Y: -0.0425
Training Loss: 0.4675
X: 0.0175, Y: -0.0425
Training Loss: 0.4273
X: 0.024999999999999994, Y: -0.0425
Training Loss: 0.4466
X: 0.03249999999999999, Y: -0.0425
Training Loss: 0.5077
X: 0.039999999999999994, Y: -0.0425
Training Loss: 0.6084
X: 0.0475, Y: -0.0425
Training Loss: 0.6646
X: -0.05, Y: -0.035
Training Loss: 1.4614
X: -0.0425, Y: -0.035
Training Loss: 1.1542
X: -0.035, Y: -0.035
Training Loss: 0.7823
X: -0.027500000000000004, Y: -0.035
Training Loss: 0.4690
X: -0.020000000000000004, Y: -0.035
Training Loss: 0.3993
X: -0.012500000000000004, Y: -0.035
Training Loss: 0.4589
X: -0.0050000000000000044, Y: -0.035
Training Loss: 0.4767
X: 0.0024999999999999953, Y: -0.035
Training Loss: 0.4744
X: 0.009999999999999995, Y: -0.035
Training Loss: 0.4687
X: 0.0175, Y: -0.035
Training Loss: 0.4339
X: 0.024999999999999994, Y: -0.035
Training Loss: 0.4352
X: 0.03249999999999999, Y: -0.035
Training Loss: 0.4920
X: 0.039999999999999994, Y: -0.035
Training Loss: 0.6782
X: 0.0475, Y: -0.035
Training Loss: 0.8531
X: -0.05, Y: -0.027500000000000004
Training Loss: 1.6014
X: -0.0425, Y: -0.027500000000000004
Training Loss: 1.0540
X: -0.035, Y: -0.027500000000000004
Training Loss: 0.6818
X: -0.027500000000000004, Y: -0.027500000000000004
Training Loss: 0.4888
X: -0.020000000000000004, Y: -0.027500000000000004
Training Loss: 0.3930
X: -0.012500000000000004, Y: -0.027500000000000004
Training Loss: 0.3943
X: -0.0050000000000000044, Y: -0.027500000000000004
Training Loss: 0.4571
X: 0.0024999999999999953, Y: -0.027500000000000004
Training Loss: 0.4190
X: 0.009999999999999995, Y: -0.027500000000000004
Training Loss: 0.4283
X: 0.0175, Y: -0.027500000000000004
Training Loss: 0.4114
X: 0.024999999999999994, Y: -0.027500000000000004
Training Loss: 0.4337
X: 0.03249999999999999, Y: -0.027500000000000004
Training Loss: 0.5983
X: 0.039999999999999994, Y: -0.027500000000000004
Training Loss: 1.0304
X: 0.0475, Y: -0.027500000000000004
Training Loss: 1.3289
X: -0.05, Y: -0.020000000000000004
Training Loss: 1.5386
X: -0.0425, Y: -0.020000000000000004
Training Loss: 1.0573
X: -0.035, Y: -0.020000000000000004
Training Loss: 0.6605
X: -0.027500000000000004, Y: -0.020000000000000004
Training Loss: 0.5102
X: -0.020000000000000004, Y: -0.020000000000000004
Training Loss: 0.3833
X: -0.012500000000000004, Y: -0.020000000000000004
Training Loss: 0.3346
X: -0.0050000000000000044, Y: -0.020000000000000004
Training Loss: 0.3495
X: 0.0024999999999999953, Y: -0.020000000000000004
Training Loss: 0.2815
X: 0.009999999999999995, Y: -0.020000000000000004
Training Loss: 0.2926
X: 0.0175, Y: -0.020000000000000004
Training Loss: 0.3993
X: 0.024999999999999994, Y: -0.020000000000000004
Training Loss: 0.5141
X: 0.03249999999999999, Y: -0.020000000000000004
Training Loss: 0.7304
X: 0.039999999999999994, Y: -0.020000000000000004
Training Loss: 1.0989
X: 0.0475, Y: -0.020000000000000004
Training Loss: 1.4331
X: -0.05, Y: -0.012500000000000004
Training Loss: 1.2804
X: -0.0425, Y: -0.012500000000000004
Training Loss: 0.9742
X: -0.035, Y: -0.012500000000000004
Training Loss: 0.6424
X: -0.027500000000000004, Y: -0.012500000000000004
Training Loss: 0.4507
X: -0.020000000000000004, Y: -0.012500000000000004
Training Loss: 0.3657
X: -0.012500000000000004, Y: -0.012500000000000004
Training Loss: 0.2422
X: -0.0050000000000000044, Y: -0.012500000000000004
Training Loss: 0.1858
X: 0.0024999999999999953, Y: -0.012500000000000004
Training Loss: 0.1215
X: 0.009999999999999995, Y: -0.012500000000000004
Training Loss: 0.1760
X: 0.0175, Y: -0.012500000000000004
Training Loss: 0.4945
X: 0.024999999999999994, Y: -0.012500000000000004
Training Loss: 0.5470
X: 0.03249999999999999, Y: -0.012500000000000004
Training Loss: 0.6727
X: 0.039999999999999994, Y: -0.012500000000000004
Training Loss: 1.0074
X: 0.0475, Y: -0.012500000000000004
Training Loss: 1.3524
X: -0.05, Y: -0.0050000000000000044
Training Loss: 1.2629
X: -0.0425, Y: -0.0050000000000000044
Training Loss: 0.9508
X: -0.035, Y: -0.0050000000000000044
Training Loss: 0.6444
X: -0.027500000000000004, Y: -0.0050000000000000044
Training Loss: 0.4555
X: -0.020000000000000004, Y: -0.0050000000000000044
Training Loss: 0.3760
X: -0.012500000000000004, Y: -0.0050000000000000044
Training Loss: 0.1889
X: -0.0050000000000000044, Y: -0.0050000000000000044
Training Loss: 0.0678
X: 0.0024999999999999953, Y: -0.0050000000000000044
Training Loss: 0.0399
X: 0.009999999999999995, Y: -0.0050000000000000044
Training Loss: 0.1775
X: 0.0175, Y: -0.0050000000000000044
Training Loss: 0.5396
X: 0.024999999999999994, Y: -0.0050000000000000044
Training Loss: 0.5681
X: 0.03249999999999999, Y: -0.0050000000000000044
Training Loss: 0.7336
X: 0.039999999999999994, Y: -0.0050000000000000044
Training Loss: 1.1071
X: 0.0475, Y: -0.0050000000000000044
Training Loss: 1.5497
X: -0.05, Y: 0.0024999999999999953
Training Loss: 1.8430
X: -0.0425, Y: 0.0024999999999999953
Training Loss: 1.5381
X: -0.035, Y: 0.0024999999999999953
Training Loss: 1.2551
X: -0.027500000000000004, Y: 0.0024999999999999953
Training Loss: 0.9679
X: -0.020000000000000004, Y: 0.0024999999999999953
Training Loss: 0.6052
X: -0.012500000000000004, Y: 0.0024999999999999953
Training Loss: 0.3049
X: -0.0050000000000000044, Y: 0.0024999999999999953
Training Loss: 0.1393
X: 0.0024999999999999953, Y: 0.0024999999999999953
Training Loss: 0.0569
X: 0.009999999999999995, Y: 0.0024999999999999953
Training Loss: 0.3208
X: 0.0175, Y: 0.0024999999999999953
Training Loss: 0.5748
X: 0.024999999999999994, Y: 0.0024999999999999953
Training Loss: 0.6272
X: 0.03249999999999999, Y: 0.0024999999999999953
Training Loss: 0.9537
X: 0.039999999999999994, Y: 0.0024999999999999953
Training Loss: 1.4377
X: 0.0475, Y: 0.0024999999999999953
Training Loss: 2.0059
X: -0.05, Y: 0.009999999999999995
Training Loss: 2.3424
X: -0.0425, Y: 0.009999999999999995
Training Loss: 2.0375
X: -0.035, Y: 0.009999999999999995
Training Loss: 1.7812
X: -0.027500000000000004, Y: 0.009999999999999995
Training Loss: 1.5786
X: -0.020000000000000004, Y: 0.009999999999999995
Training Loss: 1.3137
X: -0.012500000000000004, Y: 0.009999999999999995
Training Loss: 0.9934
X: -0.0050000000000000044, Y: 0.009999999999999995
Training Loss: 0.7813
X: 0.0024999999999999953, Y: 0.009999999999999995
Training Loss: 0.5864
X: 0.009999999999999995, Y: 0.009999999999999995
Training Loss: 0.6186
X: 0.0175, Y: 0.009999999999999995
Training Loss: 0.6087
X: 0.024999999999999994, Y: 0.009999999999999995
Training Loss: 0.7400
X: 0.03249999999999999, Y: 0.009999999999999995
Training Loss: 1.3098
X: 0.039999999999999994, Y: 0.009999999999999995
Training Loss: 1.8869
X: 0.0475, Y: 0.009999999999999995
Training Loss: 2.4912
X: -0.05, Y: 0.0175
Training Loss: 2.4322
X: -0.0425, Y: 0.0175
Training Loss: 2.1093
X: -0.035, Y: 0.0175
Training Loss: 1.8619
X: -0.027500000000000004, Y: 0.0175
Training Loss: 1.7480
X: -0.020000000000000004, Y: 0.0175
Training Loss: 1.5716
X: -0.012500000000000004, Y: 0.0175
Training Loss: 1.4164
X: -0.0050000000000000044, Y: 0.0175
Training Loss: 1.3121
X: 0.0024999999999999953, Y: 0.0175
Training Loss: 1.3028
X: 0.009999999999999995, Y: 0.0175
Training Loss: 1.2946
X: 0.0175, Y: 0.0175
Training Loss: 1.3126
X: 0.024999999999999994, Y: 0.0175
Training Loss: 1.3651
X: 0.03249999999999999, Y: 0.0175
Training Loss: 1.6981
X: 0.039999999999999994, Y: 0.0175
Training Loss: 2.2179
X: 0.0475, Y: 0.0175
Training Loss: 2.8353
X: -0.05, Y: 0.024999999999999994
Training Loss: 2.3686
X: -0.0425, Y: 0.024999999999999994
Training Loss: 2.0902
X: -0.035, Y: 0.024999999999999994
Training Loss: 1.9586
X: -0.027500000000000004, Y: 0.024999999999999994
Training Loss: 1.7878
X: -0.020000000000000004, Y: 0.024999999999999994
Training Loss: 1.6679
X: -0.012500000000000004, Y: 0.024999999999999994
Training Loss: 1.7026
X: -0.0050000000000000044, Y: 0.024999999999999994
Training Loss: 1.6167
X: 0.0024999999999999953, Y: 0.024999999999999994
Training Loss: 1.6380
X: 0.009999999999999995, Y: 0.024999999999999994
Training Loss: 1.9027
X: 0.0175, Y: 0.024999999999999994
Training Loss: 1.9027
X: 0.024999999999999994, Y: 0.024999999999999994
Training Loss: 2.0260
X: 0.03249999999999999, Y: 0.024999999999999994
Training Loss: 2.2495
X: 0.039999999999999994, Y: 0.024999999999999994
Training Loss: 2.5996
X: 0.0475, Y: 0.024999999999999994
Training Loss: 3.0362
X: -0.05, Y: 0.03249999999999999
Training Loss: 2.1027
X: -0.0425, Y: 0.03249999999999999
Training Loss: 2.0804
X: -0.035, Y: 0.03249999999999999
Training Loss: 1.9919
X: -0.027500000000000004, Y: 0.03249999999999999
Training Loss: 1.8234
X: -0.020000000000000004, Y: 0.03249999999999999
Training Loss: 1.8373
X: -0.012500000000000004, Y: 0.03249999999999999
Training Loss: 1.8884
X: -0.0050000000000000044, Y: 0.03249999999999999
Training Loss: 1.7169
X: 0.0024999999999999953, Y: 0.03249999999999999
Training Loss: 1.6635
X: 0.009999999999999995, Y: 0.03249999999999999
Training Loss: 2.0045
X: 0.0175, Y: 0.03249999999999999
Training Loss: 2.1546
X: 0.024999999999999994, Y: 0.03249999999999999
Training Loss: 2.1933
X: 0.03249999999999999, Y: 0.03249999999999999
Training Loss: 2.4754
X: 0.039999999999999994, Y: 0.03249999999999999
Training Loss: 2.7102
X: 0.0475, Y: 0.03249999999999999
Training Loss: 3.1720
X: -0.05, Y: 0.039999999999999994
Training Loss: 1.9838
X: -0.0425, Y: 0.039999999999999994
Training Loss: 2.0923
X: -0.035, Y: 0.039999999999999994
Training Loss: 2.1192
X: -0.027500000000000004, Y: 0.039999999999999994
Training Loss: 1.9840
X: -0.020000000000000004, Y: 0.039999999999999994
Training Loss: 1.8424
X: -0.012500000000000004, Y: 0.039999999999999994
Training Loss: 1.7423
X: -0.0050000000000000044, Y: 0.039999999999999994
Training Loss: 1.7292
X: 0.0024999999999999953, Y: 0.039999999999999994
Training Loss: 1.6703
X: 0.009999999999999995, Y: 0.039999999999999994
Training Loss: 1.8286
X: 0.0175, Y: 0.039999999999999994
Training Loss: 2.0817
X: 0.024999999999999994, Y: 0.039999999999999994
Training Loss: 2.2721
X: 0.03249999999999999, Y: 0.039999999999999994
Training Loss: 2.3749
X: 0.039999999999999994, Y: 0.039999999999999994
Training Loss: 2.4985
X: 0.0475, Y: 0.039999999999999994
Training Loss: 2.8479
X: -0.05, Y: 0.0475
Training Loss: 2.4643
X: -0.0425, Y: 0.0475
Training Loss: 2.5280
X: -0.035, Y: 0.0475
Training Loss: 2.4844
X: -0.027500000000000004, Y: 0.0475
Training Loss: 2.2105
X: -0.020000000000000004, Y: 0.0475
Training Loss: 1.9250
X: -0.012500000000000004, Y: 0.0475
Training Loss: 1.9005
X: -0.0050000000000000044, Y: 0.0475
Training Loss: 1.8109
X: 0.0024999999999999953, Y: 0.0475
Training Loss: 1.7080
X: 0.009999999999999995, Y: 0.0475
Training Loss: 1.6196
X: 0.0175, Y: 0.0475
Training Loss: 1.7145
X: 0.024999999999999994, Y: 0.0475
Training Loss: 2.1095
X: 0.03249999999999999, Y: 0.0475
Training Loss: 2.3312
X: 0.039999999999999994, Y: 0.0475
Training Loss: 2.4430
X: 0.0475, Y: 0.0475
Training Loss: 2.4860
Saved to runs/10_epochs_11_16_12_7_2022/new_data_ResNet_Random_Directions_random.ply
