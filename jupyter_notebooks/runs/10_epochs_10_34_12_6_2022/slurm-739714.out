getting data
Files already downloaded and verified
Files already downloaded and verified
training VGG
epoch: 0
Train Loss: 0.2947, Train Acc: 0.17
Test Loss: 0.1901, Test Acc: 0.30
epoch: 1
Train Loss: 0.1858, Train Acc: 0.32
Test Loss: 0.1419, Test Acc: 0.46
epoch: 2
Train Loss: 0.1616, Train Acc: 0.41
Test Loss: 0.1297, Test Acc: 0.50
epoch: 3
Train Loss: 0.1312, Train Acc: 0.54
Test Loss: 0.1098, Test Acc: 0.61
epoch: 4
Train Loss: 0.1255, Train Acc: 0.56
Test Loss: 0.0963, Test Acc: 0.66
epoch: 5
Train Loss: 0.1104, Train Acc: 0.61
Test Loss: 0.1080, Test Acc: 0.61
epoch: 6
Train Loss: 0.0959, Train Acc: 0.66
Test Loss: 0.0795, Test Acc: 0.72
epoch: 7
Train Loss: 0.0880, Train Acc: 0.69
Test Loss: 0.0839, Test Acc: 0.71
epoch: 8
Train Loss: 0.0798, Train Acc: 0.71
Test Loss: 0.0717, Test Acc: 0.75
epoch: 9
Train Loss: 0.0809, Train Acc: 0.72
Test Loss: 0.0711, Test Acc: 0.78
X: -0.1, Y: -0.1
Training Loss: 0.0633
X: -0.09000000000000001, Y: -0.1
Training Loss: 0.0687
X: -0.08000000000000002, Y: -0.1
Training Loss: 0.0719
X: -0.07000000000000002, Y: -0.1
Training Loss: 0.0758
X: -0.060000000000000026, Y: -0.1
Training Loss: 0.0718
X: -0.05000000000000003, Y: -0.1
Training Loss: 0.0719
X: -0.040000000000000036, Y: -0.1
Training Loss: 0.0734
X: -0.03000000000000004, Y: -0.1
Training Loss: 0.0682
X: -0.020000000000000046, Y: -0.1
Training Loss: 0.0742
X: -0.01000000000000005, Y: -0.1
Training Loss: 0.0695
X: -5.551115123125783e-17, Y: -0.1
Training Loss: 0.0711
X: 0.00999999999999994, Y: -0.1
Training Loss: 0.0699
X: 0.019999999999999934, Y: -0.1
Training Loss: 0.0620
X: 0.029999999999999943, Y: -0.1
Training Loss: 0.0695
X: 0.039999999999999925, Y: -0.1
Training Loss: 0.0660
X: 0.049999999999999906, Y: -0.1
Training Loss: 0.0686
X: 0.059999999999999915, Y: -0.1
Training Loss: 0.0679
X: 0.06999999999999992, Y: -0.1
Training Loss: 0.0720
X: 0.0799999999999999, Y: -0.1
Training Loss: 0.0719
X: 0.08999999999999989, Y: -0.1
Training Loss: 0.0744
X: 0.0999999999999999, Y: -0.1
Training Loss: 0.0643
X: -0.1, Y: -0.09000000000000001
Training Loss: 0.0733
X: -0.09000000000000001, Y: -0.09000000000000001
Training Loss: 0.0750
X: -0.08000000000000002, Y: -0.09000000000000001
Training Loss: 0.0714
X: -0.07000000000000002, Y: -0.09000000000000001
Training Loss: 0.0712
X: -0.060000000000000026, Y: -0.09000000000000001
Training Loss: 0.0699
X: -0.05000000000000003, Y: -0.09000000000000001
Training Loss: 0.0675
X: -0.040000000000000036, Y: -0.09000000000000001
Training Loss: 0.0619
X: -0.03000000000000004, Y: -0.09000000000000001
Training Loss: 0.0684
X: -0.020000000000000046, Y: -0.09000000000000001
Training Loss: 0.0694
X: -0.01000000000000005, Y: -0.09000000000000001
Training Loss: 0.0736
X: -5.551115123125783e-17, Y: -0.09000000000000001
Training Loss: 0.0736
X: 0.00999999999999994, Y: -0.09000000000000001
Training Loss: 0.0622
X: 0.019999999999999934, Y: -0.09000000000000001
Training Loss: 0.0739
X: 0.029999999999999943, Y: -0.09000000000000001
Training Loss: 0.0655
X: 0.039999999999999925, Y: -0.09000000000000001
Training Loss: 0.0640
X: 0.049999999999999906, Y: -0.09000000000000001
Training Loss: 0.0701
X: 0.059999999999999915, Y: -0.09000000000000001
Training Loss: 0.0658
X: 0.06999999999999992, Y: -0.09000000000000001
Training Loss: 0.0684
X: 0.0799999999999999, Y: -0.09000000000000001
Training Loss: 0.0710
X: 0.08999999999999989, Y: -0.09000000000000001
Training Loss: 0.0711
X: 0.0999999999999999, Y: -0.09000000000000001
Training Loss: 0.0686
X: -0.1, Y: -0.08000000000000002
Training Loss: 0.0644
X: -0.09000000000000001, Y: -0.08000000000000002
Training Loss: 0.0720
X: -0.08000000000000002, Y: -0.08000000000000002
Training Loss: 0.0663
X: -0.07000000000000002, Y: -0.08000000000000002
Training Loss: 0.0757
X: -0.060000000000000026, Y: -0.08000000000000002
Training Loss: 0.0633
X: -0.05000000000000003, Y: -0.08000000000000002
Training Loss: 0.0684
X: -0.040000000000000036, Y: -0.08000000000000002
Training Loss: 0.0711
X: -0.03000000000000004, Y: -0.08000000000000002
Training Loss: 0.0694
X: -0.020000000000000046, Y: -0.08000000000000002
Training Loss: 0.0608
X: -0.01000000000000005, Y: -0.08000000000000002
Training Loss: 0.0650
X: -5.551115123125783e-17, Y: -0.08000000000000002
Training Loss: 0.0632
X: 0.00999999999999994, Y: -0.08000000000000002
Training Loss: 0.0650
X: 0.019999999999999934, Y: -0.08000000000000002
Training Loss: 0.0704
X: 0.029999999999999943, Y: -0.08000000000000002
Training Loss: 0.0746
X: 0.039999999999999925, Y: -0.08000000000000002
Training Loss: 0.0698
X: 0.049999999999999906, Y: -0.08000000000000002
Training Loss: 0.0650
X: 0.059999999999999915, Y: -0.08000000000000002
Training Loss: 0.0621
X: 0.06999999999999992, Y: -0.08000000000000002
Training Loss: 0.0674
X: 0.0799999999999999, Y: -0.08000000000000002
Training Loss: 0.0679
X: 0.08999999999999989, Y: -0.08000000000000002
Training Loss: 0.0678
X: 0.0999999999999999, Y: -0.08000000000000002
Training Loss: 0.0655
X: -0.1, Y: -0.07000000000000002
Training Loss: 0.0644
X: -0.09000000000000001, Y: -0.07000000000000002
Training Loss: 0.0693
X: -0.08000000000000002, Y: -0.07000000000000002
Training Loss: 0.0647
X: -0.07000000000000002, Y: -0.07000000000000002
Training Loss: 0.0635
X: -0.060000000000000026, Y: -0.07000000000000002
Training Loss: 0.0691
X: -0.05000000000000003, Y: -0.07000000000000002
Training Loss: 0.0652
X: -0.040000000000000036, Y: -0.07000000000000002
Training Loss: 0.0628
X: -0.03000000000000004, Y: -0.07000000000000002
Training Loss: 0.0699
X: -0.020000000000000046, Y: -0.07000000000000002
Training Loss: 0.0713
X: -0.01000000000000005, Y: -0.07000000000000002
Training Loss: 0.0689
X: -5.551115123125783e-17, Y: -0.07000000000000002
Training Loss: 0.0731
X: 0.00999999999999994, Y: -0.07000000000000002
Training Loss: 0.0708
X: 0.019999999999999934, Y: -0.07000000000000002
Training Loss: 0.0746
X: 0.029999999999999943, Y: -0.07000000000000002
Training Loss: 0.0679
X: 0.039999999999999925, Y: -0.07000000000000002
Training Loss: 0.0703
X: 0.049999999999999906, Y: -0.07000000000000002
Training Loss: 0.0632
X: 0.059999999999999915, Y: -0.07000000000000002
Training Loss: 0.0740
X: 0.06999999999999992, Y: -0.07000000000000002
Training Loss: 0.0704
X: 0.0799999999999999, Y: -0.07000000000000002
Training Loss: 0.0695
X: 0.08999999999999989, Y: -0.07000000000000002
Training Loss: 0.0635
X: 0.0999999999999999, Y: -0.07000000000000002
Training Loss: 0.0690
X: -0.1, Y: -0.060000000000000026
Training Loss: 0.0713
X: -0.09000000000000001, Y: -0.060000000000000026
Training Loss: 0.0715
X: -0.08000000000000002, Y: -0.060000000000000026
Training Loss: 0.0673
X: -0.07000000000000002, Y: -0.060000000000000026
Training Loss: 0.0667
X: -0.060000000000000026, Y: -0.060000000000000026
Training Loss: 0.0624
X: -0.05000000000000003, Y: -0.060000000000000026
Training Loss: 0.0651
X: -0.040000000000000036, Y: -0.060000000000000026
Training Loss: 0.0633
X: -0.03000000000000004, Y: -0.060000000000000026
Training Loss: 0.0707
X: -0.020000000000000046, Y: -0.060000000000000026
Training Loss: 0.0677
X: -0.01000000000000005, Y: -0.060000000000000026
Training Loss: 0.0678
X: -5.551115123125783e-17, Y: -0.060000000000000026
Training Loss: 0.0731
X: 0.00999999999999994, Y: -0.060000000000000026
Training Loss: 0.0681
X: 0.019999999999999934, Y: -0.060000000000000026
Training Loss: 0.0661
X: 0.029999999999999943, Y: -0.060000000000000026
Training Loss: 0.0644
X: 0.039999999999999925, Y: -0.060000000000000026
Training Loss: 0.0679
X: 0.049999999999999906, Y: -0.060000000000000026
Training Loss: 0.0674
X: 0.059999999999999915, Y: -0.060000000000000026
Training Loss: 0.0679
X: 0.06999999999999992, Y: -0.060000000000000026
Training Loss: 0.0707
X: 0.0799999999999999, Y: -0.060000000000000026
Training Loss: 0.0738
X: 0.08999999999999989, Y: -0.060000000000000026
Training Loss: 0.0676
X: 0.0999999999999999, Y: -0.060000000000000026
Training Loss: 0.0641
X: -0.1, Y: -0.05000000000000003
Training Loss: 0.0684
X: -0.09000000000000001, Y: -0.05000000000000003
Training Loss: 0.0748
X: -0.08000000000000002, Y: -0.05000000000000003
Training Loss: 0.0653
X: -0.07000000000000002, Y: -0.05000000000000003
Training Loss: 0.0706
X: -0.060000000000000026, Y: -0.05000000000000003
Training Loss: 0.0692
X: -0.05000000000000003, Y: -0.05000000000000003
Training Loss: 0.0715
X: -0.040000000000000036, Y: -0.05000000000000003
Training Loss: 0.0631
X: -0.03000000000000004, Y: -0.05000000000000003
Training Loss: 0.0661
X: -0.020000000000000046, Y: -0.05000000000000003
Training Loss: 0.0629
X: -0.01000000000000005, Y: -0.05000000000000003
Training Loss: 0.0670
X: -5.551115123125783e-17, Y: -0.05000000000000003
Training Loss: 0.0751
X: 0.00999999999999994, Y: -0.05000000000000003
Training Loss: 0.0692
X: 0.019999999999999934, Y: -0.05000000000000003
Training Loss: 0.0684
X: 0.029999999999999943, Y: -0.05000000000000003
Training Loss: 0.0699
X: 0.039999999999999925, Y: -0.05000000000000003
Training Loss: 0.0717
X: 0.049999999999999906, Y: -0.05000000000000003
Training Loss: 0.0681
X: 0.059999999999999915, Y: -0.05000000000000003
Training Loss: 0.0666
X: 0.06999999999999992, Y: -0.05000000000000003
Training Loss: 0.0697
X: 0.0799999999999999, Y: -0.05000000000000003
Training Loss: 0.0690
X: 0.08999999999999989, Y: -0.05000000000000003
Training Loss: 0.0682
X: 0.0999999999999999, Y: -0.05000000000000003
Training Loss: 0.0657
X: -0.1, Y: -0.040000000000000036
Training Loss: 0.0717
X: -0.09000000000000001, Y: -0.040000000000000036
Training Loss: 0.0702
X: -0.08000000000000002, Y: -0.040000000000000036
Training Loss: 0.0690
X: -0.07000000000000002, Y: -0.040000000000000036
Training Loss: 0.0703
X: -0.060000000000000026, Y: -0.040000000000000036
Training Loss: 0.0691
X: -0.05000000000000003, Y: -0.040000000000000036
Training Loss: 0.0697
X: -0.040000000000000036, Y: -0.040000000000000036
Training Loss: 0.0700
X: -0.03000000000000004, Y: -0.040000000000000036
Training Loss: 0.0612
X: -0.020000000000000046, Y: -0.040000000000000036
Training Loss: 0.0696
X: -0.01000000000000005, Y: -0.040000000000000036
Training Loss: 0.0672
X: -5.551115123125783e-17, Y: -0.040000000000000036
Training Loss: 0.0721
X: 0.00999999999999994, Y: -0.040000000000000036
Training Loss: 0.0681
X: 0.019999999999999934, Y: -0.040000000000000036
Training Loss: 0.0685
X: 0.029999999999999943, Y: -0.040000000000000036
Training Loss: 0.0673
X: 0.039999999999999925, Y: -0.040000000000000036
Training Loss: 0.0630
X: 0.049999999999999906, Y: -0.040000000000000036
Training Loss: 0.0696
X: 0.059999999999999915, Y: -0.040000000000000036
Training Loss: 0.0626
X: 0.06999999999999992, Y: -0.040000000000000036
Training Loss: 0.0750
X: 0.0799999999999999, Y: -0.040000000000000036
Training Loss: 0.0673
X: 0.08999999999999989, Y: -0.040000000000000036
Training Loss: 0.0732
X: 0.0999999999999999, Y: -0.040000000000000036
Training Loss: 0.0675
X: -0.1, Y: -0.03000000000000004
Training Loss: 0.0697
X: -0.09000000000000001, Y: -0.03000000000000004
Training Loss: 0.0620
X: -0.08000000000000002, Y: -0.03000000000000004
Training Loss: 0.0635
X: -0.07000000000000002, Y: -0.03000000000000004
Training Loss: 0.0647
X: -0.060000000000000026, Y: -0.03000000000000004
Training Loss: 0.0710
X: -0.05000000000000003, Y: -0.03000000000000004
Training Loss: 0.0755
X: -0.040000000000000036, Y: -0.03000000000000004
Training Loss: 0.0661
X: -0.03000000000000004, Y: -0.03000000000000004
Training Loss: 0.0662
X: -0.020000000000000046, Y: -0.03000000000000004
Training Loss: 0.0680
X: -0.01000000000000005, Y: -0.03000000000000004
Training Loss: 0.0681
X: -5.551115123125783e-17, Y: -0.03000000000000004
Training Loss: 0.0730
X: 0.00999999999999994, Y: -0.03000000000000004
Training Loss: 0.0657
X: 0.019999999999999934, Y: -0.03000000000000004
Training Loss: 0.0701
X: 0.029999999999999943, Y: -0.03000000000000004
Training Loss: 0.0658
X: 0.039999999999999925, Y: -0.03000000000000004
Training Loss: 0.0716
X: 0.049999999999999906, Y: -0.03000000000000004
Training Loss: 0.0692
X: 0.059999999999999915, Y: -0.03000000000000004
Training Loss: 0.0655
X: 0.06999999999999992, Y: -0.03000000000000004
Training Loss: 0.0695
X: 0.0799999999999999, Y: -0.03000000000000004
Training Loss: 0.0676
X: 0.08999999999999989, Y: -0.03000000000000004
Training Loss: 0.0692
X: 0.0999999999999999, Y: -0.03000000000000004
Training Loss: 0.0713
X: -0.1, Y: -0.020000000000000046
Training Loss: 0.0718
X: -0.09000000000000001, Y: -0.020000000000000046
Training Loss: 0.0684
X: -0.08000000000000002, Y: -0.020000000000000046
Training Loss: 0.0638
X: -0.07000000000000002, Y: -0.020000000000000046
Training Loss: 0.0716
X: -0.060000000000000026, Y: -0.020000000000000046
Training Loss: 0.0680
X: -0.05000000000000003, Y: -0.020000000000000046
Training Loss: 0.0686
X: -0.040000000000000036, Y: -0.020000000000000046
Training Loss: 0.0662
X: -0.03000000000000004, Y: -0.020000000000000046
Training Loss: 0.0674
X: -0.020000000000000046, Y: -0.020000000000000046
Training Loss: 0.0665
X: -0.01000000000000005, Y: -0.020000000000000046
Training Loss: 0.0643
X: -5.551115123125783e-17, Y: -0.020000000000000046
Training Loss: 0.0667
X: 0.00999999999999994, Y: -0.020000000000000046
Training Loss: 0.0674
X: 0.019999999999999934, Y: -0.020000000000000046
Training Loss: 0.0713
X: 0.029999999999999943, Y: -0.020000000000000046
Training Loss: 0.0686
X: 0.039999999999999925, Y: -0.020000000000000046
Training Loss: 0.0689
X: 0.049999999999999906, Y: -0.020000000000000046
Training Loss: 0.0628
X: 0.059999999999999915, Y: -0.020000000000000046
Training Loss: 0.0690
X: 0.06999999999999992, Y: -0.020000000000000046
Training Loss: 0.0725
X: 0.0799999999999999, Y: -0.020000000000000046
Training Loss: 0.0727
X: 0.08999999999999989, Y: -0.020000000000000046
Training Loss: 0.0664
X: 0.0999999999999999, Y: -0.020000000000000046
Training Loss: 0.0722
X: -0.1, Y: -0.01000000000000005
Training Loss: 0.0684
X: -0.09000000000000001, Y: -0.01000000000000005
Training Loss: 0.0697
X: -0.08000000000000002, Y: -0.01000000000000005
Training Loss: 0.0668
X: -0.07000000000000002, Y: -0.01000000000000005
Training Loss: 0.0630
X: -0.060000000000000026, Y: -0.01000000000000005
Training Loss: 0.0699
X: -0.05000000000000003, Y: -0.01000000000000005
Training Loss: 0.0681
X: -0.040000000000000036, Y: -0.01000000000000005
Training Loss: 0.0704
X: -0.03000000000000004, Y: -0.01000000000000005
Training Loss: 0.0712
X: -0.020000000000000046, Y: -0.01000000000000005
Training Loss: 0.0663
X: -0.01000000000000005, Y: -0.01000000000000005
Training Loss: 0.0720
X: -5.551115123125783e-17, Y: -0.01000000000000005
Training Loss: 0.0621
X: 0.00999999999999994, Y: -0.01000000000000005
Training Loss: 0.0720
X: 0.019999999999999934, Y: -0.01000000000000005
Training Loss: 0.0712
X: 0.029999999999999943, Y: -0.01000000000000005
Training Loss: 0.0672
X: 0.039999999999999925, Y: -0.01000000000000005
Training Loss: 0.0703
X: 0.049999999999999906, Y: -0.01000000000000005
Training Loss: 0.0649
X: 0.059999999999999915, Y: -0.01000000000000005
Training Loss: 0.0745
X: 0.06999999999999992, Y: -0.01000000000000005
Training Loss: 0.0739
X: 0.0799999999999999, Y: -0.01000000000000005
Training Loss: 0.0705
X: 0.08999999999999989, Y: -0.01000000000000005
Training Loss: 0.0637
X: 0.0999999999999999, Y: -0.01000000000000005
Training Loss: 0.0620
X: -0.1, Y: -5.551115123125783e-17
Training Loss: 0.0770
X: -0.09000000000000001, Y: -5.551115123125783e-17
Training Loss: 0.0631
X: -0.08000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0727
X: -0.07000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0666
X: -0.060000000000000026, Y: -5.551115123125783e-17
Training Loss: 0.0688
X: -0.05000000000000003, Y: -5.551115123125783e-17
Training Loss: 0.0750
X: -0.040000000000000036, Y: -5.551115123125783e-17
Training Loss: 0.0757
X: -0.03000000000000004, Y: -5.551115123125783e-17
Training Loss: 0.0681
X: -0.020000000000000046, Y: -5.551115123125783e-17
Training Loss: 0.0641
X: -0.01000000000000005, Y: -5.551115123125783e-17
Training Loss: 0.0690
X: -5.551115123125783e-17, Y: -5.551115123125783e-17
Training Loss: 0.0677
X: 0.00999999999999994, Y: -5.551115123125783e-17
Training Loss: 0.0708
X: 0.019999999999999934, Y: -5.551115123125783e-17
Training Loss: 0.0683
X: 0.029999999999999943, Y: -5.551115123125783e-17
Training Loss: 0.0723
X: 0.039999999999999925, Y: -5.551115123125783e-17
Training Loss: 0.0695
X: 0.049999999999999906, Y: -5.551115123125783e-17
Training Loss: 0.0681
X: 0.059999999999999915, Y: -5.551115123125783e-17
Training Loss: 0.0711
X: 0.06999999999999992, Y: -5.551115123125783e-17
Training Loss: 0.0664
X: 0.0799999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0649
X: 0.08999999999999989, Y: -5.551115123125783e-17
Training Loss: 0.0682
X: 0.0999999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0756
X: -0.1, Y: 0.00999999999999994
Training Loss: 0.0793
X: -0.09000000000000001, Y: 0.00999999999999994
Training Loss: 0.0671
X: -0.08000000000000002, Y: 0.00999999999999994
Training Loss: 0.0704
X: -0.07000000000000002, Y: 0.00999999999999994
Training Loss: 0.0725
X: -0.060000000000000026, Y: 0.00999999999999994
Training Loss: 0.0720
X: -0.05000000000000003, Y: 0.00999999999999994
Training Loss: 0.0684
X: -0.040000000000000036, Y: 0.00999999999999994
Training Loss: 0.0718
X: -0.03000000000000004, Y: 0.00999999999999994
Training Loss: 0.0735
X: -0.020000000000000046, Y: 0.00999999999999994
Training Loss: 0.0712
X: -0.01000000000000005, Y: 0.00999999999999994
Training Loss: 0.0666
X: -5.551115123125783e-17, Y: 0.00999999999999994
Training Loss: 0.0683
X: 0.00999999999999994, Y: 0.00999999999999994
Training Loss: 0.0698
X: 0.019999999999999934, Y: 0.00999999999999994
Training Loss: 0.0683
X: 0.029999999999999943, Y: 0.00999999999999994
Training Loss: 0.0681
X: 0.039999999999999925, Y: 0.00999999999999994
Training Loss: 0.0623
X: 0.049999999999999906, Y: 0.00999999999999994
Training Loss: 0.0689
X: 0.059999999999999915, Y: 0.00999999999999994
Training Loss: 0.0660
X: 0.06999999999999992, Y: 0.00999999999999994
Training Loss: 0.0726
X: 0.0799999999999999, Y: 0.00999999999999994
Training Loss: 0.0637
X: 0.08999999999999989, Y: 0.00999999999999994
Training Loss: 0.0684
X: 0.0999999999999999, Y: 0.00999999999999994
Training Loss: 0.0695
X: -0.1, Y: 0.019999999999999934
Training Loss: 0.0722
X: -0.09000000000000001, Y: 0.019999999999999934
Training Loss: 0.0703
X: -0.08000000000000002, Y: 0.019999999999999934
Training Loss: 0.0710
X: -0.07000000000000002, Y: 0.019999999999999934
Training Loss: 0.0693
X: -0.060000000000000026, Y: 0.019999999999999934
Training Loss: 0.0665
X: -0.05000000000000003, Y: 0.019999999999999934
Training Loss: 0.0635
X: -0.040000000000000036, Y: 0.019999999999999934
Training Loss: 0.0635
X: -0.03000000000000004, Y: 0.019999999999999934
Training Loss: 0.0714
X: -0.020000000000000046, Y: 0.019999999999999934
Training Loss: 0.0676
X: -0.01000000000000005, Y: 0.019999999999999934
Training Loss: 0.0656
X: -5.551115123125783e-17, Y: 0.019999999999999934
Training Loss: 0.0724
X: 0.00999999999999994, Y: 0.019999999999999934
Training Loss: 0.0677
X: 0.019999999999999934, Y: 0.019999999999999934
Training Loss: 0.0687
X: 0.029999999999999943, Y: 0.019999999999999934
Training Loss: 0.0601
X: 0.039999999999999925, Y: 0.019999999999999934
Training Loss: 0.0656
X: 0.049999999999999906, Y: 0.019999999999999934
Training Loss: 0.0726
X: 0.059999999999999915, Y: 0.019999999999999934
Training Loss: 0.0706
X: 0.06999999999999992, Y: 0.019999999999999934
Training Loss: 0.0742
X: 0.0799999999999999, Y: 0.019999999999999934
Training Loss: 0.0693
X: 0.08999999999999989, Y: 0.019999999999999934
Training Loss: 0.0655
X: 0.0999999999999999, Y: 0.019999999999999934
Training Loss: 0.0645
X: -0.1, Y: 0.029999999999999943
Training Loss: 0.0708
X: -0.09000000000000001, Y: 0.029999999999999943
Training Loss: 0.0744
X: -0.08000000000000002, Y: 0.029999999999999943
Training Loss: 0.0650
X: -0.07000000000000002, Y: 0.029999999999999943
Training Loss: 0.0736
X: -0.060000000000000026, Y: 0.029999999999999943
Training Loss: 0.0678
X: -0.05000000000000003, Y: 0.029999999999999943
Training Loss: 0.0730
X: -0.040000000000000036, Y: 0.029999999999999943
Training Loss: 0.0684
X: -0.03000000000000004, Y: 0.029999999999999943
Training Loss: 0.0720
X: -0.020000000000000046, Y: 0.029999999999999943
Training Loss: 0.0620
X: -0.01000000000000005, Y: 0.029999999999999943
Training Loss: 0.0685
X: -5.551115123125783e-17, Y: 0.029999999999999943
Training Loss: 0.0704
X: 0.00999999999999994, Y: 0.029999999999999943
Training Loss: 0.0689
X: 0.019999999999999934, Y: 0.029999999999999943
Training Loss: 0.0750
X: 0.029999999999999943, Y: 0.029999999999999943
Training Loss: 0.0649
X: 0.039999999999999925, Y: 0.029999999999999943
Training Loss: 0.0683
X: 0.049999999999999906, Y: 0.029999999999999943
Training Loss: 0.0669
X: 0.059999999999999915, Y: 0.029999999999999943
Training Loss: 0.0683
X: 0.06999999999999992, Y: 0.029999999999999943
Training Loss: 0.0697
X: 0.0799999999999999, Y: 0.029999999999999943
Training Loss: 0.0700
X: 0.08999999999999989, Y: 0.029999999999999943
Training Loss: 0.0648
X: 0.0999999999999999, Y: 0.029999999999999943
Training Loss: 0.0692
X: -0.1, Y: 0.039999999999999925
Training Loss: 0.0645
X: -0.09000000000000001, Y: 0.039999999999999925
Training Loss: 0.0626
X: -0.08000000000000002, Y: 0.039999999999999925
Training Loss: 0.0689
X: -0.07000000000000002, Y: 0.039999999999999925
Training Loss: 0.0660
X: -0.060000000000000026, Y: 0.039999999999999925
Training Loss: 0.0701
X: -0.05000000000000003, Y: 0.039999999999999925
Training Loss: 0.0770
X: -0.040000000000000036, Y: 0.039999999999999925
Training Loss: 0.0698
X: -0.03000000000000004, Y: 0.039999999999999925
Training Loss: 0.0722
X: -0.020000000000000046, Y: 0.039999999999999925
Training Loss: 0.0670
X: -0.01000000000000005, Y: 0.039999999999999925
Training Loss: 0.0725
X: -5.551115123125783e-17, Y: 0.039999999999999925
Training Loss: 0.0658
X: 0.00999999999999994, Y: 0.039999999999999925
Training Loss: 0.0697
X: 0.019999999999999934, Y: 0.039999999999999925
Training Loss: 0.0724
X: 0.029999999999999943, Y: 0.039999999999999925
Training Loss: 0.0702
X: 0.039999999999999925, Y: 0.039999999999999925
Training Loss: 0.0696
X: 0.049999999999999906, Y: 0.039999999999999925
Training Loss: 0.0667
X: 0.059999999999999915, Y: 0.039999999999999925
Training Loss: 0.0750
X: 0.06999999999999992, Y: 0.039999999999999925
Training Loss: 0.0643
X: 0.0799999999999999, Y: 0.039999999999999925
Training Loss: 0.0598
X: 0.08999999999999989, Y: 0.039999999999999925
Training Loss: 0.0697
X: 0.0999999999999999, Y: 0.039999999999999925
Training Loss: 0.0674
X: -0.1, Y: 0.049999999999999906
Training Loss: 0.0739
X: -0.09000000000000001, Y: 0.049999999999999906
Training Loss: 0.0757
X: -0.08000000000000002, Y: 0.049999999999999906
Training Loss: 0.0754
X: -0.07000000000000002, Y: 0.049999999999999906
Training Loss: 0.0683
X: -0.060000000000000026, Y: 0.049999999999999906
Training Loss: 0.0641
X: -0.05000000000000003, Y: 0.049999999999999906
Training Loss: 0.0688
X: -0.040000000000000036, Y: 0.049999999999999906
Training Loss: 0.0704
X: -0.03000000000000004, Y: 0.049999999999999906
Training Loss: 0.0688
X: -0.020000000000000046, Y: 0.049999999999999906
Training Loss: 0.0794
X: -0.01000000000000005, Y: 0.049999999999999906
Training Loss: 0.0667
X: -5.551115123125783e-17, Y: 0.049999999999999906
Training Loss: 0.0711
X: 0.00999999999999994, Y: 0.049999999999999906
Training Loss: 0.0690
X: 0.019999999999999934, Y: 0.049999999999999906
Training Loss: 0.0646
X: 0.029999999999999943, Y: 0.049999999999999906
Training Loss: 0.0694
X: 0.039999999999999925, Y: 0.049999999999999906
Training Loss: 0.0684
X: 0.049999999999999906, Y: 0.049999999999999906
Training Loss: 0.0594
X: 0.059999999999999915, Y: 0.049999999999999906
Training Loss: 0.0718
X: 0.06999999999999992, Y: 0.049999999999999906
Training Loss: 0.0688
X: 0.0799999999999999, Y: 0.049999999999999906
Training Loss: 0.0613
X: 0.08999999999999989, Y: 0.049999999999999906
Training Loss: 0.0624
X: 0.0999999999999999, Y: 0.049999999999999906
Training Loss: 0.0700
X: -0.1, Y: 0.059999999999999915
Training Loss: 0.0690
X: -0.09000000000000001, Y: 0.059999999999999915
Training Loss: 0.0668
X: -0.08000000000000002, Y: 0.059999999999999915
Training Loss: 0.0697
X: -0.07000000000000002, Y: 0.059999999999999915
Training Loss: 0.0715
X: -0.060000000000000026, Y: 0.059999999999999915
Training Loss: 0.0710
X: -0.05000000000000003, Y: 0.059999999999999915
Training Loss: 0.0741
X: -0.040000000000000036, Y: 0.059999999999999915
Training Loss: 0.0682
X: -0.03000000000000004, Y: 0.059999999999999915
Training Loss: 0.0636
X: -0.020000000000000046, Y: 0.059999999999999915
Training Loss: 0.0677
X: -0.01000000000000005, Y: 0.059999999999999915
Training Loss: 0.0678
X: -5.551115123125783e-17, Y: 0.059999999999999915
Training Loss: 0.0699
X: 0.00999999999999994, Y: 0.059999999999999915
Training Loss: 0.0704
X: 0.019999999999999934, Y: 0.059999999999999915
Training Loss: 0.0667
X: 0.029999999999999943, Y: 0.059999999999999915
Training Loss: 0.0684
X: 0.039999999999999925, Y: 0.059999999999999915
Training Loss: 0.0621
X: 0.049999999999999906, Y: 0.059999999999999915
Training Loss: 0.0655
X: 0.059999999999999915, Y: 0.059999999999999915
Training Loss: 0.0731
X: 0.06999999999999992, Y: 0.059999999999999915
Training Loss: 0.0685
X: 0.0799999999999999, Y: 0.059999999999999915
Training Loss: 0.0677
X: 0.08999999999999989, Y: 0.059999999999999915
Training Loss: 0.0695
X: 0.0999999999999999, Y: 0.059999999999999915
Training Loss: 0.0623
X: -0.1, Y: 0.06999999999999992
Training Loss: 0.0652
X: -0.09000000000000001, Y: 0.06999999999999992
Training Loss: 0.0723
X: -0.08000000000000002, Y: 0.06999999999999992
Training Loss: 0.0668
X: -0.07000000000000002, Y: 0.06999999999999992
Training Loss: 0.0695
X: -0.060000000000000026, Y: 0.06999999999999992
Training Loss: 0.0669
X: -0.05000000000000003, Y: 0.06999999999999992
Training Loss: 0.0646
X: -0.040000000000000036, Y: 0.06999999999999992
Training Loss: 0.0700
X: -0.03000000000000004, Y: 0.06999999999999992
Training Loss: 0.0689
X: -0.020000000000000046, Y: 0.06999999999999992
Training Loss: 0.0605
X: -0.01000000000000005, Y: 0.06999999999999992
Training Loss: 0.0666
X: -5.551115123125783e-17, Y: 0.06999999999999992
Training Loss: 0.0678
X: 0.00999999999999994, Y: 0.06999999999999992
Training Loss: 0.0696
X: 0.019999999999999934, Y: 0.06999999999999992
Training Loss: 0.0707
X: 0.029999999999999943, Y: 0.06999999999999992
Training Loss: 0.0674
X: 0.039999999999999925, Y: 0.06999999999999992
Training Loss: 0.0750
X: 0.049999999999999906, Y: 0.06999999999999992
Training Loss: 0.0621
X: 0.059999999999999915, Y: 0.06999999999999992
Training Loss: 0.0667
X: 0.06999999999999992, Y: 0.06999999999999992
Training Loss: 0.0647
X: 0.0799999999999999, Y: 0.06999999999999992
Training Loss: 0.0709
X: 0.08999999999999989, Y: 0.06999999999999992
Training Loss: 0.0683
X: 0.0999999999999999, Y: 0.06999999999999992
Training Loss: 0.0749
X: -0.1, Y: 0.0799999999999999
Training Loss: 0.0679
X: -0.09000000000000001, Y: 0.0799999999999999
Training Loss: 0.0693
X: -0.08000000000000002, Y: 0.0799999999999999
Training Loss: 0.0785
X: -0.07000000000000002, Y: 0.0799999999999999
Training Loss: 0.0716
X: -0.060000000000000026, Y: 0.0799999999999999
Training Loss: 0.0715
X: -0.05000000000000003, Y: 0.0799999999999999
Training Loss: 0.0678
X: -0.040000000000000036, Y: 0.0799999999999999
Training Loss: 0.0700
X: -0.03000000000000004, Y: 0.0799999999999999
Training Loss: 0.0747
X: -0.020000000000000046, Y: 0.0799999999999999
Training Loss: 0.0704
X: -0.01000000000000005, Y: 0.0799999999999999
Training Loss: 0.0722
X: -5.551115123125783e-17, Y: 0.0799999999999999
Training Loss: 0.0683
X: 0.00999999999999994, Y: 0.0799999999999999
Training Loss: 0.0643
X: 0.019999999999999934, Y: 0.0799999999999999
Training Loss: 0.0702
X: 0.029999999999999943, Y: 0.0799999999999999
Training Loss: 0.0668
X: 0.039999999999999925, Y: 0.0799999999999999
Training Loss: 0.0676
X: 0.049999999999999906, Y: 0.0799999999999999
Training Loss: 0.0704
X: 0.059999999999999915, Y: 0.0799999999999999
Training Loss: 0.0711
X: 0.06999999999999992, Y: 0.0799999999999999
Training Loss: 0.0646
X: 0.0799999999999999, Y: 0.0799999999999999
Training Loss: 0.0713
X: 0.08999999999999989, Y: 0.0799999999999999
Training Loss: 0.0634
X: 0.0999999999999999, Y: 0.0799999999999999
Training Loss: 0.0693
X: -0.1, Y: 0.08999999999999989
Training Loss: 0.0663
X: -0.09000000000000001, Y: 0.08999999999999989
Training Loss: 0.0667
X: -0.08000000000000002, Y: 0.08999999999999989
Training Loss: 0.0662
X: -0.07000000000000002, Y: 0.08999999999999989
Training Loss: 0.0677
X: -0.060000000000000026, Y: 0.08999999999999989
Training Loss: 0.0683
X: -0.05000000000000003, Y: 0.08999999999999989
Training Loss: 0.0668
X: -0.040000000000000036, Y: 0.08999999999999989
Training Loss: 0.0670
X: -0.03000000000000004, Y: 0.08999999999999989
Training Loss: 0.0682
X: -0.020000000000000046, Y: 0.08999999999999989
Training Loss: 0.0703
X: -0.01000000000000005, Y: 0.08999999999999989
Training Loss: 0.0694
X: -5.551115123125783e-17, Y: 0.08999999999999989
Training Loss: 0.0656
X: 0.00999999999999994, Y: 0.08999999999999989
Training Loss: 0.0675
X: 0.019999999999999934, Y: 0.08999999999999989
Training Loss: 0.0682
X: 0.029999999999999943, Y: 0.08999999999999989
Training Loss: 0.0686
X: 0.039999999999999925, Y: 0.08999999999999989
Training Loss: 0.0668
X: 0.049999999999999906, Y: 0.08999999999999989
Training Loss: 0.0711
X: 0.059999999999999915, Y: 0.08999999999999989
Training Loss: 0.0680
X: 0.06999999999999992, Y: 0.08999999999999989
Training Loss: 0.0648
X: 0.0799999999999999, Y: 0.08999999999999989
Training Loss: 0.0645
X: 0.08999999999999989, Y: 0.08999999999999989
Training Loss: 0.0687
X: 0.0999999999999999, Y: 0.08999999999999989
Training Loss: 0.0704
X: -0.1, Y: 0.0999999999999999
Training Loss: 0.0618
X: -0.09000000000000001, Y: 0.0999999999999999
Training Loss: 0.0666
X: -0.08000000000000002, Y: 0.0999999999999999
Training Loss: 0.0708
X: -0.07000000000000002, Y: 0.0999999999999999
Training Loss: 0.0723
X: -0.060000000000000026, Y: 0.0999999999999999
Training Loss: 0.0644
X: -0.05000000000000003, Y: 0.0999999999999999
Training Loss: 0.0747
X: -0.040000000000000036, Y: 0.0999999999999999
Training Loss: 0.0698
X: -0.03000000000000004, Y: 0.0999999999999999
Training Loss: 0.0708
X: -0.020000000000000046, Y: 0.0999999999999999
Training Loss: 0.0657
X: -0.01000000000000005, Y: 0.0999999999999999
Training Loss: 0.0644
X: -5.551115123125783e-17, Y: 0.0999999999999999
Training Loss: 0.0667
X: 0.00999999999999994, Y: 0.0999999999999999
Training Loss: 0.0707
X: 0.019999999999999934, Y: 0.0999999999999999
Training Loss: 0.0685
X: 0.029999999999999943, Y: 0.0999999999999999
Training Loss: 0.0585
X: 0.039999999999999925, Y: 0.0999999999999999
Training Loss: 0.0657
X: 0.049999999999999906, Y: 0.0999999999999999
Training Loss: 0.0684
X: 0.059999999999999915, Y: 0.0999999999999999
Training Loss: 0.0674
X: 0.06999999999999992, Y: 0.0999999999999999
Training Loss: 0.0688
X: 0.0799999999999999, Y: 0.0999999999999999
Training Loss: 0.0739
X: 0.08999999999999989, Y: 0.0999999999999999
Training Loss: 0.0658
X: 0.0999999999999999, Y: 0.0999999999999999
Training Loss: 0.0659
Saved to runs/10_epochs_10:34_12_6_2022/new_data_YGG_PCA_pca.ply
X: -0.1, Y: -0.1
Training Loss: 0.0761
X: -0.09000000000000001, Y: -0.1
Training Loss: 0.0707
X: -0.08000000000000002, Y: -0.1
Training Loss: 0.0678
X: -0.07000000000000002, Y: -0.1
Training Loss: 0.0725
X: -0.060000000000000026, Y: -0.1
Training Loss: 0.0678
X: -0.05000000000000003, Y: -0.1
Training Loss: 0.0682
X: -0.040000000000000036, Y: -0.1
Training Loss: 0.0753
X: -0.03000000000000004, Y: -0.1
Training Loss: 0.0698
X: -0.020000000000000046, Y: -0.1
Training Loss: 0.0664
X: -0.01000000000000005, Y: -0.1
Training Loss: 0.0713
X: -5.551115123125783e-17, Y: -0.1
Training Loss: 0.0747
X: 0.00999999999999994, Y: -0.1
Training Loss: 0.0671
X: 0.019999999999999934, Y: -0.1
Training Loss: 0.0813
X: 0.029999999999999943, Y: -0.1
Training Loss: 0.0734
X: 0.039999999999999925, Y: -0.1
Training Loss: 0.0735
X: 0.049999999999999906, Y: -0.1
Training Loss: 0.0723
X: 0.059999999999999915, Y: -0.1
Training Loss: 0.0752
X: 0.06999999999999992, Y: -0.1
Training Loss: 0.0835
X: 0.0799999999999999, Y: -0.1
Training Loss: 0.0863
X: 0.08999999999999989, Y: -0.1
Training Loss: 0.0820
X: 0.0999999999999999, Y: -0.1
Training Loss: 0.0864
X: -0.1, Y: -0.09000000000000001
Training Loss: 0.0685
X: -0.09000000000000001, Y: -0.09000000000000001
Training Loss: 0.0706
X: -0.08000000000000002, Y: -0.09000000000000001
Training Loss: 0.0741
X: -0.07000000000000002, Y: -0.09000000000000001
Training Loss: 0.0670
X: -0.060000000000000026, Y: -0.09000000000000001
Training Loss: 0.0683
X: -0.05000000000000003, Y: -0.09000000000000001
Training Loss: 0.0703
X: -0.040000000000000036, Y: -0.09000000000000001
Training Loss: 0.0662
X: -0.03000000000000004, Y: -0.09000000000000001
Training Loss: 0.0727
X: -0.020000000000000046, Y: -0.09000000000000001
Training Loss: 0.0709
X: -0.01000000000000005, Y: -0.09000000000000001
Training Loss: 0.0729
X: -5.551115123125783e-17, Y: -0.09000000000000001
Training Loss: 0.0719
X: 0.00999999999999994, Y: -0.09000000000000001
Training Loss: 0.0650
X: 0.019999999999999934, Y: -0.09000000000000001
Training Loss: 0.0738
X: 0.029999999999999943, Y: -0.09000000000000001
Training Loss: 0.0760
X: 0.039999999999999925, Y: -0.09000000000000001
Training Loss: 0.0752
X: 0.049999999999999906, Y: -0.09000000000000001
Training Loss: 0.0778
X: 0.059999999999999915, Y: -0.09000000000000001
Training Loss: 0.0819
X: 0.06999999999999992, Y: -0.09000000000000001
Training Loss: 0.0813
X: 0.0799999999999999, Y: -0.09000000000000001
Training Loss: 0.0884
X: 0.08999999999999989, Y: -0.09000000000000001
Training Loss: 0.0857
X: 0.0999999999999999, Y: -0.09000000000000001
Training Loss: 0.0872
X: -0.1, Y: -0.08000000000000002
Training Loss: 0.0724
X: -0.09000000000000001, Y: -0.08000000000000002
Training Loss: 0.0693
X: -0.08000000000000002, Y: -0.08000000000000002
Training Loss: 0.0660
X: -0.07000000000000002, Y: -0.08000000000000002
Training Loss: 0.0734
X: -0.060000000000000026, Y: -0.08000000000000002
Training Loss: 0.0713
X: -0.05000000000000003, Y: -0.08000000000000002
Training Loss: 0.0653
X: -0.040000000000000036, Y: -0.08000000000000002
Training Loss: 0.0717
X: -0.03000000000000004, Y: -0.08000000000000002
Training Loss: 0.0673
X: -0.020000000000000046, Y: -0.08000000000000002
Training Loss: 0.0673
X: -0.01000000000000005, Y: -0.08000000000000002
Training Loss: 0.0653
X: -5.551115123125783e-17, Y: -0.08000000000000002
Training Loss: 0.0767
X: 0.00999999999999994, Y: -0.08000000000000002
Training Loss: 0.0728
X: 0.019999999999999934, Y: -0.08000000000000002
Training Loss: 0.0721
X: 0.029999999999999943, Y: -0.08000000000000002
Training Loss: 0.0728
X: 0.039999999999999925, Y: -0.08000000000000002
Training Loss: 0.0699
X: 0.049999999999999906, Y: -0.08000000000000002
Training Loss: 0.0767
X: 0.059999999999999915, Y: -0.08000000000000002
Training Loss: 0.0819
X: 0.06999999999999992, Y: -0.08000000000000002
Training Loss: 0.0891
X: 0.0799999999999999, Y: -0.08000000000000002
Training Loss: 0.0811
X: 0.08999999999999989, Y: -0.08000000000000002
Training Loss: 0.0838
X: 0.0999999999999999, Y: -0.08000000000000002
Training Loss: 0.0847
X: -0.1, Y: -0.07000000000000002
Training Loss: 0.0778
X: -0.09000000000000001, Y: -0.07000000000000002
Training Loss: 0.0737
X: -0.08000000000000002, Y: -0.07000000000000002
Training Loss: 0.0718
X: -0.07000000000000002, Y: -0.07000000000000002
Training Loss: 0.0736
X: -0.060000000000000026, Y: -0.07000000000000002
Training Loss: 0.0728
X: -0.05000000000000003, Y: -0.07000000000000002
Training Loss: 0.0718
X: -0.040000000000000036, Y: -0.07000000000000002
Training Loss: 0.0683
X: -0.03000000000000004, Y: -0.07000000000000002
Training Loss: 0.0680
X: -0.020000000000000046, Y: -0.07000000000000002
Training Loss: 0.0648
X: -0.01000000000000005, Y: -0.07000000000000002
Training Loss: 0.0626
X: -5.551115123125783e-17, Y: -0.07000000000000002
Training Loss: 0.0738
X: 0.00999999999999994, Y: -0.07000000000000002
Training Loss: 0.0747
X: 0.019999999999999934, Y: -0.07000000000000002
Training Loss: 0.0755
X: 0.029999999999999943, Y: -0.07000000000000002
Training Loss: 0.0746
X: 0.039999999999999925, Y: -0.07000000000000002
Training Loss: 0.0766
X: 0.049999999999999906, Y: -0.07000000000000002
Training Loss: 0.0692
X: 0.059999999999999915, Y: -0.07000000000000002
Training Loss: 0.0792
X: 0.06999999999999992, Y: -0.07000000000000002
Training Loss: 0.0778
X: 0.0799999999999999, Y: -0.07000000000000002
Training Loss: 0.0824
X: 0.08999999999999989, Y: -0.07000000000000002
Training Loss: 0.0784
X: 0.0999999999999999, Y: -0.07000000000000002
Training Loss: 0.0853
X: -0.1, Y: -0.060000000000000026
Training Loss: 0.0765
X: -0.09000000000000001, Y: -0.060000000000000026
Training Loss: 0.0775
X: -0.08000000000000002, Y: -0.060000000000000026
Training Loss: 0.0736
X: -0.07000000000000002, Y: -0.060000000000000026
Training Loss: 0.0703
X: -0.060000000000000026, Y: -0.060000000000000026
Training Loss: 0.0692
X: -0.05000000000000003, Y: -0.060000000000000026
Training Loss: 0.0662
X: -0.040000000000000036, Y: -0.060000000000000026
Training Loss: 0.0704
X: -0.03000000000000004, Y: -0.060000000000000026
Training Loss: 0.0633
X: -0.020000000000000046, Y: -0.060000000000000026
Training Loss: 0.0652
X: -0.01000000000000005, Y: -0.060000000000000026
Training Loss: 0.0697
X: -5.551115123125783e-17, Y: -0.060000000000000026
Training Loss: 0.0747
X: 0.00999999999999994, Y: -0.060000000000000026
Training Loss: 0.0695
X: 0.019999999999999934, Y: -0.060000000000000026
Training Loss: 0.0673
X: 0.029999999999999943, Y: -0.060000000000000026
Training Loss: 0.0738
X: 0.039999999999999925, Y: -0.060000000000000026
Training Loss: 0.0693
X: 0.049999999999999906, Y: -0.060000000000000026
Training Loss: 0.0693
X: 0.059999999999999915, Y: -0.060000000000000026
Training Loss: 0.0727
X: 0.06999999999999992, Y: -0.060000000000000026
Training Loss: 0.0782
X: 0.0799999999999999, Y: -0.060000000000000026
Training Loss: 0.0806
X: 0.08999999999999989, Y: -0.060000000000000026
Training Loss: 0.0795
X: 0.0999999999999999, Y: -0.060000000000000026
Training Loss: 0.0852
X: -0.1, Y: -0.05000000000000003
Training Loss: 0.0751
X: -0.09000000000000001, Y: -0.05000000000000003
Training Loss: 0.0701
X: -0.08000000000000002, Y: -0.05000000000000003
Training Loss: 0.0708
X: -0.07000000000000002, Y: -0.05000000000000003
Training Loss: 0.0711
X: -0.060000000000000026, Y: -0.05000000000000003
Training Loss: 0.0674
X: -0.05000000000000003, Y: -0.05000000000000003
Training Loss: 0.0705
X: -0.040000000000000036, Y: -0.05000000000000003
Training Loss: 0.0708
X: -0.03000000000000004, Y: -0.05000000000000003
Training Loss: 0.0694
X: -0.020000000000000046, Y: -0.05000000000000003
Training Loss: 0.0636
X: -0.01000000000000005, Y: -0.05000000000000003
Training Loss: 0.0685
X: -5.551115123125783e-17, Y: -0.05000000000000003
Training Loss: 0.0671
X: 0.00999999999999994, Y: -0.05000000000000003
Training Loss: 0.0675
X: 0.019999999999999934, Y: -0.05000000000000003
Training Loss: 0.0719
X: 0.029999999999999943, Y: -0.05000000000000003
Training Loss: 0.0724
X: 0.039999999999999925, Y: -0.05000000000000003
Training Loss: 0.0719
X: 0.049999999999999906, Y: -0.05000000000000003
Training Loss: 0.0742
X: 0.059999999999999915, Y: -0.05000000000000003
Training Loss: 0.0789
X: 0.06999999999999992, Y: -0.05000000000000003
Training Loss: 0.0837
X: 0.0799999999999999, Y: -0.05000000000000003
Training Loss: 0.0785
X: 0.08999999999999989, Y: -0.05000000000000003
Training Loss: 0.0735
X: 0.0999999999999999, Y: -0.05000000000000003
Training Loss: 0.0777
X: -0.1, Y: -0.040000000000000036
Training Loss: 0.0741
X: -0.09000000000000001, Y: -0.040000000000000036
Training Loss: 0.0737
X: -0.08000000000000002, Y: -0.040000000000000036
Training Loss: 0.0780
X: -0.07000000000000002, Y: -0.040000000000000036
Training Loss: 0.0700
X: -0.060000000000000026, Y: -0.040000000000000036
Training Loss: 0.0760
X: -0.05000000000000003, Y: -0.040000000000000036
Training Loss: 0.0734
X: -0.040000000000000036, Y: -0.040000000000000036
Training Loss: 0.0682
X: -0.03000000000000004, Y: -0.040000000000000036
Training Loss: 0.0685
X: -0.020000000000000046, Y: -0.040000000000000036
Training Loss: 0.0701
X: -0.01000000000000005, Y: -0.040000000000000036
Training Loss: 0.0660
X: -5.551115123125783e-17, Y: -0.040000000000000036
Training Loss: 0.0664
X: 0.00999999999999994, Y: -0.040000000000000036
Training Loss: 0.0638
X: 0.019999999999999934, Y: -0.040000000000000036
Training Loss: 0.0711
X: 0.029999999999999943, Y: -0.040000000000000036
Training Loss: 0.0710
X: 0.039999999999999925, Y: -0.040000000000000036
Training Loss: 0.0730
X: 0.049999999999999906, Y: -0.040000000000000036
Training Loss: 0.0738
X: 0.059999999999999915, Y: -0.040000000000000036
Training Loss: 0.0688
X: 0.06999999999999992, Y: -0.040000000000000036
Training Loss: 0.0774
X: 0.0799999999999999, Y: -0.040000000000000036
Training Loss: 0.0815
X: 0.08999999999999989, Y: -0.040000000000000036
Training Loss: 0.0786
X: 0.0999999999999999, Y: -0.040000000000000036
Training Loss: 0.0811
X: -0.1, Y: -0.03000000000000004
Training Loss: 0.0813
X: -0.09000000000000001, Y: -0.03000000000000004
Training Loss: 0.0732
X: -0.08000000000000002, Y: -0.03000000000000004
Training Loss: 0.0734
X: -0.07000000000000002, Y: -0.03000000000000004
Training Loss: 0.0661
X: -0.060000000000000026, Y: -0.03000000000000004
Training Loss: 0.0717
X: -0.05000000000000003, Y: -0.03000000000000004
Training Loss: 0.0685
X: -0.040000000000000036, Y: -0.03000000000000004
Training Loss: 0.0711
X: -0.03000000000000004, Y: -0.03000000000000004
Training Loss: 0.0653
X: -0.020000000000000046, Y: -0.03000000000000004
Training Loss: 0.0661
X: -0.01000000000000005, Y: -0.03000000000000004
Training Loss: 0.0643
X: -5.551115123125783e-17, Y: -0.03000000000000004
Training Loss: 0.0682
X: 0.00999999999999994, Y: -0.03000000000000004
Training Loss: 0.0728
X: 0.019999999999999934, Y: -0.03000000000000004
Training Loss: 0.0694
X: 0.029999999999999943, Y: -0.03000000000000004
Training Loss: 0.0734
X: 0.039999999999999925, Y: -0.03000000000000004
Training Loss: 0.0691
X: 0.049999999999999906, Y: -0.03000000000000004
Training Loss: 0.0751
X: 0.059999999999999915, Y: -0.03000000000000004
Training Loss: 0.0709
X: 0.06999999999999992, Y: -0.03000000000000004
Training Loss: 0.0723
X: 0.0799999999999999, Y: -0.03000000000000004
Training Loss: 0.0717
X: 0.08999999999999989, Y: -0.03000000000000004
Training Loss: 0.0793
X: 0.0999999999999999, Y: -0.03000000000000004
Training Loss: 0.0758
X: -0.1, Y: -0.020000000000000046
Training Loss: 0.0717
X: -0.09000000000000001, Y: -0.020000000000000046
Training Loss: 0.0723
X: -0.08000000000000002, Y: -0.020000000000000046
Training Loss: 0.0713
X: -0.07000000000000002, Y: -0.020000000000000046
Training Loss: 0.0764
X: -0.060000000000000026, Y: -0.020000000000000046
Training Loss: 0.0709
X: -0.05000000000000003, Y: -0.020000000000000046
Training Loss: 0.0761
X: -0.040000000000000036, Y: -0.020000000000000046
Training Loss: 0.0672
X: -0.03000000000000004, Y: -0.020000000000000046
Training Loss: 0.0733
X: -0.020000000000000046, Y: -0.020000000000000046
Training Loss: 0.0747
X: -0.01000000000000005, Y: -0.020000000000000046
Training Loss: 0.0618
X: -5.551115123125783e-17, Y: -0.020000000000000046
Training Loss: 0.0706
X: 0.00999999999999994, Y: -0.020000000000000046
Training Loss: 0.0751
X: 0.019999999999999934, Y: -0.020000000000000046
Training Loss: 0.0714
X: 0.029999999999999943, Y: -0.020000000000000046
Training Loss: 0.0714
X: 0.039999999999999925, Y: -0.020000000000000046
Training Loss: 0.0727
X: 0.049999999999999906, Y: -0.020000000000000046
Training Loss: 0.0698
X: 0.059999999999999915, Y: -0.020000000000000046
Training Loss: 0.0698
X: 0.06999999999999992, Y: -0.020000000000000046
Training Loss: 0.0755
X: 0.0799999999999999, Y: -0.020000000000000046
Training Loss: 0.0687
X: 0.08999999999999989, Y: -0.020000000000000046
Training Loss: 0.0677
X: 0.0999999999999999, Y: -0.020000000000000046
Training Loss: 0.0801
X: -0.1, Y: -0.01000000000000005
Training Loss: 0.0794
X: -0.09000000000000001, Y: -0.01000000000000005
Training Loss: 0.0672
X: -0.08000000000000002, Y: -0.01000000000000005
Training Loss: 0.0776
X: -0.07000000000000002, Y: -0.01000000000000005
Training Loss: 0.0730
X: -0.060000000000000026, Y: -0.01000000000000005
Training Loss: 0.0666
X: -0.05000000000000003, Y: -0.01000000000000005
Training Loss: 0.0686
X: -0.040000000000000036, Y: -0.01000000000000005
Training Loss: 0.0679
X: -0.03000000000000004, Y: -0.01000000000000005
Training Loss: 0.0679
X: -0.020000000000000046, Y: -0.01000000000000005
Training Loss: 0.0695
X: -0.01000000000000005, Y: -0.01000000000000005
Training Loss: 0.0688
X: -5.551115123125783e-17, Y: -0.01000000000000005
Training Loss: 0.0752
X: 0.00999999999999994, Y: -0.01000000000000005
Training Loss: 0.0645
X: 0.019999999999999934, Y: -0.01000000000000005
Training Loss: 0.0671
X: 0.029999999999999943, Y: -0.01000000000000005
Training Loss: 0.0697
X: 0.039999999999999925, Y: -0.01000000000000005
Training Loss: 0.0669
X: 0.049999999999999906, Y: -0.01000000000000005
Training Loss: 0.0671
X: 0.059999999999999915, Y: -0.01000000000000005
Training Loss: 0.0744
X: 0.06999999999999992, Y: -0.01000000000000005
Training Loss: 0.0692
X: 0.0799999999999999, Y: -0.01000000000000005
Training Loss: 0.0764
X: 0.08999999999999989, Y: -0.01000000000000005
Training Loss: 0.0701
X: 0.0999999999999999, Y: -0.01000000000000005
Training Loss: 0.0729
X: -0.1, Y: -5.551115123125783e-17
Training Loss: 0.0741
X: -0.09000000000000001, Y: -5.551115123125783e-17
Training Loss: 0.0717
X: -0.08000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0708
X: -0.07000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0718
X: -0.060000000000000026, Y: -5.551115123125783e-17
Training Loss: 0.0735
X: -0.05000000000000003, Y: -5.551115123125783e-17
Training Loss: 0.0710
X: -0.040000000000000036, Y: -5.551115123125783e-17
Training Loss: 0.0670
X: -0.03000000000000004, Y: -5.551115123125783e-17
Training Loss: 0.0704
X: -0.020000000000000046, Y: -5.551115123125783e-17
Training Loss: 0.0664
X: -0.01000000000000005, Y: -5.551115123125783e-17
Training Loss: 0.0668
X: -5.551115123125783e-17, Y: -5.551115123125783e-17
Training Loss: 0.0710
X: 0.00999999999999994, Y: -5.551115123125783e-17
Training Loss: 0.0687
X: 0.019999999999999934, Y: -5.551115123125783e-17
Training Loss: 0.0741
X: 0.029999999999999943, Y: -5.551115123125783e-17
Training Loss: 0.0678
X: 0.039999999999999925, Y: -5.551115123125783e-17
Training Loss: 0.0733
X: 0.049999999999999906, Y: -5.551115123125783e-17
Training Loss: 0.0693
X: 0.059999999999999915, Y: -5.551115123125783e-17
Training Loss: 0.0707
X: 0.06999999999999992, Y: -5.551115123125783e-17
Training Loss: 0.0728
X: 0.0799999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0699
X: 0.08999999999999989, Y: -5.551115123125783e-17
Training Loss: 0.0732
X: 0.0999999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0785
X: -0.1, Y: 0.00999999999999994
Training Loss: 0.0771
X: -0.09000000000000001, Y: 0.00999999999999994
Training Loss: 0.0745
X: -0.08000000000000002, Y: 0.00999999999999994
Training Loss: 0.0767
X: -0.07000000000000002, Y: 0.00999999999999994
Training Loss: 0.0681
X: -0.060000000000000026, Y: 0.00999999999999994
Training Loss: 0.0739
X: -0.05000000000000003, Y: 0.00999999999999994
Training Loss: 0.0752
X: -0.040000000000000036, Y: 0.00999999999999994
Training Loss: 0.0741
X: -0.03000000000000004, Y: 0.00999999999999994
Training Loss: 0.0674
X: -0.020000000000000046, Y: 0.00999999999999994
Training Loss: 0.0689
X: -0.01000000000000005, Y: 0.00999999999999994
Training Loss: 0.0748
X: -5.551115123125783e-17, Y: 0.00999999999999994
Training Loss: 0.0728
X: 0.00999999999999994, Y: 0.00999999999999994
Training Loss: 0.0688
X: 0.019999999999999934, Y: 0.00999999999999994
Training Loss: 0.0706
X: 0.029999999999999943, Y: 0.00999999999999994
Training Loss: 0.0658
X: 0.039999999999999925, Y: 0.00999999999999994
Training Loss: 0.0713
X: 0.049999999999999906, Y: 0.00999999999999994
Training Loss: 0.0670
X: 0.059999999999999915, Y: 0.00999999999999994
Training Loss: 0.0747
X: 0.06999999999999992, Y: 0.00999999999999994
Training Loss: 0.0704
X: 0.0799999999999999, Y: 0.00999999999999994
Training Loss: 0.0727
X: 0.08999999999999989, Y: 0.00999999999999994
Training Loss: 0.0768
X: 0.0999999999999999, Y: 0.00999999999999994
Training Loss: 0.0785
X: -0.1, Y: 0.019999999999999934
Training Loss: 0.0783
X: -0.09000000000000001, Y: 0.019999999999999934
Training Loss: 0.0792
X: -0.08000000000000002, Y: 0.019999999999999934
Training Loss: 0.0785
X: -0.07000000000000002, Y: 0.019999999999999934
Training Loss: 0.0768
X: -0.060000000000000026, Y: 0.019999999999999934
Training Loss: 0.0773
X: -0.05000000000000003, Y: 0.019999999999999934
Training Loss: 0.0713
X: -0.040000000000000036, Y: 0.019999999999999934
Training Loss: 0.0717
X: -0.03000000000000004, Y: 0.019999999999999934
Training Loss: 0.0712
X: -0.020000000000000046, Y: 0.019999999999999934
Training Loss: 0.0663
X: -0.01000000000000005, Y: 0.019999999999999934
Training Loss: 0.0741
X: -5.551115123125783e-17, Y: 0.019999999999999934
Training Loss: 0.0702
X: 0.00999999999999994, Y: 0.019999999999999934
Training Loss: 0.0673
X: 0.019999999999999934, Y: 0.019999999999999934
Training Loss: 0.0687
X: 0.029999999999999943, Y: 0.019999999999999934
Training Loss: 0.0687
X: 0.039999999999999925, Y: 0.019999999999999934
Training Loss: 0.0718
X: 0.049999999999999906, Y: 0.019999999999999934
Training Loss: 0.0753
X: 0.059999999999999915, Y: 0.019999999999999934
Training Loss: 0.0719
X: 0.06999999999999992, Y: 0.019999999999999934
Training Loss: 0.0702
X: 0.0799999999999999, Y: 0.019999999999999934
Training Loss: 0.0718
X: 0.08999999999999989, Y: 0.019999999999999934
Training Loss: 0.0688
X: 0.0999999999999999, Y: 0.019999999999999934
Training Loss: 0.0740
X: -0.1, Y: 0.029999999999999943
Training Loss: 0.0746
X: -0.09000000000000001, Y: 0.029999999999999943
Training Loss: 0.0803
X: -0.08000000000000002, Y: 0.029999999999999943
Training Loss: 0.0810
X: -0.07000000000000002, Y: 0.029999999999999943
Training Loss: 0.0746
X: -0.060000000000000026, Y: 0.029999999999999943
Training Loss: 0.0735
X: -0.05000000000000003, Y: 0.029999999999999943
Training Loss: 0.0725
X: -0.040000000000000036, Y: 0.029999999999999943
Training Loss: 0.0724
X: -0.03000000000000004, Y: 0.029999999999999943
Training Loss: 0.0692
X: -0.020000000000000046, Y: 0.029999999999999943
Training Loss: 0.0740
X: -0.01000000000000005, Y: 0.029999999999999943
Training Loss: 0.0664
X: -5.551115123125783e-17, Y: 0.029999999999999943
Training Loss: 0.0694
X: 0.00999999999999994, Y: 0.029999999999999943
Training Loss: 0.0690
X: 0.019999999999999934, Y: 0.029999999999999943
Training Loss: 0.0660
X: 0.029999999999999943, Y: 0.029999999999999943
Training Loss: 0.0767
X: 0.039999999999999925, Y: 0.029999999999999943
Training Loss: 0.0645
X: 0.049999999999999906, Y: 0.029999999999999943
Training Loss: 0.0708
X: 0.059999999999999915, Y: 0.029999999999999943
Training Loss: 0.0715
X: 0.06999999999999992, Y: 0.029999999999999943
Training Loss: 0.0747
X: 0.0799999999999999, Y: 0.029999999999999943
Training Loss: 0.0663
X: 0.08999999999999989, Y: 0.029999999999999943
Training Loss: 0.0710
X: 0.0999999999999999, Y: 0.029999999999999943
Training Loss: 0.0745
X: -0.1, Y: 0.039999999999999925
Training Loss: 0.0795
X: -0.09000000000000001, Y: 0.039999999999999925
Training Loss: 0.0830
X: -0.08000000000000002, Y: 0.039999999999999925
Training Loss: 0.0753
X: -0.07000000000000002, Y: 0.039999999999999925
Training Loss: 0.0746
X: -0.060000000000000026, Y: 0.039999999999999925
Training Loss: 0.0799
X: -0.05000000000000003, Y: 0.039999999999999925
Training Loss: 0.0690
X: -0.040000000000000036, Y: 0.039999999999999925
Training Loss: 0.0697
X: -0.03000000000000004, Y: 0.039999999999999925
Training Loss: 0.0726
X: -0.020000000000000046, Y: 0.039999999999999925
Training Loss: 0.0737
X: -0.01000000000000005, Y: 0.039999999999999925
Training Loss: 0.0707
X: -5.551115123125783e-17, Y: 0.039999999999999925
Training Loss: 0.0696
X: 0.00999999999999994, Y: 0.039999999999999925
Training Loss: 0.0761
X: 0.019999999999999934, Y: 0.039999999999999925
Training Loss: 0.0677
X: 0.029999999999999943, Y: 0.039999999999999925
Training Loss: 0.0692
X: 0.039999999999999925, Y: 0.039999999999999925
Training Loss: 0.0746
X: 0.049999999999999906, Y: 0.039999999999999925
Training Loss: 0.0736
X: 0.059999999999999915, Y: 0.039999999999999925
Training Loss: 0.0779
X: 0.06999999999999992, Y: 0.039999999999999925
Training Loss: 0.0757
X: 0.0799999999999999, Y: 0.039999999999999925
Training Loss: 0.0776
X: 0.08999999999999989, Y: 0.039999999999999925
Training Loss: 0.0697
X: 0.0999999999999999, Y: 0.039999999999999925
Training Loss: 0.0760
X: -0.1, Y: 0.049999999999999906
Training Loss: 0.0785
X: -0.09000000000000001, Y: 0.049999999999999906
Training Loss: 0.0832
X: -0.08000000000000002, Y: 0.049999999999999906
Training Loss: 0.0861
X: -0.07000000000000002, Y: 0.049999999999999906
Training Loss: 0.0798
X: -0.060000000000000026, Y: 0.049999999999999906
Training Loss: 0.0688
X: -0.05000000000000003, Y: 0.049999999999999906
Training Loss: 0.0692
X: -0.040000000000000036, Y: 0.049999999999999906
Training Loss: 0.0779
X: -0.03000000000000004, Y: 0.049999999999999906
Training Loss: 0.0739
X: -0.020000000000000046, Y: 0.049999999999999906
Training Loss: 0.0736
X: -0.01000000000000005, Y: 0.049999999999999906
Training Loss: 0.0744
X: -5.551115123125783e-17, Y: 0.049999999999999906
Training Loss: 0.0732
X: 0.00999999999999994, Y: 0.049999999999999906
Training Loss: 0.0735
X: 0.019999999999999934, Y: 0.049999999999999906
Training Loss: 0.0742
X: 0.029999999999999943, Y: 0.049999999999999906
Training Loss: 0.0756
X: 0.039999999999999925, Y: 0.049999999999999906
Training Loss: 0.0760
X: 0.049999999999999906, Y: 0.049999999999999906
Training Loss: 0.0725
X: 0.059999999999999915, Y: 0.049999999999999906
Training Loss: 0.0682
X: 0.06999999999999992, Y: 0.049999999999999906
Training Loss: 0.0767
X: 0.0799999999999999, Y: 0.049999999999999906
Training Loss: 0.0778
X: 0.08999999999999989, Y: 0.049999999999999906
Training Loss: 0.0744
X: 0.0999999999999999, Y: 0.049999999999999906
Training Loss: 0.0747
X: -0.1, Y: 0.059999999999999915
Training Loss: 0.0833
X: -0.09000000000000001, Y: 0.059999999999999915
Training Loss: 0.0740
X: -0.08000000000000002, Y: 0.059999999999999915
Training Loss: 0.0813
X: -0.07000000000000002, Y: 0.059999999999999915
Training Loss: 0.0886
X: -0.060000000000000026, Y: 0.059999999999999915
Training Loss: 0.0780
X: -0.05000000000000003, Y: 0.059999999999999915
Training Loss: 0.0754
X: -0.040000000000000036, Y: 0.059999999999999915
Training Loss: 0.0738
X: -0.03000000000000004, Y: 0.059999999999999915
Training Loss: 0.0730
X: -0.020000000000000046, Y: 0.059999999999999915
Training Loss: 0.0759
X: -0.01000000000000005, Y: 0.059999999999999915
Training Loss: 0.0769
X: -5.551115123125783e-17, Y: 0.059999999999999915
Training Loss: 0.0752
X: 0.00999999999999994, Y: 0.059999999999999915
Training Loss: 0.0730
X: 0.019999999999999934, Y: 0.059999999999999915
Training Loss: 0.0711
X: 0.029999999999999943, Y: 0.059999999999999915
Training Loss: 0.0802
X: 0.039999999999999925, Y: 0.059999999999999915
Training Loss: 0.0760
X: 0.049999999999999906, Y: 0.059999999999999915
Training Loss: 0.0691
X: 0.059999999999999915, Y: 0.059999999999999915
Training Loss: 0.0748
X: 0.06999999999999992, Y: 0.059999999999999915
Training Loss: 0.0754
X: 0.0799999999999999, Y: 0.059999999999999915
Training Loss: 0.0742
X: 0.08999999999999989, Y: 0.059999999999999915
Training Loss: 0.0689
X: 0.0999999999999999, Y: 0.059999999999999915
Training Loss: 0.0810
X: -0.1, Y: 0.06999999999999992
Training Loss: 0.0856
X: -0.09000000000000001, Y: 0.06999999999999992
Training Loss: 0.0806
X: -0.08000000000000002, Y: 0.06999999999999992
Training Loss: 0.0797
X: -0.07000000000000002, Y: 0.06999999999999992
Training Loss: 0.0781
X: -0.060000000000000026, Y: 0.06999999999999992
Training Loss: 0.0794
X: -0.05000000000000003, Y: 0.06999999999999992
Training Loss: 0.0844
X: -0.040000000000000036, Y: 0.06999999999999992
Training Loss: 0.0744
X: -0.03000000000000004, Y: 0.06999999999999992
Training Loss: 0.0738
X: -0.020000000000000046, Y: 0.06999999999999992
Training Loss: 0.0780
X: -0.01000000000000005, Y: 0.06999999999999992
Training Loss: 0.0760
X: -5.551115123125783e-17, Y: 0.06999999999999992
Training Loss: 0.0747
X: 0.00999999999999994, Y: 0.06999999999999992
Training Loss: 0.0800
X: 0.019999999999999934, Y: 0.06999999999999992
Training Loss: 0.0788
X: 0.029999999999999943, Y: 0.06999999999999992
Training Loss: 0.0781
X: 0.039999999999999925, Y: 0.06999999999999992
Training Loss: 0.0795
X: 0.049999999999999906, Y: 0.06999999999999992
Training Loss: 0.0779
X: 0.059999999999999915, Y: 0.06999999999999992
Training Loss: 0.0810
X: 0.06999999999999992, Y: 0.06999999999999992
Training Loss: 0.0774
X: 0.0799999999999999, Y: 0.06999999999999992
Training Loss: 0.0748
X: 0.08999999999999989, Y: 0.06999999999999992
Training Loss: 0.0727
X: 0.0999999999999999, Y: 0.06999999999999992
Training Loss: 0.0812
X: -0.1, Y: 0.0799999999999999
Training Loss: 0.0850
X: -0.09000000000000001, Y: 0.0799999999999999
Training Loss: 0.0809
X: -0.08000000000000002, Y: 0.0799999999999999
Training Loss: 0.0834
X: -0.07000000000000002, Y: 0.0799999999999999
Training Loss: 0.0748
X: -0.060000000000000026, Y: 0.0799999999999999
Training Loss: 0.0784
X: -0.05000000000000003, Y: 0.0799999999999999
Training Loss: 0.0845
X: -0.040000000000000036, Y: 0.0799999999999999
Training Loss: 0.0780
X: -0.03000000000000004, Y: 0.0799999999999999
Training Loss: 0.0780
X: -0.020000000000000046, Y: 0.0799999999999999
Training Loss: 0.0743
X: -0.01000000000000005, Y: 0.0799999999999999
Training Loss: 0.0747
X: -5.551115123125783e-17, Y: 0.0799999999999999
Training Loss: 0.0816
X: 0.00999999999999994, Y: 0.0799999999999999
Training Loss: 0.0833
X: 0.019999999999999934, Y: 0.0799999999999999
Training Loss: 0.0777
X: 0.029999999999999943, Y: 0.0799999999999999
Training Loss: 0.0756
X: 0.039999999999999925, Y: 0.0799999999999999
Training Loss: 0.0739
X: 0.049999999999999906, Y: 0.0799999999999999
Training Loss: 0.0831
X: 0.059999999999999915, Y: 0.0799999999999999
Training Loss: 0.0738
X: 0.06999999999999992, Y: 0.0799999999999999
Training Loss: 0.0794
X: 0.0799999999999999, Y: 0.0799999999999999
Training Loss: 0.0755
X: 0.08999999999999989, Y: 0.0799999999999999
Training Loss: 0.0812
X: 0.0999999999999999, Y: 0.0799999999999999
Training Loss: 0.0779
X: -0.1, Y: 0.08999999999999989
Training Loss: 0.0841
X: -0.09000000000000001, Y: 0.08999999999999989
Training Loss: 0.0823
X: -0.08000000000000002, Y: 0.08999999999999989
Training Loss: 0.0815
X: -0.07000000000000002, Y: 0.08999999999999989
Training Loss: 0.0754
X: -0.060000000000000026, Y: 0.08999999999999989
Training Loss: 0.0818
X: -0.05000000000000003, Y: 0.08999999999999989
Training Loss: 0.0757
X: -0.040000000000000036, Y: 0.08999999999999989
Training Loss: 0.0786
X: -0.03000000000000004, Y: 0.08999999999999989
Training Loss: 0.0859
X: -0.020000000000000046, Y: 0.08999999999999989
Training Loss: 0.0863
X: -0.01000000000000005, Y: 0.08999999999999989
Training Loss: 0.0806
X: -5.551115123125783e-17, Y: 0.08999999999999989
Training Loss: 0.0789
X: 0.00999999999999994, Y: 0.08999999999999989
Training Loss: 0.0919
X: 0.019999999999999934, Y: 0.08999999999999989
Training Loss: 0.0797
X: 0.029999999999999943, Y: 0.08999999999999989
Training Loss: 0.0788
X: 0.039999999999999925, Y: 0.08999999999999989
Training Loss: 0.0772
X: 0.049999999999999906, Y: 0.08999999999999989
Training Loss: 0.0805
X: 0.059999999999999915, Y: 0.08999999999999989
Training Loss: 0.0740
X: 0.06999999999999992, Y: 0.08999999999999989
Training Loss: 0.0782
X: 0.0799999999999999, Y: 0.08999999999999989
Training Loss: 0.0811
X: 0.08999999999999989, Y: 0.08999999999999989
Training Loss: 0.0787
X: 0.0999999999999999, Y: 0.08999999999999989
Training Loss: 0.0783
X: -0.1, Y: 0.0999999999999999
Training Loss: 0.0887
X: -0.09000000000000001, Y: 0.0999999999999999
Training Loss: 0.0820
X: -0.08000000000000002, Y: 0.0999999999999999
Training Loss: 0.0908
X: -0.07000000000000002, Y: 0.0999999999999999
Training Loss: 0.0871
X: -0.060000000000000026, Y: 0.0999999999999999
Training Loss: 0.0790
X: -0.05000000000000003, Y: 0.0999999999999999
Training Loss: 0.0827
X: -0.040000000000000036, Y: 0.0999999999999999
Training Loss: 0.0858
X: -0.03000000000000004, Y: 0.0999999999999999
Training Loss: 0.0857
X: -0.020000000000000046, Y: 0.0999999999999999
Training Loss: 0.0798
X: -0.01000000000000005, Y: 0.0999999999999999
Training Loss: 0.0794
X: -5.551115123125783e-17, Y: 0.0999999999999999
Training Loss: 0.0898
X: 0.00999999999999994, Y: 0.0999999999999999
Training Loss: 0.0844
X: 0.019999999999999934, Y: 0.0999999999999999
Training Loss: 0.0835
X: 0.029999999999999943, Y: 0.0999999999999999
Training Loss: 0.0824
X: 0.039999999999999925, Y: 0.0999999999999999
Training Loss: 0.0785
X: 0.049999999999999906, Y: 0.0999999999999999
Training Loss: 0.0920
X: 0.059999999999999915, Y: 0.0999999999999999
Training Loss: 0.0811
X: 0.06999999999999992, Y: 0.0999999999999999
Training Loss: 0.0827
X: 0.0799999999999999, Y: 0.0999999999999999
Training Loss: 0.0837
X: 0.08999999999999989, Y: 0.0999999999999999
Training Loss: 0.0796
X: 0.0999999999999999, Y: 0.0999999999999999
Training Loss: 0.0864
Saved to runs/10_epochs_10:34_12_6_2022/new_data_YGG_Random_Directions_random.ply
training ResNet
epoch: 0
Train Loss: 0.4248, Train Acc: 0.19
Test Loss: 0.1920, Test Acc: 0.42
epoch: 1
Train Loss: 0.1516, Train Acc: 0.51
Test Loss: 0.1133, Test Acc: 0.61
epoch: 2
Train Loss: 0.1011, Train Acc: 0.67
Test Loss: 0.0846, Test Acc: 0.72
epoch: 3
Train Loss: 0.0864, Train Acc: 0.72
Test Loss: 0.0568, Test Acc: 0.82
epoch: 4
Train Loss: 0.0654, Train Acc: 0.78
Test Loss: 0.0516, Test Acc: 0.83
epoch: 5
Train Loss: 0.0631, Train Acc: 0.78
Test Loss: 0.0461, Test Acc: 0.85
epoch: 6
Train Loss: 0.0581, Train Acc: 0.80
Test Loss: 0.0476, Test Acc: 0.84
epoch: 7
Train Loss: 0.0535, Train Acc: 0.83
Test Loss: 0.0434, Test Acc: 0.85
epoch: 8
Train Loss: 0.0494, Train Acc: 0.83
Test Loss: 0.0474, Test Acc: 0.85
epoch: 9
Train Loss: 0.0441, Train Acc: 0.85
Test Loss: 0.0363, Test Acc: 0.88
X: -0.1, Y: -0.1
Training Loss: 0.0345
X: -0.09000000000000001, Y: -0.1
Training Loss: 0.0299
X: -0.08000000000000002, Y: -0.1
Training Loss: 0.0292
X: -0.07000000000000002, Y: -0.1
Training Loss: 0.0315
X: -0.060000000000000026, Y: -0.1
Training Loss: 0.0336
X: -0.05000000000000003, Y: -0.1
Training Loss: 0.0295
X: -0.040000000000000036, Y: -0.1
Training Loss: 0.0357
X: -0.03000000000000004, Y: -0.1
Training Loss: 0.0304
X: -0.020000000000000046, Y: -0.1
Training Loss: 0.0313
X: -0.01000000000000005, Y: -0.1
Training Loss: 0.0369
X: -5.551115123125783e-17, Y: -0.1
Training Loss: 0.0321
X: 0.00999999999999994, Y: -0.1
Training Loss: 0.0304
X: 0.019999999999999934, Y: -0.1
Training Loss: 0.0282
X: 0.029999999999999943, Y: -0.1
Training Loss: 0.0255
X: 0.039999999999999925, Y: -0.1
Training Loss: 0.0313
X: 0.049999999999999906, Y: -0.1
Training Loss: 0.0353
X: 0.059999999999999915, Y: -0.1
Training Loss: 0.0364
X: 0.06999999999999992, Y: -0.1
Training Loss: 0.0307
X: 0.0799999999999999, Y: -0.1
Training Loss: 0.0333
X: 0.08999999999999989, Y: -0.1
Training Loss: 0.0350
X: 0.0999999999999999, Y: -0.1
Training Loss: 0.0317
X: -0.1, Y: -0.09000000000000001
Training Loss: 0.0302
X: -0.09000000000000001, Y: -0.09000000000000001
Training Loss: 0.0380
X: -0.08000000000000002, Y: -0.09000000000000001
Training Loss: 0.0327
X: -0.07000000000000002, Y: -0.09000000000000001
Training Loss: 0.0317
X: -0.060000000000000026, Y: -0.09000000000000001
Training Loss: 0.0307
X: -0.05000000000000003, Y: -0.09000000000000001
Training Loss: 0.0308
X: -0.040000000000000036, Y: -0.09000000000000001
Training Loss: 0.0333
X: -0.03000000000000004, Y: -0.09000000000000001
Training Loss: 0.0315
X: -0.020000000000000046, Y: -0.09000000000000001
Training Loss: 0.0326
X: -0.01000000000000005, Y: -0.09000000000000001
Training Loss: 0.0328
X: -5.551115123125783e-17, Y: -0.09000000000000001
Training Loss: 0.0314
X: 0.00999999999999994, Y: -0.09000000000000001
Training Loss: 0.0379
X: 0.019999999999999934, Y: -0.09000000000000001
Training Loss: 0.0325
X: 0.029999999999999943, Y: -0.09000000000000001
Training Loss: 0.0337
X: 0.039999999999999925, Y: -0.09000000000000001
Training Loss: 0.0337
X: 0.049999999999999906, Y: -0.09000000000000001
Training Loss: 0.0320
X: 0.059999999999999915, Y: -0.09000000000000001
Training Loss: 0.0335
X: 0.06999999999999992, Y: -0.09000000000000001
Training Loss: 0.0346
X: 0.0799999999999999, Y: -0.09000000000000001
Training Loss: 0.0324
X: 0.08999999999999989, Y: -0.09000000000000001
Training Loss: 0.0315
X: 0.0999999999999999, Y: -0.09000000000000001
Training Loss: 0.0310
X: -0.1, Y: -0.08000000000000002
Training Loss: 0.0309
X: -0.09000000000000001, Y: -0.08000000000000002
Training Loss: 0.0284
X: -0.08000000000000002, Y: -0.08000000000000002
Training Loss: 0.0364
X: -0.07000000000000002, Y: -0.08000000000000002
Training Loss: 0.0337
X: -0.060000000000000026, Y: -0.08000000000000002
Training Loss: 0.0325
X: -0.05000000000000003, Y: -0.08000000000000002
Training Loss: 0.0308
X: -0.040000000000000036, Y: -0.08000000000000002
Training Loss: 0.0300
X: -0.03000000000000004, Y: -0.08000000000000002
Training Loss: 0.0327
X: -0.020000000000000046, Y: -0.08000000000000002
Training Loss: 0.0336
X: -0.01000000000000005, Y: -0.08000000000000002
Training Loss: 0.0333
X: -5.551115123125783e-17, Y: -0.08000000000000002
Training Loss: 0.0344
X: 0.00999999999999994, Y: -0.08000000000000002
Training Loss: 0.0316
X: 0.019999999999999934, Y: -0.08000000000000002
Training Loss: 0.0302
X: 0.029999999999999943, Y: -0.08000000000000002
Training Loss: 0.0295
X: 0.039999999999999925, Y: -0.08000000000000002
Training Loss: 0.0297
X: 0.049999999999999906, Y: -0.08000000000000002
Training Loss: 0.0330
X: 0.059999999999999915, Y: -0.08000000000000002
Training Loss: 0.0313
X: 0.06999999999999992, Y: -0.08000000000000002
Training Loss: 0.0321
X: 0.0799999999999999, Y: -0.08000000000000002
Training Loss: 0.0316
X: 0.08999999999999989, Y: -0.08000000000000002
Training Loss: 0.0320
X: 0.0999999999999999, Y: -0.08000000000000002
Training Loss: 0.0301
X: -0.1, Y: -0.07000000000000002
Training Loss: 0.0324
X: -0.09000000000000001, Y: -0.07000000000000002
Training Loss: 0.0293
X: -0.08000000000000002, Y: -0.07000000000000002
Training Loss: 0.0302
X: -0.07000000000000002, Y: -0.07000000000000002
Training Loss: 0.0306
X: -0.060000000000000026, Y: -0.07000000000000002
Training Loss: 0.0282
X: -0.05000000000000003, Y: -0.07000000000000002
Training Loss: 0.0347
X: -0.040000000000000036, Y: -0.07000000000000002
Training Loss: 0.0322
X: -0.03000000000000004, Y: -0.07000000000000002
Training Loss: 0.0308
X: -0.020000000000000046, Y: -0.07000000000000002
Training Loss: 0.0335
X: -0.01000000000000005, Y: -0.07000000000000002
Training Loss: 0.0350
X: -5.551115123125783e-17, Y: -0.07000000000000002
Training Loss: 0.0362
X: 0.00999999999999994, Y: -0.07000000000000002
Training Loss: 0.0345
X: 0.019999999999999934, Y: -0.07000000000000002
Training Loss: 0.0301
X: 0.029999999999999943, Y: -0.07000000000000002
Training Loss: 0.0346
X: 0.039999999999999925, Y: -0.07000000000000002
Training Loss: 0.0272
X: 0.049999999999999906, Y: -0.07000000000000002
Training Loss: 0.0297
X: 0.059999999999999915, Y: -0.07000000000000002
Training Loss: 0.0324
X: 0.06999999999999992, Y: -0.07000000000000002
Training Loss: 0.0330
X: 0.0799999999999999, Y: -0.07000000000000002
Training Loss: 0.0360
X: 0.08999999999999989, Y: -0.07000000000000002
Training Loss: 0.0308
X: 0.0999999999999999, Y: -0.07000000000000002
Training Loss: 0.0333
X: -0.1, Y: -0.060000000000000026
Training Loss: 0.0310
X: -0.09000000000000001, Y: -0.060000000000000026
Training Loss: 0.0334
X: -0.08000000000000002, Y: -0.060000000000000026
Training Loss: 0.0291
X: -0.07000000000000002, Y: -0.060000000000000026
Training Loss: 0.0327
X: -0.060000000000000026, Y: -0.060000000000000026
Training Loss: 0.0302
X: -0.05000000000000003, Y: -0.060000000000000026
Training Loss: 0.0339
X: -0.040000000000000036, Y: -0.060000000000000026
Training Loss: 0.0320
X: -0.03000000000000004, Y: -0.060000000000000026
Training Loss: 0.0327
X: -0.020000000000000046, Y: -0.060000000000000026
Training Loss: 0.0332
X: -0.01000000000000005, Y: -0.060000000000000026
Training Loss: 0.0329
X: -5.551115123125783e-17, Y: -0.060000000000000026
Training Loss: 0.0307
X: 0.00999999999999994, Y: -0.060000000000000026
Training Loss: 0.0362
X: 0.019999999999999934, Y: -0.060000000000000026
Training Loss: 0.0373
X: 0.029999999999999943, Y: -0.060000000000000026
Training Loss: 0.0332
X: 0.039999999999999925, Y: -0.060000000000000026
Training Loss: 0.0342
X: 0.049999999999999906, Y: -0.060000000000000026
Training Loss: 0.0300
X: 0.059999999999999915, Y: -0.060000000000000026
Training Loss: 0.0327
X: 0.06999999999999992, Y: -0.060000000000000026
Training Loss: 0.0335
X: 0.0799999999999999, Y: -0.060000000000000026
Training Loss: 0.0323
X: 0.08999999999999989, Y: -0.060000000000000026
Training Loss: 0.0349
X: 0.0999999999999999, Y: -0.060000000000000026
Training Loss: 0.0302
X: -0.1, Y: -0.05000000000000003
Training Loss: 0.0304
X: -0.09000000000000001, Y: -0.05000000000000003
Training Loss: 0.0332
X: -0.08000000000000002, Y: -0.05000000000000003
Training Loss: 0.0318
X: -0.07000000000000002, Y: -0.05000000000000003
Training Loss: 0.0318
X: -0.060000000000000026, Y: -0.05000000000000003
Training Loss: 0.0307
X: -0.05000000000000003, Y: -0.05000000000000003
Training Loss: 0.0294
X: -0.040000000000000036, Y: -0.05000000000000003
Training Loss: 0.0318
X: -0.03000000000000004, Y: -0.05000000000000003
Training Loss: 0.0315
X: -0.020000000000000046, Y: -0.05000000000000003
Training Loss: 0.0325
X: -0.01000000000000005, Y: -0.05000000000000003
Training Loss: 0.0310
X: -5.551115123125783e-17, Y: -0.05000000000000003
Training Loss: 0.0330
X: 0.00999999999999994, Y: -0.05000000000000003
Training Loss: 0.0307
X: 0.019999999999999934, Y: -0.05000000000000003
Training Loss: 0.0303
X: 0.029999999999999943, Y: -0.05000000000000003
Training Loss: 0.0285
X: 0.039999999999999925, Y: -0.05000000000000003
Training Loss: 0.0325
X: 0.049999999999999906, Y: -0.05000000000000003
Training Loss: 0.0297
X: 0.059999999999999915, Y: -0.05000000000000003
Training Loss: 0.0321
X: 0.06999999999999992, Y: -0.05000000000000003
Training Loss: 0.0315
X: 0.0799999999999999, Y: -0.05000000000000003
Training Loss: 0.0364
X: 0.08999999999999989, Y: -0.05000000000000003
Training Loss: 0.0313
X: 0.0999999999999999, Y: -0.05000000000000003
Training Loss: 0.0332
X: -0.1, Y: -0.040000000000000036
Training Loss: 0.0296
X: -0.09000000000000001, Y: -0.040000000000000036
Training Loss: 0.0280
X: -0.08000000000000002, Y: -0.040000000000000036
Training Loss: 0.0338
X: -0.07000000000000002, Y: -0.040000000000000036
Training Loss: 0.0256
X: -0.060000000000000026, Y: -0.040000000000000036
Training Loss: 0.0301
X: -0.05000000000000003, Y: -0.040000000000000036
Training Loss: 0.0316
X: -0.040000000000000036, Y: -0.040000000000000036
Training Loss: 0.0307
X: -0.03000000000000004, Y: -0.040000000000000036
Training Loss: 0.0281
X: -0.020000000000000046, Y: -0.040000000000000036
Training Loss: 0.0308
X: -0.01000000000000005, Y: -0.040000000000000036
Training Loss: 0.0298
X: -5.551115123125783e-17, Y: -0.040000000000000036
Training Loss: 0.0317
X: 0.00999999999999994, Y: -0.040000000000000036
Training Loss: 0.0344
X: 0.019999999999999934, Y: -0.040000000000000036
Training Loss: 0.0312
X: 0.029999999999999943, Y: -0.040000000000000036
Training Loss: 0.0311
X: 0.039999999999999925, Y: -0.040000000000000036
Training Loss: 0.0336
X: 0.049999999999999906, Y: -0.040000000000000036
Training Loss: 0.0335
X: 0.059999999999999915, Y: -0.040000000000000036
Training Loss: 0.0312
X: 0.06999999999999992, Y: -0.040000000000000036
Training Loss: 0.0344
X: 0.0799999999999999, Y: -0.040000000000000036
Training Loss: 0.0339
X: 0.08999999999999989, Y: -0.040000000000000036
Training Loss: 0.0288
X: 0.0999999999999999, Y: -0.040000000000000036
Training Loss: 0.0350
X: -0.1, Y: -0.03000000000000004
Training Loss: 0.0277
X: -0.09000000000000001, Y: -0.03000000000000004
Training Loss: 0.0327
X: -0.08000000000000002, Y: -0.03000000000000004
Training Loss: 0.0333
X: -0.07000000000000002, Y: -0.03000000000000004
Training Loss: 0.0353
X: -0.060000000000000026, Y: -0.03000000000000004
Training Loss: 0.0342
X: -0.05000000000000003, Y: -0.03000000000000004
Training Loss: 0.0311
X: -0.040000000000000036, Y: -0.03000000000000004
Training Loss: 0.0314
X: -0.03000000000000004, Y: -0.03000000000000004
Training Loss: 0.0310
X: -0.020000000000000046, Y: -0.03000000000000004
Training Loss: 0.0308
X: -0.01000000000000005, Y: -0.03000000000000004
Training Loss: 0.0346
X: -5.551115123125783e-17, Y: -0.03000000000000004
Training Loss: 0.0320
X: 0.00999999999999994, Y: -0.03000000000000004
Training Loss: 0.0345
X: 0.019999999999999934, Y: -0.03000000000000004
Training Loss: 0.0310
X: 0.029999999999999943, Y: -0.03000000000000004
Training Loss: 0.0325
X: 0.039999999999999925, Y: -0.03000000000000004
Training Loss: 0.0305
X: 0.049999999999999906, Y: -0.03000000000000004
Training Loss: 0.0303
X: 0.059999999999999915, Y: -0.03000000000000004
Training Loss: 0.0312
X: 0.06999999999999992, Y: -0.03000000000000004
Training Loss: 0.0337
X: 0.0799999999999999, Y: -0.03000000000000004
Training Loss: 0.0317
X: 0.08999999999999989, Y: -0.03000000000000004
Training Loss: 0.0311
X: 0.0999999999999999, Y: -0.03000000000000004
Training Loss: 0.0279
X: -0.1, Y: -0.020000000000000046
Training Loss: 0.0296
X: -0.09000000000000001, Y: -0.020000000000000046
Training Loss: 0.0321
X: -0.08000000000000002, Y: -0.020000000000000046
Training Loss: 0.0341
X: -0.07000000000000002, Y: -0.020000000000000046
Training Loss: 0.0353
X: -0.060000000000000026, Y: -0.020000000000000046
Training Loss: 0.0297
X: -0.05000000000000003, Y: -0.020000000000000046
Training Loss: 0.0320
X: -0.040000000000000036, Y: -0.020000000000000046
Training Loss: 0.0329
X: -0.03000000000000004, Y: -0.020000000000000046
Training Loss: 0.0277
X: -0.020000000000000046, Y: -0.020000000000000046
Training Loss: 0.0315
X: -0.01000000000000005, Y: -0.020000000000000046
Training Loss: 0.0342
X: -5.551115123125783e-17, Y: -0.020000000000000046
Training Loss: 0.0300
X: 0.00999999999999994, Y: -0.020000000000000046
Training Loss: 0.0304
X: 0.019999999999999934, Y: -0.020000000000000046
Training Loss: 0.0278
X: 0.029999999999999943, Y: -0.020000000000000046
Training Loss: 0.0313
X: 0.039999999999999925, Y: -0.020000000000000046
Training Loss: 0.0310
X: 0.049999999999999906, Y: -0.020000000000000046
Training Loss: 0.0301
X: 0.059999999999999915, Y: -0.020000000000000046
Training Loss: 0.0338
X: 0.06999999999999992, Y: -0.020000000000000046
Training Loss: 0.0272
X: 0.0799999999999999, Y: -0.020000000000000046
Training Loss: 0.0349
X: 0.08999999999999989, Y: -0.020000000000000046
Training Loss: 0.0288
X: 0.0999999999999999, Y: -0.020000000000000046
Training Loss: 0.0298
X: -0.1, Y: -0.01000000000000005
Training Loss: 0.0336
X: -0.09000000000000001, Y: -0.01000000000000005
Training Loss: 0.0285
X: -0.08000000000000002, Y: -0.01000000000000005
Training Loss: 0.0354
X: -0.07000000000000002, Y: -0.01000000000000005
Training Loss: 0.0315
X: -0.060000000000000026, Y: -0.01000000000000005
Training Loss: 0.0297
X: -0.05000000000000003, Y: -0.01000000000000005
Training Loss: 0.0255
X: -0.040000000000000036, Y: -0.01000000000000005
Training Loss: 0.0299
X: -0.03000000000000004, Y: -0.01000000000000005
Training Loss: 0.0317
X: -0.020000000000000046, Y: -0.01000000000000005
Training Loss: 0.0369
X: -0.01000000000000005, Y: -0.01000000000000005
Training Loss: 0.0368
X: -5.551115123125783e-17, Y: -0.01000000000000005
Training Loss: 0.0332
X: 0.00999999999999994, Y: -0.01000000000000005
Training Loss: 0.0317
X: 0.019999999999999934, Y: -0.01000000000000005
Training Loss: 0.0367
X: 0.029999999999999943, Y: -0.01000000000000005
Training Loss: 0.0324
X: 0.039999999999999925, Y: -0.01000000000000005
Training Loss: 0.0304
X: 0.049999999999999906, Y: -0.01000000000000005
Training Loss: 0.0332
X: 0.059999999999999915, Y: -0.01000000000000005
Training Loss: 0.0339
X: 0.06999999999999992, Y: -0.01000000000000005
Training Loss: 0.0337
X: 0.0799999999999999, Y: -0.01000000000000005
Training Loss: 0.0359
X: 0.08999999999999989, Y: -0.01000000000000005
Training Loss: 0.0353
X: 0.0999999999999999, Y: -0.01000000000000005
Training Loss: 0.0266
X: -0.1, Y: -5.551115123125783e-17
Training Loss: 0.0398
X: -0.09000000000000001, Y: -5.551115123125783e-17
Training Loss: 0.0296
X: -0.08000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0292
X: -0.07000000000000002, Y: -5.551115123125783e-17
Training Loss: 0.0341
X: -0.060000000000000026, Y: -5.551115123125783e-17
Training Loss: 0.0342
X: -0.05000000000000003, Y: -5.551115123125783e-17
Training Loss: 0.0325
X: -0.040000000000000036, Y: -5.551115123125783e-17
Training Loss: 0.0305
X: -0.03000000000000004, Y: -5.551115123125783e-17
Training Loss: 0.0285
X: -0.020000000000000046, Y: -5.551115123125783e-17
Training Loss: 0.0291
X: -0.01000000000000005, Y: -5.551115123125783e-17
Training Loss: 0.0283
X: -5.551115123125783e-17, Y: -5.551115123125783e-17
Training Loss: 0.0304
X: 0.00999999999999994, Y: -5.551115123125783e-17
Training Loss: 0.0311
X: 0.019999999999999934, Y: -5.551115123125783e-17
Training Loss: 0.0354
X: 0.029999999999999943, Y: -5.551115123125783e-17
Training Loss: 0.0301
X: 0.039999999999999925, Y: -5.551115123125783e-17
Training Loss: 0.0328
X: 0.049999999999999906, Y: -5.551115123125783e-17
Training Loss: 0.0322
X: 0.059999999999999915, Y: -5.551115123125783e-17
Training Loss: 0.0349
X: 0.06999999999999992, Y: -5.551115123125783e-17
Training Loss: 0.0335
X: 0.0799999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0295
X: 0.08999999999999989, Y: -5.551115123125783e-17
Training Loss: 0.0352
X: 0.0999999999999999, Y: -5.551115123125783e-17
Training Loss: 0.0327
X: -0.1, Y: 0.00999999999999994
Training Loss: 0.0321
X: -0.09000000000000001, Y: 0.00999999999999994
Training Loss: 0.0327
X: -0.08000000000000002, Y: 0.00999999999999994
Training Loss: 0.0327
X: -0.07000000000000002, Y: 0.00999999999999994
Training Loss: 0.0304
X: -0.060000000000000026, Y: 0.00999999999999994
Training Loss: 0.0313
X: -0.05000000000000003, Y: 0.00999999999999994
Training Loss: 0.0314
X: -0.040000000000000036, Y: 0.00999999999999994
Training Loss: 0.0354
X: -0.03000000000000004, Y: 0.00999999999999994
Training Loss: 0.0335
X: -0.020000000000000046, Y: 0.00999999999999994
Training Loss: 0.0335
X: -0.01000000000000005, Y: 0.00999999999999994
Training Loss: 0.0319
X: -5.551115123125783e-17, Y: 0.00999999999999994
Training Loss: 0.0306
X: 0.00999999999999994, Y: 0.00999999999999994
Training Loss: 0.0292
X: 0.019999999999999934, Y: 0.00999999999999994
Training Loss: 0.0326
X: 0.029999999999999943, Y: 0.00999999999999994
Training Loss: 0.0332
X: 0.039999999999999925, Y: 0.00999999999999994
Training Loss: 0.0336
X: 0.049999999999999906, Y: 0.00999999999999994
Training Loss: 0.0323
X: 0.059999999999999915, Y: 0.00999999999999994
Training Loss: 0.0320
X: 0.06999999999999992, Y: 0.00999999999999994
Training Loss: 0.0306
X: 0.0799999999999999, Y: 0.00999999999999994
Training Loss: 0.0327
X: 0.08999999999999989, Y: 0.00999999999999994
Training Loss: 0.0318
X: 0.0999999999999999, Y: 0.00999999999999994
Training Loss: 0.0318
X: -0.1, Y: 0.019999999999999934
Training Loss: 0.0323
X: -0.09000000000000001, Y: 0.019999999999999934
Training Loss: 0.0349
X: -0.08000000000000002, Y: 0.019999999999999934
Training Loss: 0.0340
X: -0.07000000000000002, Y: 0.019999999999999934
Training Loss: 0.0295
X: -0.060000000000000026, Y: 0.019999999999999934
Training Loss: 0.0268
X: -0.05000000000000003, Y: 0.019999999999999934
Training Loss: 0.0325
X: -0.040000000000000036, Y: 0.019999999999999934
Training Loss: 0.0309
X: -0.03000000000000004, Y: 0.019999999999999934
Training Loss: 0.0307
X: -0.020000000000000046, Y: 0.019999999999999934
Training Loss: 0.0367
X: -0.01000000000000005, Y: 0.019999999999999934
Training Loss: 0.0332
X: -5.551115123125783e-17, Y: 0.019999999999999934
Training Loss: 0.0345
X: 0.00999999999999994, Y: 0.019999999999999934
Training Loss: 0.0310
X: 0.019999999999999934, Y: 0.019999999999999934
Training Loss: 0.0309
X: 0.029999999999999943, Y: 0.019999999999999934
Training Loss: 0.0355
X: 0.039999999999999925, Y: 0.019999999999999934
Training Loss: 0.0329
X: 0.049999999999999906, Y: 0.019999999999999934
Training Loss: 0.0331
X: 0.059999999999999915, Y: 0.019999999999999934
Training Loss: 0.0349
X: 0.06999999999999992, Y: 0.019999999999999934
Training Loss: 0.0296
X: 0.0799999999999999, Y: 0.019999999999999934
Training Loss: 0.0284
X: 0.08999999999999989, Y: 0.019999999999999934
Training Loss: 0.0314
X: 0.0999999999999999, Y: 0.019999999999999934
Training Loss: 0.0312
X: -0.1, Y: 0.029999999999999943
Training Loss: 0.0316
X: -0.09000000000000001, Y: 0.029999999999999943
Training Loss: 0.0332
X: -0.08000000000000002, Y: 0.029999999999999943
Training Loss: 0.0330
X: -0.07000000000000002, Y: 0.029999999999999943
Training Loss: 0.0306
X: -0.060000000000000026, Y: 0.029999999999999943
Training Loss: 0.0329
X: -0.05000000000000003, Y: 0.029999999999999943
Training Loss: 0.0311
X: -0.040000000000000036, Y: 0.029999999999999943
Training Loss: 0.0319
X: -0.03000000000000004, Y: 0.029999999999999943
Training Loss: 0.0327
X: -0.020000000000000046, Y: 0.029999999999999943
Training Loss: 0.0342
X: -0.01000000000000005, Y: 0.029999999999999943
Training Loss: 0.0318
X: -5.551115123125783e-17, Y: 0.029999999999999943
Training Loss: 0.0298
X: 0.00999999999999994, Y: 0.029999999999999943
Training Loss: 0.0343
X: 0.019999999999999934, Y: 0.029999999999999943
Training Loss: 0.0340
X: 0.029999999999999943, Y: 0.029999999999999943
Training Loss: 0.0382
X: 0.039999999999999925, Y: 0.029999999999999943
Training Loss: 0.0290
X: 0.049999999999999906, Y: 0.029999999999999943
Training Loss: 0.0305
X: 0.059999999999999915, Y: 0.029999999999999943
Training Loss: 0.0327
X: 0.06999999999999992, Y: 0.029999999999999943
Training Loss: 0.0336
X: 0.0799999999999999, Y: 0.029999999999999943
Training Loss: 0.0272
X: 0.08999999999999989, Y: 0.029999999999999943
Training Loss: 0.0340
X: 0.0999999999999999, Y: 0.029999999999999943
Training Loss: 0.0334
X: -0.1, Y: 0.039999999999999925
Training Loss: 0.0315
X: -0.09000000000000001, Y: 0.039999999999999925
Training Loss: 0.0355
X: -0.08000000000000002, Y: 0.039999999999999925
Training Loss: 0.0332
X: -0.07000000000000002, Y: 0.039999999999999925
Training Loss: 0.0370
X: -0.060000000000000026, Y: 0.039999999999999925
Training Loss: 0.0308
X: -0.05000000000000003, Y: 0.039999999999999925
Training Loss: 0.0329
X: -0.040000000000000036, Y: 0.039999999999999925
Training Loss: 0.0332
X: -0.03000000000000004, Y: 0.039999999999999925
Training Loss: 0.0329
X: -0.020000000000000046, Y: 0.039999999999999925
Training Loss: 0.0315
X: -0.01000000000000005, Y: 0.039999999999999925
Training Loss: 0.0286
X: -5.551115123125783e-17, Y: 0.039999999999999925
Training Loss: 0.0354
X: 0.00999999999999994, Y: 0.039999999999999925
Training Loss: 0.0306
X: 0.019999999999999934, Y: 0.039999999999999925
Training Loss: 0.0318
X: 0.029999999999999943, Y: 0.039999999999999925
Training Loss: 0.0345
X: 0.039999999999999925, Y: 0.039999999999999925
Training Loss: 0.0324
X: 0.049999999999999906, Y: 0.039999999999999925
Training Loss: 0.0334
X: 0.059999999999999915, Y: 0.039999999999999925
Training Loss: 0.0293
X: 0.06999999999999992, Y: 0.039999999999999925
Training Loss: 0.0326
X: 0.0799999999999999, Y: 0.039999999999999925
Training Loss: 0.0303
X: 0.08999999999999989, Y: 0.039999999999999925
Training Loss: 0.0325
X: 0.0999999999999999, Y: 0.039999999999999925
Training Loss: 0.0316
X: -0.1, Y: 0.049999999999999906
Training Loss: 0.0272
X: -0.09000000000000001, Y: 0.049999999999999906
Training Loss: 0.0366
X: -0.08000000000000002, Y: 0.049999999999999906
Training Loss: 0.0305
X: -0.07000000000000002, Y: 0.049999999999999906
Training Loss: 0.0285
X: -0.060000000000000026, Y: 0.049999999999999906
Training Loss: 0.0334
X: -0.05000000000000003, Y: 0.049999999999999906
Training Loss: 0.0332
X: -0.040000000000000036, Y: 0.049999999999999906
Training Loss: 0.0355
X: -0.03000000000000004, Y: 0.049999999999999906
Training Loss: 0.0305
X: -0.020000000000000046, Y: 0.049999999999999906
Training Loss: 0.0332
X: -0.01000000000000005, Y: 0.049999999999999906
Training Loss: 0.0277
X: -5.551115123125783e-17, Y: 0.049999999999999906
Training Loss: 0.0313
X: 0.00999999999999994, Y: 0.049999999999999906
Training Loss: 0.0275
X: 0.019999999999999934, Y: 0.049999999999999906
Training Loss: 0.0336
X: 0.029999999999999943, Y: 0.049999999999999906
Training Loss: 0.0339
X: 0.039999999999999925, Y: 0.049999999999999906
Training Loss: 0.0299
X: 0.049999999999999906, Y: 0.049999999999999906
Training Loss: 0.0315
X: 0.059999999999999915, Y: 0.049999999999999906
Training Loss: 0.0335
X: 0.06999999999999992, Y: 0.049999999999999906
Training Loss: 0.0301
X: 0.0799999999999999, Y: 0.049999999999999906
Training Loss: 0.0321
X: 0.08999999999999989, Y: 0.049999999999999906
Training Loss: 0.0304
X: 0.0999999999999999, Y: 0.049999999999999906
Training Loss: 0.0370
X: -0.1, Y: 0.059999999999999915
Training Loss: 0.0324
X: -0.09000000000000001, Y: 0.059999999999999915
Training Loss: 0.0319
X: -0.08000000000000002, Y: 0.059999999999999915
Training Loss: 0.0318
X: -0.07000000000000002, Y: 0.059999999999999915
Training Loss: 0.0334
X: -0.060000000000000026, Y: 0.059999999999999915
Training Loss: 0.0285
X: -0.05000000000000003, Y: 0.059999999999999915
Training Loss: 0.0333
X: -0.040000000000000036, Y: 0.059999999999999915
Training Loss: 0.0287
X: -0.03000000000000004, Y: 0.059999999999999915
Training Loss: 0.0298
X: -0.020000000000000046, Y: 0.059999999999999915
Training Loss: 0.0297
X: -0.01000000000000005, Y: 0.059999999999999915
Training Loss: 0.0366
X: -5.551115123125783e-17, Y: 0.059999999999999915
Training Loss: 0.0328
X: 0.00999999999999994, Y: 0.059999999999999915
Training Loss: 0.0342
X: 0.019999999999999934, Y: 0.059999999999999915
Training Loss: 0.0319
X: 0.029999999999999943, Y: 0.059999999999999915
Training Loss: 0.0342
X: 0.039999999999999925, Y: 0.059999999999999915
Training Loss: 0.0322
X: 0.049999999999999906, Y: 0.059999999999999915
Training Loss: 0.0290
X: 0.059999999999999915, Y: 0.059999999999999915
Training Loss: 0.0298
X: 0.06999999999999992, Y: 0.059999999999999915
Training Loss: 0.0290
X: 0.0799999999999999, Y: 0.059999999999999915
Training Loss: 0.0289
X: 0.08999999999999989, Y: 0.059999999999999915
Training Loss: 0.0304
X: 0.0999999999999999, Y: 0.059999999999999915
Training Loss: 0.0329
X: -0.1, Y: 0.06999999999999992
Training Loss: 0.0358
X: -0.09000000000000001, Y: 0.06999999999999992
Training Loss: 0.0313
X: -0.08000000000000002, Y: 0.06999999999999992
Training Loss: 0.0312
X: -0.07000000000000002, Y: 0.06999999999999992
Training Loss: 0.0325
X: -0.060000000000000026, Y: 0.06999999999999992
Training Loss: 0.0312
X: -0.05000000000000003, Y: 0.06999999999999992
Training Loss: 0.0278
X: -0.040000000000000036, Y: 0.06999999999999992
Training Loss: 0.0341
X: -0.03000000000000004, Y: 0.06999999999999992
Training Loss: 0.0292
X: -0.020000000000000046, Y: 0.06999999999999992
Training Loss: 0.0301
X: -0.01000000000000005, Y: 0.06999999999999992
Training Loss: 0.0326
X: -5.551115123125783e-17, Y: 0.06999999999999992
Training Loss: 0.0353
X: 0.00999999999999994, Y: 0.06999999999999992
Training Loss: 0.0328
X: 0.019999999999999934, Y: 0.06999999999999992
Training Loss: 0.0322
X: 0.029999999999999943, Y: 0.06999999999999992
Training Loss: 0.0299
X: 0.039999999999999925, Y: 0.06999999999999992
Training Loss: 0.0290
X: 0.049999999999999906, Y: 0.06999999999999992
Training Loss: 0.0311
X: 0.059999999999999915, Y: 0.06999999999999992
Training Loss: 0.0327
X: 0.06999999999999992, Y: 0.06999999999999992
Training Loss: 0.0356
X: 0.0799999999999999, Y: 0.06999999999999992
Training Loss: 0.0342
X: 0.08999999999999989, Y: 0.06999999999999992
Training Loss: 0.0367
X: 0.0999999999999999, Y: 0.06999999999999992
Training Loss: 0.0338
X: -0.1, Y: 0.0799999999999999
Training Loss: 0.0324
X: -0.09000000000000001, Y: 0.0799999999999999
Training Loss: 0.0287
X: -0.08000000000000002, Y: 0.0799999999999999
Training Loss: 0.0329
X: -0.07000000000000002, Y: 0.0799999999999999
Training Loss: 0.0285
X: -0.060000000000000026, Y: 0.0799999999999999
Training Loss: 0.0334
X: -0.05000000000000003, Y: 0.0799999999999999
Training Loss: 0.0347
X: -0.040000000000000036, Y: 0.0799999999999999
Training Loss: 0.0301
X: -0.03000000000000004, Y: 0.0799999999999999
Training Loss: 0.0310
X: -0.020000000000000046, Y: 0.0799999999999999
Training Loss: 0.0294
X: -0.01000000000000005, Y: 0.0799999999999999
Training Loss: 0.0321
X: -5.551115123125783e-17, Y: 0.0799999999999999
Training Loss: 0.0327
X: 0.00999999999999994, Y: 0.0799999999999999
Training Loss: 0.0315
X: 0.019999999999999934, Y: 0.0799999999999999
Training Loss: 0.0348
X: 0.029999999999999943, Y: 0.0799999999999999
Training Loss: 0.0309
X: 0.039999999999999925, Y: 0.0799999999999999
Training Loss: 0.0299
X: 0.049999999999999906, Y: 0.0799999999999999
Training Loss: 0.0306
X: 0.059999999999999915, Y: 0.0799999999999999
Training Loss: 0.0325
X: 0.06999999999999992, Y: 0.0799999999999999
Training Loss: 0.0296
X: 0.0799999999999999, Y: 0.0799999999999999
Training Loss: 0.0328
X: 0.08999999999999989, Y: 0.0799999999999999
Training Loss: 0.0346
X: 0.0999999999999999, Y: 0.0799999999999999
Training Loss: 0.0356
X: -0.1, Y: 0.08999999999999989
Training Loss: 0.0310
X: -0.09000000000000001, Y: 0.08999999999999989
Training Loss: 0.0283
X: -0.08000000000000002, Y: 0.08999999999999989
Training Loss: 0.0324
X: -0.07000000000000002, Y: 0.08999999999999989
Training Loss: 0.0311
X: -0.060000000000000026, Y: 0.08999999999999989
Training Loss: 0.0317
X: -0.05000000000000003, Y: 0.08999999999999989
Training Loss: 0.0290
X: -0.040000000000000036, Y: 0.08999999999999989
Training Loss: 0.0323
X: -0.03000000000000004, Y: 0.08999999999999989
Training Loss: 0.0295
X: -0.020000000000000046, Y: 0.08999999999999989
Training Loss: 0.0352
X: -0.01000000000000005, Y: 0.08999999999999989
Training Loss: 0.0348
X: -5.551115123125783e-17, Y: 0.08999999999999989
Training Loss: 0.0323
X: 0.00999999999999994, Y: 0.08999999999999989
Training Loss: 0.0312
X: 0.019999999999999934, Y: 0.08999999999999989
Training Loss: 0.0293
X: 0.029999999999999943, Y: 0.08999999999999989
Training Loss: 0.0305
X: 0.039999999999999925, Y: 0.08999999999999989
Training Loss: 0.0354
X: 0.049999999999999906, Y: 0.08999999999999989
Training Loss: 0.0322
X: 0.059999999999999915, Y: 0.08999999999999989
Training Loss: 0.0320
X: 0.06999999999999992, Y: 0.08999999999999989
Training Loss: 0.0328
X: 0.0799999999999999, Y: 0.08999999999999989
Training Loss: 0.0318
X: 0.08999999999999989, Y: 0.08999999999999989
Training Loss: 0.0316
X: 0.0999999999999999, Y: 0.08999999999999989
Training Loss: 0.0326
X: -0.1, Y: 0.0999999999999999
Training Loss: 0.0282
X: -0.09000000000000001, Y: 0.0999999999999999
Training Loss: 0.0338
X: -0.08000000000000002, Y: 0.0999999999999999
Training Loss: 0.0314
X: -0.07000000000000002, Y: 0.0999999999999999
Training Loss: 0.0302
X: -0.060000000000000026, Y: 0.0999999999999999
Training Loss: 0.0318
X: -0.05000000000000003, Y: 0.0999999999999999
Training Loss: 0.0331
X: -0.040000000000000036, Y: 0.0999999999999999
Training Loss: 0.0314
X: -0.03000000000000004, Y: 0.0999999999999999
Training Loss: 0.0294
X: -0.020000000000000046, Y: 0.0999999999999999
Training Loss: 0.0293
X: -0.01000000000000005, Y: 0.0999999999999999
Training Loss: 0.0342
X: -5.551115123125783e-17, Y: 0.0999999999999999
Training Loss: 0.0296
X: 0.00999999999999994, Y: 0.0999999999999999
Training Loss: 0.0355
X: 0.019999999999999934, Y: 0.0999999999999999
Training Loss: 0.0320
X: 0.029999999999999943, Y: 0.0999999999999999
Training Loss: 0.0307
X: 0.039999999999999925, Y: 0.0999999999999999
Training Loss: 0.0366
X: 0.049999999999999906, Y: 0.0999999999999999
Training Loss: 0.0298
X: 0.059999999999999915, Y: 0.0999999999999999
Training Loss: 0.0361
X: 0.06999999999999992, Y: 0.0999999999999999
Training Loss: 0.0329
X: 0.0799999999999999, Y: 0.0999999999999999
Training Loss: 0.0324
X: 0.08999999999999989, Y: 0.0999999999999999
Training Loss: 0.0303
X: 0.0999999999999999, Y: 0.0999999999999999
Training Loss: 0.0363
Saved to runs/10_epochs_10:34_12_6_2022/new_data_ResNet_PCA_pca.ply
X: -0.1, Y: -0.1
Training Loss: 2.3046
X: -0.09000000000000001, Y: -0.1
Training Loss: 1.6581
X: -0.08000000000000002, Y: -0.1
Training Loss: 1.2066
X: -0.07000000000000002, Y: -0.1
Training Loss: 1.0138
X: -0.060000000000000026, Y: -0.1
Training Loss: 0.8825
X: -0.05000000000000003, Y: -0.1
Training Loss: 0.7602
X: -0.040000000000000036, Y: -0.1
Training Loss: 0.6845
X: -0.03000000000000004, Y: -0.1
Training Loss: 0.6310
X: -0.020000000000000046, Y: -0.1
Training Loss: 0.7953
X: -0.01000000000000005, Y: -0.1
Training Loss: 0.9891
X: -5.551115123125783e-17, Y: -0.1
Training Loss: 1.2212
X: 0.00999999999999994, Y: -0.1
Training Loss: 1.1691
X: 0.019999999999999934, Y: -0.1
Training Loss: 1.0551
X: 0.029999999999999943, Y: -0.1
Training Loss: 0.8766
X: 0.039999999999999925, Y: -0.1
Training Loss: 0.7530
X: 0.049999999999999906, Y: -0.1
Training Loss: 0.8358
X: 0.059999999999999915, Y: -0.1
Training Loss: 0.9962
X: 0.06999999999999992, Y: -0.1
Training Loss: 1.1146
X: 0.0799999999999999, Y: -0.1
Training Loss: 1.2066
X: 0.08999999999999989, Y: -0.1
Training Loss: 1.2561
X: 0.0999999999999999, Y: -0.1
Training Loss: 1.3958
X: -0.1, Y: -0.09000000000000001
Training Loss: 1.9032
X: -0.09000000000000001, Y: -0.09000000000000001
Training Loss: 1.4543
X: -0.08000000000000002, Y: -0.09000000000000001
Training Loss: 1.0647
X: -0.07000000000000002, Y: -0.09000000000000001
Training Loss: 0.9026
X: -0.060000000000000026, Y: -0.09000000000000001
Training Loss: 0.8072
X: -0.05000000000000003, Y: -0.09000000000000001
Training Loss: 0.7195
X: -0.040000000000000036, Y: -0.09000000000000001
Training Loss: 0.6181
X: -0.03000000000000004, Y: -0.09000000000000001
Training Loss: 0.5966
X: -0.020000000000000046, Y: -0.09000000000000001
Training Loss: 0.7313
X: -0.01000000000000005, Y: -0.09000000000000001
Training Loss: 0.9284
X: -5.551115123125783e-17, Y: -0.09000000000000001
Training Loss: 1.1275
X: 0.00999999999999994, Y: -0.09000000000000001
Training Loss: 1.0412
X: 0.019999999999999934, Y: -0.09000000000000001
Training Loss: 0.9426
X: 0.029999999999999943, Y: -0.09000000000000001
Training Loss: 0.7781
X: 0.039999999999999925, Y: -0.09000000000000001
Training Loss: 0.7996
X: 0.049999999999999906, Y: -0.09000000000000001
Training Loss: 1.0067
X: 0.059999999999999915, Y: -0.09000000000000001
Training Loss: 1.1826
X: 0.06999999999999992, Y: -0.09000000000000001
Training Loss: 1.2936
X: 0.0799999999999999, Y: -0.09000000000000001
Training Loss: 1.3542
X: 0.08999999999999989, Y: -0.09000000000000001
Training Loss: 1.3537
X: 0.0999999999999999, Y: -0.09000000000000001
Training Loss: 1.4554
X: -0.1, Y: -0.08000000000000002
Training Loss: 1.7597
X: -0.09000000000000001, Y: -0.08000000000000002
Training Loss: 1.3239
X: -0.08000000000000002, Y: -0.08000000000000002
Training Loss: 0.9514
X: -0.07000000000000002, Y: -0.08000000000000002
Training Loss: 0.8226
X: -0.060000000000000026, Y: -0.08000000000000002
Training Loss: 0.6867
X: -0.05000000000000003, Y: -0.08000000000000002
Training Loss: 0.6527
X: -0.040000000000000036, Y: -0.08000000000000002
Training Loss: 0.5600
X: -0.03000000000000004, Y: -0.08000000000000002
Training Loss: 0.5280
X: -0.020000000000000046, Y: -0.08000000000000002
Training Loss: 0.6291
X: -0.01000000000000005, Y: -0.08000000000000002
Training Loss: 0.8130
X: -5.551115123125783e-17, Y: -0.08000000000000002
Training Loss: 0.9782
X: 0.00999999999999994, Y: -0.08000000000000002
Training Loss: 0.8843
X: 0.019999999999999934, Y: -0.08000000000000002
Training Loss: 0.8195
X: 0.029999999999999943, Y: -0.08000000000000002
Training Loss: 0.7940
X: 0.039999999999999925, Y: -0.08000000000000002
Training Loss: 0.9321
X: 0.049999999999999906, Y: -0.08000000000000002
Training Loss: 1.2321
X: 0.059999999999999915, Y: -0.08000000000000002
Training Loss: 1.3950
X: 0.06999999999999992, Y: -0.08000000000000002
Training Loss: 1.4527
X: 0.0799999999999999, Y: -0.08000000000000002
Training Loss: 1.4554
X: 0.08999999999999989, Y: -0.08000000000000002
Training Loss: 1.4514
X: 0.0999999999999999, Y: -0.08000000000000002
Training Loss: 1.4871
X: -0.1, Y: -0.07000000000000002
Training Loss: 1.3595
X: -0.09000000000000001, Y: -0.07000000000000002
Training Loss: 1.0442
X: -0.08000000000000002, Y: -0.07000000000000002
Training Loss: 0.8245
X: -0.07000000000000002, Y: -0.07000000000000002
Training Loss: 0.7669
X: -0.060000000000000026, Y: -0.07000000000000002
Training Loss: 0.6450
X: -0.05000000000000003, Y: -0.07000000000000002
Training Loss: 0.5473
X: -0.040000000000000036, Y: -0.07000000000000002
Training Loss: 0.5001
X: -0.03000000000000004, Y: -0.07000000000000002
Training Loss: 0.4730
X: -0.020000000000000046, Y: -0.07000000000000002
Training Loss: 0.5361
X: -0.01000000000000005, Y: -0.07000000000000002
Training Loss: 0.6738
X: -5.551115123125783e-17, Y: -0.07000000000000002
Training Loss: 0.7850
X: 0.00999999999999994, Y: -0.07000000000000002
Training Loss: 0.7362
X: 0.019999999999999934, Y: -0.07000000000000002
Training Loss: 0.7431
X: 0.029999999999999943, Y: -0.07000000000000002
Training Loss: 0.8150
X: 0.039999999999999925, Y: -0.07000000000000002
Training Loss: 1.0971
X: 0.049999999999999906, Y: -0.07000000000000002
Training Loss: 1.3736
X: 0.059999999999999915, Y: -0.07000000000000002
Training Loss: 1.4994
X: 0.06999999999999992, Y: -0.07000000000000002
Training Loss: 1.5603
X: 0.0799999999999999, Y: -0.07000000000000002
Training Loss: 1.5576
X: 0.08999999999999989, Y: -0.07000000000000002
Training Loss: 1.5276
X: 0.0999999999999999, Y: -0.07000000000000002
Training Loss: 1.5571
X: -0.1, Y: -0.060000000000000026
Training Loss: 0.9171
X: -0.09000000000000001, Y: -0.060000000000000026
Training Loss: 0.7444
X: -0.08000000000000002, Y: -0.060000000000000026
Training Loss: 0.6617
X: -0.07000000000000002, Y: -0.060000000000000026
Training Loss: 0.5888
X: -0.060000000000000026, Y: -0.060000000000000026
Training Loss: 0.4994
X: -0.05000000000000003, Y: -0.060000000000000026
Training Loss: 0.4424
X: -0.040000000000000036, Y: -0.060000000000000026
Training Loss: 0.4283
X: -0.03000000000000004, Y: -0.060000000000000026
Training Loss: 0.4216
X: -0.020000000000000046, Y: -0.060000000000000026
Training Loss: 0.4375
X: -0.01000000000000005, Y: -0.060000000000000026
Training Loss: 0.5096
X: -5.551115123125783e-17, Y: -0.060000000000000026
Training Loss: 0.5353
X: 0.00999999999999994, Y: -0.060000000000000026
Training Loss: 0.5393
X: 0.019999999999999934, Y: -0.060000000000000026
Training Loss: 0.6102
X: 0.029999999999999943, Y: -0.060000000000000026
Training Loss: 0.7729
X: 0.039999999999999925, Y: -0.060000000000000026
Training Loss: 1.0912
X: 0.049999999999999906, Y: -0.060000000000000026
Training Loss: 1.3755
X: 0.059999999999999915, Y: -0.060000000000000026
Training Loss: 1.5136
X: 0.06999999999999992, Y: -0.060000000000000026
Training Loss: 1.5931
X: 0.0799999999999999, Y: -0.060000000000000026
Training Loss: 1.6047
X: 0.08999999999999989, Y: -0.060000000000000026
Training Loss: 1.5902
X: 0.0999999999999999, Y: -0.060000000000000026
Training Loss: 1.5810
X: -0.1, Y: -0.05000000000000003
Training Loss: 0.7925
X: -0.09000000000000001, Y: -0.05000000000000003
Training Loss: 0.5893
X: -0.08000000000000002, Y: -0.05000000000000003
Training Loss: 0.5262
X: -0.07000000000000002, Y: -0.05000000000000003
Training Loss: 0.4501
X: -0.060000000000000026, Y: -0.05000000000000003
Training Loss: 0.3949
X: -0.05000000000000003, Y: -0.05000000000000003
Training Loss: 0.3853
X: -0.040000000000000036, Y: -0.05000000000000003
Training Loss: 0.3813
X: -0.03000000000000004, Y: -0.05000000000000003
Training Loss: 0.3864
X: -0.020000000000000046, Y: -0.05000000000000003
Training Loss: 0.3573
X: -0.01000000000000005, Y: -0.05000000000000003
Training Loss: 0.3618
X: -5.551115123125783e-17, Y: -0.05000000000000003
Training Loss: 0.3550
X: 0.00999999999999994, Y: -0.05000000000000003
Training Loss: 0.3746
X: 0.019999999999999934, Y: -0.05000000000000003
Training Loss: 0.4906
X: 0.029999999999999943, Y: -0.05000000000000003
Training Loss: 0.7040
X: 0.039999999999999925, Y: -0.05000000000000003
Training Loss: 0.9612
X: 0.049999999999999906, Y: -0.05000000000000003
Training Loss: 1.2471
X: 0.059999999999999915, Y: -0.05000000000000003
Training Loss: 1.4166
X: 0.06999999999999992, Y: -0.05000000000000003
Training Loss: 1.5363
X: 0.0799999999999999, Y: -0.05000000000000003
Training Loss: 1.5902
X: 0.08999999999999989, Y: -0.05000000000000003
Training Loss: 1.5150
X: 0.0999999999999999, Y: -0.05000000000000003
Training Loss: 1.4708
X: -0.1, Y: -0.040000000000000036
Training Loss: 0.8893
X: -0.09000000000000001, Y: -0.040000000000000036
Training Loss: 0.6836
X: -0.08000000000000002, Y: -0.040000000000000036
Training Loss: 0.4825
X: -0.07000000000000002, Y: -0.040000000000000036
Training Loss: 0.4188
X: -0.060000000000000026, Y: -0.040000000000000036
Training Loss: 0.3749
X: -0.05000000000000003, Y: -0.040000000000000036
Training Loss: 0.3366
X: -0.040000000000000036, Y: -0.040000000000000036
Training Loss: 0.3527
X: -0.03000000000000004, Y: -0.040000000000000036
Training Loss: 0.3553
X: -0.020000000000000046, Y: -0.040000000000000036
Training Loss: 0.3362
X: -0.01000000000000005, Y: -0.040000000000000036
Training Loss: 0.3319
X: -5.551115123125783e-17, Y: -0.040000000000000036
Training Loss: 0.3368
X: 0.00999999999999994, Y: -0.040000000000000036
Training Loss: 0.3401
X: 0.019999999999999934, Y: -0.040000000000000036
Training Loss: 0.4454
X: 0.029999999999999943, Y: -0.040000000000000036
Training Loss: 0.5984
X: 0.039999999999999925, Y: -0.040000000000000036
Training Loss: 0.7895
X: 0.049999999999999906, Y: -0.040000000000000036
Training Loss: 1.0262
X: 0.059999999999999915, Y: -0.040000000000000036
Training Loss: 1.2185
X: 0.06999999999999992, Y: -0.040000000000000036
Training Loss: 1.3755
X: 0.0799999999999999, Y: -0.040000000000000036
Training Loss: 1.4693
X: 0.08999999999999989, Y: -0.040000000000000036
Training Loss: 1.4131
X: 0.0999999999999999, Y: -0.040000000000000036
Training Loss: 1.3845
X: -0.1, Y: -0.03000000000000004
Training Loss: 1.2190
X: -0.09000000000000001, Y: -0.03000000000000004
Training Loss: 1.0135
X: -0.08000000000000002, Y: -0.03000000000000004
Training Loss: 0.6372
X: -0.07000000000000002, Y: -0.03000000000000004
Training Loss: 0.4208
X: -0.060000000000000026, Y: -0.03000000000000004
Training Loss: 0.3568
X: -0.05000000000000003, Y: -0.03000000000000004
Training Loss: 0.3373
X: -0.040000000000000036, Y: -0.03000000000000004
Training Loss: 0.3157
X: -0.03000000000000004, Y: -0.03000000000000004
Training Loss: 0.3098
X: -0.020000000000000046, Y: -0.03000000000000004
Training Loss: 0.3242
X: -0.01000000000000005, Y: -0.03000000000000004
Training Loss: 0.3768
X: -5.551115123125783e-17, Y: -0.03000000000000004
Training Loss: 0.4079
X: 0.00999999999999994, Y: -0.03000000000000004
Training Loss: 0.3987
X: 0.019999999999999934, Y: -0.03000000000000004
Training Loss: 0.4541
X: 0.029999999999999943, Y: -0.03000000000000004
Training Loss: 0.5506
X: 0.039999999999999925, Y: -0.03000000000000004
Training Loss: 0.6469
X: 0.049999999999999906, Y: -0.03000000000000004
Training Loss: 0.8214
X: 0.059999999999999915, Y: -0.03000000000000004
Training Loss: 1.0283
X: 0.06999999999999992, Y: -0.03000000000000004
Training Loss: 1.1998
X: 0.0799999999999999, Y: -0.03000000000000004
Training Loss: 1.3085
X: 0.08999999999999989, Y: -0.03000000000000004
Training Loss: 1.2838
X: 0.0999999999999999, Y: -0.03000000000000004
Training Loss: 1.2409
X: -0.1, Y: -0.020000000000000046
Training Loss: 1.8971
X: -0.09000000000000001, Y: -0.020000000000000046
Training Loss: 1.5386
X: -0.08000000000000002, Y: -0.020000000000000046
Training Loss: 0.9333
X: -0.07000000000000002, Y: -0.020000000000000046
Training Loss: 0.5409
X: -0.060000000000000026, Y: -0.020000000000000046
Training Loss: 0.3809
X: -0.05000000000000003, Y: -0.020000000000000046
Training Loss: 0.3439
X: -0.040000000000000036, Y: -0.020000000000000046
Training Loss: 0.2713
X: -0.03000000000000004, Y: -0.020000000000000046
Training Loss: 0.2311
X: -0.020000000000000046, Y: -0.020000000000000046
Training Loss: 0.2098
X: -0.01000000000000005, Y: -0.020000000000000046
Training Loss: 0.2593
X: -5.551115123125783e-17, Y: -0.020000000000000046
Training Loss: 0.3454
X: 0.00999999999999994, Y: -0.020000000000000046
Training Loss: 0.3963
X: 0.019999999999999934, Y: -0.020000000000000046
Training Loss: 0.4220
X: 0.029999999999999943, Y: -0.020000000000000046
Training Loss: 0.4931
X: 0.039999999999999925, Y: -0.020000000000000046
Training Loss: 0.5457
X: 0.049999999999999906, Y: -0.020000000000000046
Training Loss: 0.7003
X: 0.059999999999999915, Y: -0.020000000000000046
Training Loss: 0.9158
X: 0.06999999999999992, Y: -0.020000000000000046
Training Loss: 1.1190
X: 0.0799999999999999, Y: -0.020000000000000046
Training Loss: 1.2564
X: 0.08999999999999989, Y: -0.020000000000000046
Training Loss: 1.2711
X: 0.0999999999999999, Y: -0.020000000000000046
Training Loss: 1.2519
X: -0.1, Y: -0.01000000000000005
Training Loss: 2.6798
X: -0.09000000000000001, Y: -0.01000000000000005
Training Loss: 1.9553
X: -0.08000000000000002, Y: -0.01000000000000005
Training Loss: 0.9820
X: -0.07000000000000002, Y: -0.01000000000000005
Training Loss: 0.6272
X: -0.060000000000000026, Y: -0.01000000000000005
Training Loss: 0.4780
X: -0.05000000000000003, Y: -0.01000000000000005
Training Loss: 0.3756
X: -0.040000000000000036, Y: -0.01000000000000005
Training Loss: 0.2907
X: -0.03000000000000004, Y: -0.01000000000000005
Training Loss: 0.2447
X: -0.020000000000000046, Y: -0.01000000000000005
Training Loss: 0.1641
X: -0.01000000000000005, Y: -0.01000000000000005
Training Loss: 0.0797
X: -5.551115123125783e-17, Y: -0.01000000000000005
Training Loss: 0.1202
X: 0.00999999999999994, Y: -0.01000000000000005
Training Loss: 0.2228
X: 0.019999999999999934, Y: -0.01000000000000005
Training Loss: 0.3098
X: 0.029999999999999943, Y: -0.01000000000000005
Training Loss: 0.4019
X: 0.039999999999999925, Y: -0.01000000000000005
Training Loss: 0.4783
X: 0.049999999999999906, Y: -0.01000000000000005
Training Loss: 0.6432
X: 0.059999999999999915, Y: -0.01000000000000005
Training Loss: 0.9047
X: 0.06999999999999992, Y: -0.01000000000000005
Training Loss: 1.1481
X: 0.0799999999999999, Y: -0.01000000000000005
Training Loss: 1.2842
X: 0.08999999999999989, Y: -0.01000000000000005
Training Loss: 1.3058
X: 0.0999999999999999, Y: -0.01000000000000005
Training Loss: 1.3055
X: -0.1, Y: -5.551115123125783e-17
Training Loss: 2.5683
X: -0.09000000000000001, Y: -5.551115123125783e-17
Training Loss: 1.7497
X: -0.08000000000000002, Y: -5.551115123125783e-17
Training Loss: 1.3509
X: -0.07000000000000002, Y: -5.551115123125783e-17
Training Loss: 1.0757
X: -0.060000000000000026, Y: -5.551115123125783e-17
Training Loss: 0.6524
X: -0.05000000000000003, Y: -5.551115123125783e-17
Training Loss: 0.4467
X: -0.040000000000000036, Y: -5.551115123125783e-17
Training Loss: 0.3410
X: -0.03000000000000004, Y: -5.551115123125783e-17
Training Loss: 0.2753
X: -0.020000000000000046, Y: -5.551115123125783e-17
Training Loss: 0.2448
X: -0.01000000000000005, Y: -5.551115123125783e-17
Training Loss: 0.1756
X: -5.551115123125783e-17, Y: -5.551115123125783e-17
Training Loss: 0.0373
X: 0.00999999999999994, Y: -5.551115123125783e-17
Training Loss: 0.0667
X: 0.019999999999999934, Y: -5.551115123125783e-17
Training Loss: 0.2020
X: 0.029999999999999943, Y: -5.551115123125783e-17
Training Loss: 0.3099
X: 0.039999999999999925, Y: -5.551115123125783e-17
Training Loss: 0.4512
X: 0.049999999999999906, Y: -5.551115123125783e-17
Training Loss: 0.6734
X: 0.059999999999999915, Y: -5.551115123125783e-17
Training Loss: 0.9538
X: 0.06999999999999992, Y: -5.551115123125783e-17
Training Loss: 1.1797
X: 0.0799999999999999, Y: -5.551115123125783e-17
Training Loss: 1.3082
X: 0.08999999999999989, Y: -5.551115123125783e-17
Training Loss: 1.3805
X: 0.0999999999999999, Y: -5.551115123125783e-17
Training Loss: 1.4084
X: -0.1, Y: 0.00999999999999994
Training Loss: 2.6322
X: -0.09000000000000001, Y: 0.00999999999999994
Training Loss: 2.5601
X: -0.08000000000000002, Y: 0.00999999999999994
Training Loss: 2.6636
X: -0.07000000000000002, Y: 0.00999999999999994
Training Loss: 1.8191
X: -0.060000000000000026, Y: 0.00999999999999994
Training Loss: 0.9160
X: -0.05000000000000003, Y: 0.00999999999999994
Training Loss: 0.6132
X: -0.040000000000000036, Y: 0.00999999999999994
Training Loss: 0.4902
X: -0.03000000000000004, Y: 0.00999999999999994
Training Loss: 0.3308
X: -0.020000000000000046, Y: 0.00999999999999994
Training Loss: 0.2872
X: -0.01000000000000005, Y: 0.00999999999999994
Training Loss: 0.2943
X: -5.551115123125783e-17, Y: 0.00999999999999994
Training Loss: 0.3749
X: 0.00999999999999994, Y: 0.00999999999999994
Training Loss: 0.2509
X: 0.019999999999999934, Y: 0.00999999999999994
Training Loss: 0.2540
X: 0.029999999999999943, Y: 0.00999999999999994
Training Loss: 0.3499
X: 0.039999999999999925, Y: 0.00999999999999994
Training Loss: 0.5507
X: 0.049999999999999906, Y: 0.00999999999999994
Training Loss: 0.7918
X: 0.059999999999999915, Y: 0.00999999999999994
Training Loss: 1.0461
X: 0.06999999999999992, Y: 0.00999999999999994
Training Loss: 1.2662
X: 0.0799999999999999, Y: 0.00999999999999994
Training Loss: 1.3864
X: 0.08999999999999989, Y: 0.00999999999999994
Training Loss: 1.4211
X: 0.0999999999999999, Y: 0.00999999999999994
Training Loss: 1.4997
X: -0.1, Y: 0.019999999999999934
Training Loss: 3.7576
X: -0.09000000000000001, Y: 0.019999999999999934
Training Loss: 3.8599
X: -0.08000000000000002, Y: 0.019999999999999934
Training Loss: 3.3788
X: -0.07000000000000002, Y: 0.019999999999999934
Training Loss: 1.7666
X: -0.060000000000000026, Y: 0.019999999999999934
Training Loss: 1.0914
X: -0.05000000000000003, Y: 0.019999999999999934
Training Loss: 0.9710
X: -0.040000000000000036, Y: 0.019999999999999934
Training Loss: 0.7574
X: -0.03000000000000004, Y: 0.019999999999999934
Training Loss: 0.5202
X: -0.020000000000000046, Y: 0.019999999999999934
Training Loss: 0.3735
X: -0.01000000000000005, Y: 0.019999999999999934
Training Loss: 0.3900
X: -5.551115123125783e-17, Y: 0.019999999999999934
Training Loss: 0.4961
X: 0.00999999999999994, Y: 0.019999999999999934
Training Loss: 0.4448
X: 0.019999999999999934, Y: 0.019999999999999934
Training Loss: 0.4044
X: 0.029999999999999943, Y: 0.019999999999999934
Training Loss: 0.4806
X: 0.039999999999999925, Y: 0.019999999999999934
Training Loss: 0.6487
X: 0.049999999999999906, Y: 0.019999999999999934
Training Loss: 0.8660
X: 0.059999999999999915, Y: 0.019999999999999934
Training Loss: 1.0998
X: 0.06999999999999992, Y: 0.019999999999999934
Training Loss: 1.3289
X: 0.0799999999999999, Y: 0.019999999999999934
Training Loss: 1.4509
X: 0.08999999999999989, Y: 0.019999999999999934
Training Loss: 1.5248
X: 0.0999999999999999, Y: 0.019999999999999934
Training Loss: 1.5947
X: -0.1, Y: 0.029999999999999943
Training Loss: 3.8287
X: -0.09000000000000001, Y: 0.029999999999999943
Training Loss: 2.9755
X: -0.08000000000000002, Y: 0.029999999999999943
Training Loss: 2.2924
X: -0.07000000000000002, Y: 0.029999999999999943
Training Loss: 2.1678
X: -0.060000000000000026, Y: 0.029999999999999943
Training Loss: 1.5872
X: -0.05000000000000003, Y: 0.029999999999999943
Training Loss: 1.3742
X: -0.040000000000000036, Y: 0.029999999999999943
Training Loss: 1.1782
X: -0.03000000000000004, Y: 0.029999999999999943
Training Loss: 0.9190
X: -0.020000000000000046, Y: 0.029999999999999943
Training Loss: 0.6215
X: -0.01000000000000005, Y: 0.029999999999999943
Training Loss: 0.5625
X: -5.551115123125783e-17, Y: 0.029999999999999943
Training Loss: 0.6718
X: 0.00999999999999994, Y: 0.029999999999999943
Training Loss: 0.6761
X: 0.019999999999999934, Y: 0.029999999999999943
Training Loss: 0.8137
X: 0.029999999999999943, Y: 0.029999999999999943
Training Loss: 0.7705
X: 0.039999999999999925, Y: 0.029999999999999943
Training Loss: 0.8255
X: 0.049999999999999906, Y: 0.029999999999999943
Training Loss: 0.9699
X: 0.059999999999999915, Y: 0.029999999999999943
Training Loss: 1.1494
X: 0.06999999999999992, Y: 0.029999999999999943
Training Loss: 1.3235
X: 0.0799999999999999, Y: 0.029999999999999943
Training Loss: 1.4574
X: 0.08999999999999989, Y: 0.029999999999999943
Training Loss: 1.5848
X: 0.0999999999999999, Y: 0.029999999999999943
Training Loss: 1.7482
X: -0.1, Y: 0.039999999999999925
Training Loss: 2.2953
X: -0.09000000000000001, Y: 0.039999999999999925
Training Loss: 2.2749
X: -0.08000000000000002, Y: 0.039999999999999925
Training Loss: 2.7972
X: -0.07000000000000002, Y: 0.039999999999999925
Training Loss: 2.7244
X: -0.060000000000000026, Y: 0.039999999999999925
Training Loss: 1.8594
X: -0.05000000000000003, Y: 0.039999999999999925
Training Loss: 1.7564
X: -0.040000000000000036, Y: 0.039999999999999925
Training Loss: 1.4706
X: -0.03000000000000004, Y: 0.039999999999999925
Training Loss: 1.3017
X: -0.020000000000000046, Y: 0.039999999999999925
Training Loss: 0.9997
X: -0.01000000000000005, Y: 0.039999999999999925
Training Loss: 0.9039
X: -5.551115123125783e-17, Y: 0.039999999999999925
Training Loss: 0.9004
X: 0.00999999999999994, Y: 0.039999999999999925
Training Loss: 0.7552
X: 0.019999999999999934, Y: 0.039999999999999925
Training Loss: 0.9202
X: 0.029999999999999943, Y: 0.039999999999999925
Training Loss: 1.1288
X: 0.039999999999999925, Y: 0.039999999999999925
Training Loss: 1.1316
X: 0.049999999999999906, Y: 0.039999999999999925
Training Loss: 1.1690
X: 0.059999999999999915, Y: 0.039999999999999925
Training Loss: 1.2662
X: 0.06999999999999992, Y: 0.039999999999999925
Training Loss: 1.3919
X: 0.0799999999999999, Y: 0.039999999999999925
Training Loss: 1.5106
X: 0.08999999999999989, Y: 0.039999999999999925
Training Loss: 1.6223
X: 0.0999999999999999, Y: 0.039999999999999925
Training Loss: 1.7585
X: -0.1, Y: 0.049999999999999906
Training Loss: 2.7255
X: -0.09000000000000001, Y: 0.049999999999999906
Training Loss: 3.3270
X: -0.08000000000000002, Y: 0.049999999999999906
Training Loss: 3.1430
X: -0.07000000000000002, Y: 0.049999999999999906
Training Loss: 2.1208
X: -0.060000000000000026, Y: 0.049999999999999906
Training Loss: 1.5490
X: -0.05000000000000003, Y: 0.049999999999999906
Training Loss: 1.7915
X: -0.040000000000000036, Y: 0.049999999999999906
Training Loss: 1.7005
X: -0.03000000000000004, Y: 0.049999999999999906
Training Loss: 1.4904
X: -0.020000000000000046, Y: 0.049999999999999906
Training Loss: 1.3622
X: -0.01000000000000005, Y: 0.049999999999999906
Training Loss: 1.3573
X: -5.551115123125783e-17, Y: 0.049999999999999906
Training Loss: 1.3617
X: 0.00999999999999994, Y: 0.049999999999999906
Training Loss: 0.9663
X: 0.019999999999999934, Y: 0.049999999999999906
Training Loss: 0.9821
X: 0.029999999999999943, Y: 0.049999999999999906
Training Loss: 1.1535
X: 0.039999999999999925, Y: 0.049999999999999906
Training Loss: 1.3914
X: 0.049999999999999906, Y: 0.049999999999999906
Training Loss: 1.4217
X: 0.059999999999999915, Y: 0.049999999999999906
Training Loss: 1.4532
X: 0.06999999999999992, Y: 0.049999999999999906
Training Loss: 1.5110
X: 0.0799999999999999, Y: 0.049999999999999906
Training Loss: 1.6092
X: 0.08999999999999989, Y: 0.049999999999999906
Training Loss: 1.7356
X: 0.0999999999999999, Y: 0.049999999999999906
Training Loss: 1.9075
X: -0.1, Y: 0.059999999999999915
Training Loss: 2.8753
X: -0.09000000000000001, Y: 0.059999999999999915
Training Loss: 2.5942
X: -0.08000000000000002, Y: 0.059999999999999915
Training Loss: 1.8392
X: -0.07000000000000002, Y: 0.059999999999999915
Training Loss: 1.3007
X: -0.060000000000000026, Y: 0.059999999999999915
Training Loss: 1.5447
X: -0.05000000000000003, Y: 0.059999999999999915
Training Loss: 1.7411
X: -0.040000000000000036, Y: 0.059999999999999915
Training Loss: 1.8394
X: -0.03000000000000004, Y: 0.059999999999999915
Training Loss: 1.7365
X: -0.020000000000000046, Y: 0.059999999999999915
Training Loss: 1.7483
X: -0.01000000000000005, Y: 0.059999999999999915
Training Loss: 1.8302
X: -5.551115123125783e-17, Y: 0.059999999999999915
Training Loss: 1.8932
X: 0.00999999999999994, Y: 0.059999999999999915
Training Loss: 1.2982
X: 0.019999999999999934, Y: 0.059999999999999915
Training Loss: 1.1405
X: 0.029999999999999943, Y: 0.059999999999999915
Training Loss: 1.2686
X: 0.039999999999999925, Y: 0.059999999999999915
Training Loss: 1.4395
X: 0.049999999999999906, Y: 0.059999999999999915
Training Loss: 1.6266
X: 0.059999999999999915, Y: 0.059999999999999915
Training Loss: 1.6164
X: 0.06999999999999992, Y: 0.059999999999999915
Training Loss: 1.6803
X: 0.0799999999999999, Y: 0.059999999999999915
Training Loss: 1.6941
X: 0.08999999999999989, Y: 0.059999999999999915
Training Loss: 1.8248
X: 0.0999999999999999, Y: 0.059999999999999915
Training Loss: 1.9819
X: -0.1, Y: 0.06999999999999992
Training Loss: 2.2793
X: -0.09000000000000001, Y: 0.06999999999999992
Training Loss: 1.9191
X: -0.08000000000000002, Y: 0.06999999999999992
Training Loss: 2.2080
X: -0.07000000000000002, Y: 0.06999999999999992
Training Loss: 2.8076
X: -0.060000000000000026, Y: 0.06999999999999992
Training Loss: 2.3139
X: -0.05000000000000003, Y: 0.06999999999999992
Training Loss: 2.1291
X: -0.040000000000000036, Y: 0.06999999999999992
Training Loss: 2.2478
X: -0.03000000000000004, Y: 0.06999999999999992
Training Loss: 2.0868
X: -0.020000000000000046, Y: 0.06999999999999992
Training Loss: 1.9751
X: -0.01000000000000005, Y: 0.06999999999999992
Training Loss: 2.1163
X: -5.551115123125783e-17, Y: 0.06999999999999992
Training Loss: 2.3654
X: 0.00999999999999994, Y: 0.06999999999999992
Training Loss: 1.5870
X: 0.019999999999999934, Y: 0.06999999999999992
Training Loss: 1.3327
X: 0.029999999999999943, Y: 0.06999999999999992
Training Loss: 1.4652
X: 0.039999999999999925, Y: 0.06999999999999992
Training Loss: 1.7179
X: 0.049999999999999906, Y: 0.06999999999999992
Training Loss: 1.8373
X: 0.059999999999999915, Y: 0.06999999999999992
Training Loss: 1.8117
X: 0.06999999999999992, Y: 0.06999999999999992
Training Loss: 1.7289
X: 0.0799999999999999, Y: 0.06999999999999992
Training Loss: 1.7517
X: 0.08999999999999989, Y: 0.06999999999999992
Training Loss: 1.8150
X: 0.0999999999999999, Y: 0.06999999999999992
Training Loss: 1.9922
X: -0.1, Y: 0.0799999999999999
Training Loss: 3.2382
X: -0.09000000000000001, Y: 0.0799999999999999
Training Loss: 3.5971
X: -0.08000000000000002, Y: 0.0799999999999999
Training Loss: 3.5840
X: -0.07000000000000002, Y: 0.0799999999999999
Training Loss: 3.1392
X: -0.060000000000000026, Y: 0.0799999999999999
Training Loss: 3.0760
X: -0.05000000000000003, Y: 0.0799999999999999
Training Loss: 3.3132
X: -0.040000000000000036, Y: 0.0799999999999999
Training Loss: 3.3941
X: -0.03000000000000004, Y: 0.0799999999999999
Training Loss: 3.0311
X: -0.020000000000000046, Y: 0.0799999999999999
Training Loss: 2.6508
X: -0.01000000000000005, Y: 0.0799999999999999
Training Loss: 2.5054
X: -5.551115123125783e-17, Y: 0.0799999999999999
Training Loss: 2.7220
X: 0.00999999999999994, Y: 0.0799999999999999
Training Loss: 1.8347
X: 0.019999999999999934, Y: 0.0799999999999999
Training Loss: 1.5301
X: 0.029999999999999943, Y: 0.0799999999999999
Training Loss: 1.7573
X: 0.039999999999999925, Y: 0.0799999999999999
Training Loss: 2.0336
X: 0.049999999999999906, Y: 0.0799999999999999
Training Loss: 2.0053
X: 0.059999999999999915, Y: 0.0799999999999999
Training Loss: 1.9733
X: 0.06999999999999992, Y: 0.0799999999999999
Training Loss: 1.8829
X: 0.0799999999999999, Y: 0.0799999999999999
Training Loss: 1.7812
X: 0.08999999999999989, Y: 0.0799999999999999
Training Loss: 1.7774
X: 0.0999999999999999, Y: 0.0799999999999999
Training Loss: 1.9413
X: -0.1, Y: 0.08999999999999989
Training Loss: 2.9735
X: -0.09000000000000001, Y: 0.08999999999999989
Training Loss: 3.5268
X: -0.08000000000000002, Y: 0.08999999999999989
Training Loss: 5.0850
X: -0.07000000000000002, Y: 0.08999999999999989
Training Loss: 5.6548
X: -0.060000000000000026, Y: 0.08999999999999989
Training Loss: 5.9274
X: -0.05000000000000003, Y: 0.08999999999999989
Training Loss: 5.5867
X: -0.040000000000000036, Y: 0.08999999999999989
Training Loss: 5.1586
X: -0.03000000000000004, Y: 0.08999999999999989
Training Loss: 4.3891
X: -0.020000000000000046, Y: 0.08999999999999989
Training Loss: 3.8810
X: -0.01000000000000005, Y: 0.08999999999999989
Training Loss: 3.1332
X: -5.551115123125783e-17, Y: 0.08999999999999989
Training Loss: 2.7143
X: 0.00999999999999994, Y: 0.08999999999999989
Training Loss: 2.1336
X: 0.019999999999999934, Y: 0.08999999999999989
Training Loss: 1.8923
X: 0.029999999999999943, Y: 0.08999999999999989
Training Loss: 1.9605
X: 0.039999999999999925, Y: 0.08999999999999989
Training Loss: 2.1475
X: 0.049999999999999906, Y: 0.08999999999999989
Training Loss: 2.3089
X: 0.059999999999999915, Y: 0.08999999999999989
Training Loss: 2.1875
X: 0.06999999999999992, Y: 0.08999999999999989
Training Loss: 2.0438
X: 0.0799999999999999, Y: 0.08999999999999989
Training Loss: 1.9818
X: 0.08999999999999989, Y: 0.08999999999999989
Training Loss: 1.9005
X: 0.0999999999999999, Y: 0.08999999999999989
Training Loss: 2.0713
X: -0.1, Y: 0.0999999999999999
Training Loss: 3.2983
X: -0.09000000000000001, Y: 0.0999999999999999
Training Loss: 5.0411
X: -0.08000000000000002, Y: 0.0999999999999999
Training Loss: 8.6725
X: -0.07000000000000002, Y: 0.0999999999999999
Training Loss: 10.0734
X: -0.060000000000000026, Y: 0.0999999999999999
Training Loss: 9.8531
X: -0.05000000000000003, Y: 0.0999999999999999
Training Loss: 8.8640
X: -0.040000000000000036, Y: 0.0999999999999999
Training Loss: 7.5253
X: -0.03000000000000004, Y: 0.0999999999999999
Training Loss: 6.3540
X: -0.020000000000000046, Y: 0.0999999999999999
Training Loss: 4.9374
X: -0.01000000000000005, Y: 0.0999999999999999
Training Loss: 3.8340
X: -5.551115123125783e-17, Y: 0.0999999999999999
Training Loss: 3.0725
X: 0.00999999999999994, Y: 0.0999999999999999
Training Loss: 2.4715
X: 0.019999999999999934, Y: 0.0999999999999999
Training Loss: 2.2540
X: 0.029999999999999943, Y: 0.0999999999999999
Training Loss: 2.2272
X: 0.039999999999999925, Y: 0.0999999999999999
Training Loss: 2.4135
X: 0.049999999999999906, Y: 0.0999999999999999
Training Loss: 2.8100
X: 0.059999999999999915, Y: 0.0999999999999999
Training Loss: 2.4474
X: 0.06999999999999992, Y: 0.0999999999999999
Training Loss: 2.2896
X: 0.0799999999999999, Y: 0.0999999999999999
Training Loss: 2.0423
X: 0.08999999999999989, Y: 0.0999999999999999
Training Loss: 2.0645
X: 0.0999999999999999, Y: 0.0999999999999999
Training Loss: 2.1552
Saved to runs/10_epochs_10:34_12_6_2022/new_data_ResNet_Random_Directions_random.ply
